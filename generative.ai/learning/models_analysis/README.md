# ğŸ¤– Generative AI â€” Decoder Model á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶ á€œá€±á€·á€œá€¬á€›á€”á€º á€œá€™á€ºá€¸á€Šá€½á€¾á€”á€º

> **á€’á€® document á€™á€¾á€¬** Generative AI á€›á€²á€· á€¡á€á€¼á€±á€á€¶ concepts á€á€½á€±á€€á€”á€± Decoder-Only Transformer architecture á€›á€²á€· á€¡á€á€½á€„á€ºá€¸á€•á€­á€¯á€„á€ºá€¸ component á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€¡á€‘á€­ á€¡á€†á€„á€·á€ºá€†á€„á€·á€º á€›á€¾á€„á€ºá€¸á€•á€¼á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹  
> **á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º:** Decoder model á€€á€­á€¯á€œá€±á€·á€œá€¬á€™á€šá€·á€ºá€á€°á€á€½á€±á€¡á€á€½á€€á€º á€á€­á€á€„á€·á€ºá€á€™á€»á€¾ á€¡á€€á€¯á€”á€ºá€…á€¯á€¶ á€á€…á€ºá€”á€±á€›á€¬á€á€Šá€ºá€¸á€™á€¾á€¬ á€–á€á€ºá€›á€¾á€¯á€”á€­á€¯á€„á€ºá€–á€­á€¯á€·á€•á€«á‹

---

## ğŸ“‹ Table of Contents

1. [Generative AI á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²?](#1-generative-ai-á€†á€­á€¯á€á€¬-á€˜á€¬á€œá€²)
2. [Language Model á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸](#2-language-model-á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸)
3. [Transformer Architecture â€” á€¡á€á€¼á€±á€á€¶](#3-transformer-architecture--á€¡á€á€¼á€±á€á€¶)
4. [Decoder-Only Architecture â€” á€¡á€á€±á€¸á€…á€­á€á€º](#4-decoder-only-architecture--á€¡á€á€±á€¸á€…á€­á€á€º)
5. [Tokenization â€” á€…á€¬á€œá€¯á€¶á€¸á€€á€­á€¯ á€‚á€á€”á€ºá€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸](#5-tokenization--á€…á€¬á€œá€¯á€¶á€¸á€€á€­á€¯-á€‚á€á€”á€ºá€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸)
6. [Token Embedding â€” á€‚á€á€”á€ºá€¸á€€á€­á€¯ Vector á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸](#6-token-embedding--á€‚á€á€”á€ºá€¸á€€á€­á€¯-vector-á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸)
7. [Self-Attention Mechanism â€” Model á€›á€²á€· "á€¡á€¬á€›á€¯á€¶á€…á€°á€¸á€…á€­á€¯á€€á€ºá€™á€¾á€¯"](#7-self-attention-mechanism--model-á€›á€²á€·-á€¡á€¬á€›á€¯á€¶á€…á€°á€¸á€…á€­á€¯á€€á€ºá€™á€¾á€¯)
8. [Causal Masking â€” á€¡á€”á€¬á€‚á€á€ºá€€á€­á€¯ á€™á€€á€¼á€Šá€·á€ºá€› Rule](#8-causal-masking--á€¡á€”á€¬á€‚á€á€ºá€€á€­á€¯-á€™á€€á€¼á€Šá€·á€ºá€›-rule)
9. [Positional Encoding â€” á€á€Šá€ºá€”á€±á€›á€¬ á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º](#9-positional-encoding--á€á€Šá€ºá€”á€±á€›á€¬-á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º)
10. [Feed-Forward Network (MLP) â€” á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸](#10-feed-forward-network-mlp--á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º-á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸)
11. [Normalization â€” á€á€”á€ºá€–á€­á€¯á€¸á€™á€»á€¬á€¸ á€Šá€¾á€­á€á€¼á€„á€ºá€¸](#11-normalization--á€á€”á€ºá€–á€­á€¯á€¸á€™á€»á€¬á€¸-á€Šá€¾á€­á€á€¼á€„á€ºá€¸)
12. [Residual Connection â€” Gradient á€œá€™á€ºá€¸á€€á€¼á€±á€¬á€„á€ºá€¸](#12-residual-connection--gradient-á€œá€™á€ºá€¸á€€á€¼á€±á€¬á€„á€ºá€¸)
13. [Output Layer & Token Generation](#13-output-layer--token-generation)
14. [KV Cache â€” Inference Optimization](#14-kv-cache--inference-optimization)
15. [Training â€” Model á€€á€­á€¯ á€˜á€šá€ºá€œá€­á€¯á€á€„á€ºá€•á€±á€¸á€œá€²](#15-training--model-á€€á€­á€¯-á€˜á€šá€ºá€œá€­á€¯á€á€„á€ºá€•á€±á€¸á€œá€²)
16. [Modern Decoder Models á€šá€¾á€‰á€ºá€á€»á€€á€º](#16-modern-decoder-models-á€šá€¾á€‰á€ºá€á€»á€€á€º)
17. [Key Takeaways](#17-key-takeaways)

---

## 1. Generative AI á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²?

**Generative AI** á€†á€­á€¯á€á€¬ data á€¡á€á€…á€ºá€á€½á€± (á€…á€¬áŠ á€•á€¯á€¶áŠ á€¡á€á€¶áŠ code) á€€á€­á€¯ **á€–á€”á€ºá€á€®á€¸á€”á€­á€¯á€„á€ºá€á€²á€·** AI system á€á€½á€±á€•á€«á‹

```mermaid
graph LR
    subgraph Traditional_AI ["Traditional AI (Discriminative)"]
        A1["Input: á€€á€¼á€±á€¬á€„á€ºá€•á€¯á€¶"] --> B1["Model"]
        B1 --> C1["Output: 'á€€á€¼á€±á€¬á€„á€º' label"]
    end

    subgraph Generative_AI ["Generative AI"]
        A2["Input: 'á€€á€¼á€±á€¬á€„á€ºá€á€…á€ºá€€á€±á€¬á€„á€º ...'"] --> B2["Model"]
        B2 --> C2["Output: 'á€€á€¼á€±á€¬á€„á€ºá€á€…á€ºá€€á€±á€¬á€„á€º<br/>á€á€…á€ºá€•á€„á€ºá€•á€±á€«á€ºá€™á€¾á€¬ á€‘á€­á€¯á€„á€ºá€”á€±á€á€šá€º'"]
    end

    style Traditional_AI fill:#1a1a2e,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style Generative_AI fill:#1a1a2e,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

| AI á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ | á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€»á€€á€º | á€¥á€•á€™á€¬ |
|:---:|:---:|:---:|
| **Discriminative** | Input á€€á€­á€¯ classify/label á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ | Spam detector, Image classifier |
| **Generative** | Content á€¡á€á€…á€º á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸ | ChatGPT, DALLÂ·E, Midjourney |

### Generative AI á€›á€²á€· á€¡á€á€¼á€±á€á€¶ Idea

Language Model (LM) á€†á€­á€¯á€á€¬ **á€”á€±á€¬á€€á€ºá€‘á€½á€€á€ºá€œá€¬á€™á€šá€·á€º á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€²á€· probability model** á€•á€«:

$$P(\text{next token} \mid \text{previous tokens})$$

á€¥á€•á€™á€¬ â€” "The cat sat on the" á€†á€­á€¯á€•á€¼á€®á€¸ á€•á€±á€¸á€œá€­á€¯á€€á€ºá€›á€„á€º model á€€:

$$P(\text{"mat"} \mid \text{"The cat sat on the"}) = 0.35$$
$$P(\text{"floor"} \mid \text{"The cat sat on the"}) = 0.20$$
$$P(\text{"roof"} \mid \text{"The cat sat on the"}) = 0.08$$

á€’á€®á€œá€­á€¯á€•á€² token á€á€…á€ºá€á€¯á€•á€¼á€®á€¸á€á€…á€ºá€á€¯ á€†á€€á€ºá€á€­á€¯á€€á€º á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€•á€¼á€®á€¸ á€–á€”á€ºá€á€®á€¸á€á€½á€¬á€¸á€á€¬á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 2. Language Model á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸

Transformer-based language models á€™á€¾á€¬ architecture á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€¼á€®á€¸ á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ áƒ á€™á€»á€­á€¯á€¸ á€›á€¾á€­á€•á€«á€á€šá€º:

```mermaid
graph TD
    T["ğŸ—ï¸ Transformer Architecture"] --> E["Encoder-Only"]
    T --> ED["Encoder-Decoder"]
    T --> D["Decoder-Only"]

    E --> E1["BERT, RoBERTa"]
    E --> E2["á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º: Understanding<br/>Classification, NER, QA"]

    ED --> ED1["T5, BART, mT5"]
    ED --> ED2["á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º: Seq-to-Seq<br/>Translation, Summarization"]

    D --> D1["GPT, Llama, Mistral,<br/>DeepSeek, Granite"]
    D --> D2["á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º: Generation<br/>Text Generation, Chat, Code"]

    style T fill:#0f3460,stroke:#e94560,stroke-width:2px,color:#fff
    style E fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style ED fill:#2d3436,stroke:#fdcb6e,stroke-width:2px,color:#dfe6e9
    style D fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

| Feature | Encoder-Only | Encoder-Decoder | Decoder-Only |
|:--------|:---:|:---:|:---:|
| **Attention Type** | Bidirectional | Bi (enc) + Causal (dec) | Causal (left-to-right) |
| **Input á€€á€¼á€Šá€·á€ºá€•á€¯á€¶** | á€¡á€€á€¯á€”á€ºá€œá€¯á€¶á€¸ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º | Input á€¡á€€á€¯á€”á€º â†’ Output á€á€…á€ºá€œá€¯á€¶á€¸á€á€»á€„á€ºá€¸ | á€˜á€šá€ºá€˜á€€á€ºá€€ á€Šá€¬á€˜á€€á€ºá€€á€­á€¯ |
| **á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯ Model** | BERT | T5 | GPT, Llama |
| **á€¡á€“á€­á€€ Task** | Understanding | Translation, Summary | **Text Generation** |

> **á€’á€® document á€™á€¾á€¬** Generative AI á€›á€²á€· core á€–á€¼á€…á€ºá€á€²á€· **Decoder-Only Architecture** á€€á€­á€¯ á€¡á€“á€­á€€ focus á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹

---

## 3. Transformer Architecture â€” á€¡á€á€¼á€±á€á€¶

2017 á€á€¯á€”á€¾á€…á€º "Attention Is All You Need" paper á€™á€¾á€¬ á€™á€­á€á€ºá€†á€€á€ºá€á€²á€·á€á€²á€· **Transformer** á€Ÿá€¬ modern AI á€›á€²á€· foundation á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### 3.1 Original Transformer (Encoder-Decoder)

```mermaid
graph LR
    subgraph Encoder ["ğŸ”µ Encoder"]
        direction TB
        IE["Input Embedding<br/>+ Positional Encoding"] --> SA["Multi-Head<br/>Self-Attention"]
        SA --> AN["Add & Norm"]
        AN --> FF["Feed-Forward"]
        FF --> AN2["Add & Norm"]
    end

    subgraph Decoder ["ğŸŸ¢ Decoder"]
        direction TB
        OE["Output Embedding<br/>+ Positional Encoding"] --> MSA["Masked Multi-Head<br/>Self-Attention"]
        MSA --> AN3["Add & Norm"]
        AN3 --> CA["Cross-Attention<br/>(Encoder output á€€á€­á€¯á€€á€¼á€Šá€·á€º)"]
        CA --> AN4["Add & Norm"]
        AN4 --> FF2["Feed-Forward"]
        FF2 --> AN5["Add & Norm"]
    end

    Encoder --> |"Encoder Output"| Decoder
    Decoder --> LIN["Linear + Softmax"]
    LIN --> OUT["Output Probabilities"]

    style Encoder fill:#1a1a2e,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style Decoder fill:#1a1a2e,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

### 3.2 Transformer á€›á€²á€· Core Components

Decoder model á€™á€¾á€¬ á€•á€«á€á€„á€ºá€á€²á€· á€¡á€“á€­á€€ building blocks:

| Component | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | Section |
|:----------|:-------------|:-------:|
| **Tokenization** | Text á€€á€­á€¯ á€‚á€á€”á€ºá€¸ (token IDs) á€¡á€–á€¼á€…á€ºá€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸ | [Â§5](#5-tokenization--á€…á€¬á€œá€¯á€¶á€¸á€€á€­á€¯-á€‚á€á€”á€ºá€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸) |
| **Embedding** | Token ID á€€á€­á€¯ vector (numbers list) á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸ | [Â§6](#6-token-embedding--á€‚á€á€”á€ºá€¸á€€á€­á€¯-vector-á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸) |
| **Self-Attention** | Token á€á€…á€ºá€á€¯á€€ á€¡á€á€¼á€¬á€¸ tokens á€€á€­á€¯ á€˜á€šá€ºá€œá€±á€¬á€€á€º á€‚á€›á€¯á€…á€­á€¯á€€á€ºá€›á€™á€œá€² | [Â§7](#7-self-attention-mechanism--model-á€›á€²á€·-á€¡á€¬á€›á€¯á€¶á€…á€°á€¸á€…á€­á€¯á€€á€ºá€™á€¾á€¯) |
| **Causal Mask** | á€¡á€”á€¬á€‚á€á€º tokens á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€á€½á€„á€·á€ºá€™á€›á€¾á€­á€¡á€±á€¬á€„á€º á€•á€­á€á€ºá€á€¼á€„á€ºá€¸ | [Â§8](#8-causal-masking--á€¡á€”á€¬á€‚á€á€ºá€€á€­á€¯-á€™á€€á€¼á€Šá€·á€ºá€›-rule) |
| **Positional Encoding** | Token á€›á€²á€· position (á€˜á€šá€ºá€”á€±á€›á€¬) á€á€á€„á€ºá€¸á€‘á€Šá€·á€ºá€á€¼á€„á€ºá€¸ | [Â§9](#9-positional-encoding--á€á€Šá€ºá€”á€±á€›á€¬-á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º) |
| **Feed-Forward (MLP)** | Feature transformation á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ | [Â§10](#10-feed-forward-network-mlp--á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º-á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸) |
| **Normalization** | á€á€”á€ºá€–á€­á€¯á€¸á€á€½á€± á€€á€¼á€®á€¸á€œá€½á€”á€ºá€¸/á€á€±á€¸á€œá€½á€”á€ºá€¸ á€™á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€Šá€¾á€­á€á€¼á€„á€ºá€¸ | [Â§11](#11-normalization--á€á€”á€ºá€–á€­á€¯á€¸á€™á€»á€¬á€¸-á€Šá€¾á€­á€á€¼á€„á€ºá€¸) |
| **Residual Connection** | Layer á€€á€­á€¯ skip á€•á€¼á€®á€¸ gradient flow á€€á€±á€¬á€„á€ºá€¸á€¡á€±á€¬á€„á€º | [Â§12](#12-residual-connection--gradient-á€œá€™á€ºá€¸á€€á€¼á€±á€¬á€„á€ºá€¸) |
| **Output Projection** | Vector á€€á€­á€¯ vocabulary probabilities á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸ | [Â§13](#13-output-layer--token-generation) |

---

## 4. Decoder-Only Architecture â€” á€¡á€á€±á€¸á€…á€­á€á€º

GPT, Llama, Mistral, DeepSeek á€…á€á€²á€· modern LLMs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€Ÿá€¬ **Decoder-Only** architecture á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ Original Transformer á€›á€²á€· Encoder á€€á€­á€¯ á€–á€šá€ºá€‘á€¯á€á€ºá€•á€¼á€®á€¸ Decoder blocks á€á€½á€±á€€á€­á€¯á€•á€² stack á€œá€¯á€•á€ºá€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

### 4.1 Decoder-Only End-to-End Data Flow

```mermaid
graph TD
    A["ğŸ“ Input Text<br/>'The cat sat'"] --> B["ğŸ”¤ Tokenizer<br/>(BPE/SentencePiece)"]
    B --> C["Token IDs<br/>[464, 3797, 3354]"]
    C --> D["ğŸ“Š Embedding Layer<br/>ID â†’ Vector (d-dim)"]
    D --> E["Hidden States<br/>(batch, seq_len, d_model)"]

    E --> F{"ğŸ” Decoder Layer Ã— N"}

    subgraph Layer ["Single Decoder Layer"]
        direction TB
        G["â‘  Normalization"] --> H["â‘¡ Masked Self-Attention"]
        H --> I["â‘¢ + Residual Connection"]
        I --> J["â‘£ Normalization"]
        J --> K["â‘¤ Feed-Forward (MLP)"]
        K --> L["â‘¥ + Residual Connection"]
    end

    F --> |"N á€€á€¼á€­á€™á€º á€‘á€•á€ºá€á€«á€‘á€•á€ºá€á€«"| Layer
    Layer --> M["Final Normalization"]
    M --> N["ğŸ¯ Output Linear Layer<br/>(d_model â†’ vocab_size)"]
    N --> O["Softmax â†’ Probabilities"]
    O --> P["âœ¨ Next Token: 'on'"]

    style Layer fill:#1a1a2e,stroke:#e94560,stroke-width:2px,color:#dfe6e9
    style F fill:#0f3460,stroke:#e94560,color:#fff
```

### 4.2 á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Decoder-Only á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€á€¬á€œá€²?

| á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€á€»á€€á€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º |
|:---:|:---|
| **á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€™á€¾á€¯** | Encoder-Decoder á€‘á€€á€º architecture á€•á€­á€¯á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸ â†’ train/deploy á€œá€½á€šá€º |
| **Scaling** | Parameters á€á€­á€¯á€¸á€›á€„á€º performance á€€á€±á€¬á€„á€ºá€¸á€œá€¬á€€á€¼á€±á€¬á€„á€ºá€¸ Scaling Laws á€•á€¼ |
| **Versatility** | Generation, QA, Translation, Coding â€” task á€á€­á€¯á€„á€ºá€¸ prompt engineering á€”á€²á€· á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º |
| **Efficiency** | KV Cache á€”á€²á€· inference á€€á€­á€¯ optimize á€œá€¯á€•á€ºá€› á€•á€­á€¯á€œá€½á€šá€º |

### 4.3 Llama 3.2-1B-Instruct á€¥á€•á€™á€¬

```
LlamaForCausalLM
â”œâ”€â”€ model (LlamaModel)
â”‚   â”œâ”€â”€ embed_tokens          â†’ Token Embedding (128,256 Ã— 2,048)
â”‚   â”œâ”€â”€ layers[0..15]         â†’ Decoder Layers Ã— 16
â”‚   â”‚   â”œâ”€â”€ input_layernorm   â†’ RMSNorm (Pre-Attention)
â”‚   â”‚   â”œâ”€â”€ self_attn         â†’ Grouped-Query Attention (GQA)
â”‚   â”‚   â”œâ”€â”€ post_attention_layernorm â†’ RMSNorm (Pre-FFN)
â”‚   â”‚   â””â”€â”€ mlp               â†’ SwiGLU Feed-Forward Network
â”‚   â””â”€â”€ norm                  â†’ Final RMSNorm
â””â”€â”€ lm_head                   â†’ Linear (2,048 â†’ 128,256) [tied with embed_tokens]
```

---

## 5. Tokenization â€” á€…á€¬á€œá€¯á€¶á€¸á€€á€­á€¯ á€‚á€á€”á€ºá€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸

Model á€Ÿá€¬ á€…á€¬á€œá€¯á€¶á€¸á€á€½á€±á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º á€™á€”á€¬á€¸á€œá€Šá€ºá€•á€«á€˜á€°á€¸á‹ **á€‚á€á€”á€ºá€¸** (numbers) á€€á€­á€¯á€•á€² process á€œá€¯á€•á€ºá€á€á€ºá€•á€«á€á€šá€ºá‹ Text á€€á€­á€¯ á€‚á€á€”á€ºá€¸á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€²á€· process á€€á€­á€¯ **Tokenization** á€œá€­á€¯á€·á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

### 5.1 Tokenization Process

```mermaid
graph LR
    A["'Hello world'"] --> B["Tokenizer"]
    B --> C["['Hello', ' world']"]
    C --> D["[15496, 995]"]

    style A fill:#2d3436,stroke:#00b894,color:#fff
    style D fill:#2d3436,stroke:#e94560,color:#fff
```

### 5.2 Tokenizer á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸

| Method | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€á€¯á€¶á€¸á€á€²á€· Model |
|:------:|:-------------|:---:|
| **Word-level** | á€…á€€á€¬á€¸á€œá€¯á€¶á€¸ á€á€…á€ºá€œá€¯á€¶á€¸á€á€»á€„á€ºá€¸ â†’ token á€á€…á€ºá€á€¯ | á€¡á€…á€±á€¬á€•á€­á€¯á€„á€ºá€¸ models |
| **Character-level** | á€¡á€€á€¹á€á€›á€¬ á€á€…á€ºá€œá€¯á€¶á€¸á€á€»á€„á€ºá€¸ â†’ token á€á€…á€ºá€á€¯ | á€¡á€á€¯á€¶á€¸á€”á€Šá€ºá€¸ |
| **BPE** (Byte-Pair Encoding) | á€™á€€á€¼á€¬á€á€ á€•á€±á€«á€ºá€á€²á€· character pairs á€€á€­á€¯ merge | GPT-2, GPT-3, Llama 3 |
| **WordPiece** | BPE á€”á€²á€· á€†á€„á€ºá€á€°, likelihood-based | BERT |
| **SentencePiece** | Language-independent, raw text á€•á€±á€«á€ºá€™á€¾á€¬ train | Llama 2, Mistral |
| **Unigram** | Probability-based subword selection | T5, mT5 |

### 5.3 BPE (Byte-Pair Encoding) â€” á€¡á€á€¯á€¶á€¸á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸

BPE á€›á€²á€· algorithm:

```mermaid
graph TD
    A["á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯<br/>character level á€á€½á€²"] --> B["á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ á€•á€±á€«á€ºá€á€²á€·<br/>character pair á€€á€­á€¯ á€›á€¾á€¬"]
    B --> C["á€‘á€­á€¯ pair á€€á€­á€¯<br/>merge á€•á€¼á€®á€¸ token á€¡á€á€…á€ºá€œá€¯á€•á€º"]
    C --> D{"Vocab size<br/>á€•á€¼á€Šá€·á€ºá€•á€¼á€®á€œá€¬á€¸?"}
    D --> |"á€™á€•á€¼á€Šá€·á€ºá€á€±á€¸"| B
    D --> |"á€•á€¼á€Šá€·á€ºá€•á€¼á€®"| E["âœ… Vocabulary á€•á€¼á€®á€¸á€†á€¯á€¶á€¸"]

    style E fill:#00b894,stroke:#fff,color:#fff
```

**á€¥á€•á€™á€¬:**

| Step | Action | Result |
|:----:|:-------|:-------|
| 0 | "lower" â†’ characters | `l o w e r` |
| 1 | `e` + `r` â†’ `er` (á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ pair) | `l o w er` |
| 2 | `l` + `o` â†’ `lo` | `lo w er` |
| 3 | `lo` + `w` â†’ `low` | `low er` |
| 4 | `low` + `er` â†’ `lower` | `lower` |

### 5.4 Special Tokens

| Token | á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º | á€¥á€•á€™á€¬ |
|:-----:|:-------------|:------|
| `<BOS>` / `<s>` | Sequence á€¡á€… | Beginning of Sequence |
| `<EOS>` / `</s>` | Sequence á€¡á€†á€¯á€¶á€¸ | End of Sequence |
| `<PAD>` | Sequence length á€Šá€®á€¡á€±á€¬á€„á€º á€–á€¼á€Šá€·á€º | Padding |
| `<UNK>` | Vocabulary á€™á€¾á€¬ á€™á€›á€¾á€­á€á€²á€· token | Unknown |

### 5.5 Vocab Size á€€á€­á€¯ á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€‚á€›á€¯á€…á€­á€¯á€€á€ºá€›á€œá€²?

$$\text{Embedding Parameters} = \text{vocab\_size} \times \text{d\_model}$$

| Model | Vocab Size | Embed Dim | Embedding Params |
|:------|:---------:|:---------:|:----------------:|
| GPT-2 | 50,257 | 768 | ~38.6M |
| Llama 3.2-1B | 128,256 | 2,048 | ~262.7M |
| Mistral 7B | 32,000 | 4,096 | ~131.1M |

> **Key Insight:** Vocab size á€€á€¼á€®á€¸á€œá€± language coverage á€€á€±á€¬á€„á€ºá€¸á€œá€± (multilingual support)á‹ á€’á€«á€•á€±á€™á€²á€· embedding layer á€›á€²á€· parameters count á€€á€œá€Šá€ºá€¸ á€€á€¼á€®á€¸á€œá€¬á€•á€«á€á€šá€ºá‹

---

## 6. Token Embedding â€” á€‚á€á€”á€ºá€¸á€€á€­á€¯ Vector á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸

Token ID (integer) á€€á€­á€¯ model process á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€²á€· **dense vector** (continuous numbers list) á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¬á€€á€­á€¯ **Embedding** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

### 6.1 Embedding á€˜á€šá€ºá€œá€­á€¯á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€œá€²?

```mermaid
graph LR
    A["Token ID: 3797<br/>('cat')"] --> B["Embedding Table<br/>W_E âˆˆ â„^(V Ã— d)"]
    B --> C["Vector: [0.12, -0.85, 0.34, ..., 0.67]<br/>d = 2,048 dimensions"]

    style A fill:#2d3436,stroke:#fdcb6e,color:#fff
    style B fill:#0f3460,stroke:#e94560,color:#fff
    style C fill:#2d3436,stroke:#00b894,color:#fff
```

Embedding á€†á€­á€¯á€á€¬ **lookup table** á€•á€«á€•á€²:

$$e = W_E[\text{token\_id}]$$

- $W_E \in \mathbb{R}^{V \times d}$ â€” Embedding matrix (V = vocab size, d = dimension)
- Row á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€€ token á€á€…á€ºá€á€¯á€€á€­á€¯ represent á€œá€¯á€•á€º
- Training process á€™á€¾á€¬ á€’á€® vectors á€á€½á€±á€€á€­á€¯ **learn** á€œá€¯á€•á€ºá€•á€«á€á€šá€º (random values á€€á€”á€± meaningful values á€–á€¼á€…á€ºá€œá€¬)

### 6.2 Embedding Space á€›á€²á€· á€á€˜á€±á€¬

Training á€•á€¼á€®á€¸á€á€²á€·á€¡á€á€« â€” **meaning á€†á€„á€ºá€á€°á€á€²á€· words** á€á€½á€±á€€ **vector space á€™á€¾á€¬ á€”á€®á€¸á€€á€•á€ºá€œá€¬**á€•á€«á€á€šá€º:

```mermaid
graph TD
    subgraph EmbeddingSpace ["Embedding Vector Space"]
        CAT["ğŸ± cat"]
        DOG["ğŸ¶ dog"]
        KITTEN["ğŸ± kitten"]
        CAR["ğŸš— car"]
        TRUCK["ğŸš› truck"]

        CAT -.- |"á€”á€®á€¸"| DOG
        CAT -.- |"á€”á€®á€¸"| KITTEN
        CAR -.- |"á€”á€®á€¸"| TRUCK
        CAT -.- |"á€á€±á€¸"| CAR
    end

    style EmbeddingSpace fill:#1a1a2e,stroke:#6c5ce7,stroke-width:2px,color:#dfe6e9
```

Famous á€¥á€•á€™á€¬:

$$\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}$$

### 6.3 Weight Tying (Embedding Sharing)

Model á€á€á€»á€­á€¯á€·á€™á€¾á€¬ embedding matrix á€€á€­á€¯ input (embed_tokens) á€›á€±á€¬ output (lm_head) á€›á€±á€¬á€™á€¾á€¬ **share** á€á€¯á€¶á€¸á€•á€«á€á€šá€º:

$$\text{Input:} \quad e = W_E[\text{token}] \quad \text{(row lookup)}$$
$$\text{Output:} \quad \text{logits} = W_E^T \cdot h \quad \text{(matrix multiply)}$$

| Model | Weight Tying | á€€á€»á€­á€¯á€¸á€€á€¼á€±á€¬á€„á€ºá€¸ |
|:------|:---:|:---|
| Llama 3.2-1B | âœ… Yes | Parameters á€á€€á€ºá€á€¬ (~263M save) â€” small model á€™á€­á€¯á€· |
| GPT-2 | âœ… Yes | Small model â€” efficiency |
| GPT-3 (175B) | âŒ No | Large model â€” separate weights á€€ performance á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸ |
| Mistral 7B | âŒ No | Large enough â€” weight tying á€™á€œá€­á€¯ |

---

## 7. Self-Attention Mechanism â€” Model á€›á€²á€· "á€¡á€¬á€›á€¯á€¶á€…á€°á€¸á€…á€­á€¯á€€á€ºá€™á€¾á€¯"

Self-Attention á€Ÿá€¬ Transformer á€›á€²á€· **á€¡á€›á€±á€¸á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ component** á€–á€¼á€…á€ºá€•á€¼á€®á€¸ token á€á€…á€ºá€á€¯á€€ **á€¡á€á€¼á€¬á€¸ tokens á€á€½á€±á€€á€­á€¯ á€˜á€šá€ºá€œá€±á€¬á€€á€º á€‚á€›á€¯á€…á€­á€¯á€€á€ºá€›á€™á€œá€²** á€†á€­á€¯á€á€¬ á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹

### 7.1 Q, K, V á€†á€­á€¯á€á€¬ á€˜á€¬á€á€½á€±á€œá€²?

Library á€…á€¬á€€á€¼á€Šá€·á€ºá€á€­á€¯á€€á€º á€¥á€•á€™á€¬á€”á€²á€· á€›á€¾á€„á€ºá€¸á€•á€¼á€•á€«á€™á€šá€º:

| Concept | Library á€¥á€•á€™á€¬ | Attention á€™á€¾á€¬ |
|:-------:|:-------------|:-------------|
| **Query (Q)** | "á€„á€« AI á€…á€¬á€¡á€¯á€•á€º á€›á€¾á€¬á€”á€±á€á€šá€º" â€” á€›á€¾á€¬á€”á€±á€á€²á€· á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬ | á€’á€® token á€€ á€˜á€¬á€€á€­á€¯ á€›á€¾á€¬á€”á€±á€œá€² |
| **Key (K)** | á€…á€¬á€¡á€¯á€•á€ºá€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€›á€²á€· title/description | Token á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€›á€²á€· "á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€á€²á€· description" |
| **Value (V)** | á€…á€¬á€¡á€¯á€•á€ºá€‘á€²á€€ content | Token á€‘á€²á€€ actual information |

### 7.2 Attention á€˜á€šá€ºá€œá€­á€¯á€á€½á€€á€ºá€œá€²?

**Step 1:** Input á€€á€”á€± Q, K, V á€–á€”á€ºá€á€®á€¸

$$Q = xW^Q, \quad K = xW^K, \quad V = xW^V$$

**Step 2:** Q á€”á€²á€· K á€›á€²á€· similarity á€€á€­á€¯ dot product á€”á€²á€·á€á€½á€€á€º (á€˜á€šá€º key á€€ query á€”á€²á€· á€€á€­á€¯á€€á€ºá€Šá€®á€œá€²)

$$\text{score} = QK^T$$

**Step 3:** Scale á€œá€¯á€•á€º (dimension á€€á€¼á€®á€¸á€›á€„á€º scores á€€á€¼á€®á€¸á€œá€½á€”á€ºá€¸á€™á€¾á€¬á€…á€­á€¯á€¸á€œá€­á€¯á€·)

$$\text{scaled\_score} = \frac{QK^T}{\sqrt{d_k}}$$

**Step 4:** Softmax á€”á€²á€· probabilities á€•á€¼á€±á€¬á€„á€ºá€¸ (0 á€”á€²á€· 1 á€€á€¼á€¬á€¸á€‘á€², á€•á€±á€«á€„á€ºá€¸á€œá€’á€º = 1)

$$\text{attention\_weights} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$$

**Step 5:** Weights á€”á€²á€· V á€€á€­á€¯ multiply (important values á€€á€­á€¯ á€•á€­á€¯á€šá€°)

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

```mermaid
graph TD
    X["Input (x)"] --> WQ["Ã— W_Q"]
    X --> WK["Ã— W_K"]
    X --> WV["Ã— W_V"]
    WQ --> Q["Q (Query)"]
    WK --> K["K (Key)"]
    WV --> V["V (Value)"]

    Q --> DOT["Q Ã— Káµ€<br/>(Dot Product)"]
    K --> DOT
    DOT --> SCALE["Ã· âˆšd_k<br/>(Scale)"]
    SCALE --> MASK["+ Causal Mask<br/>(-âˆ for future)"]
    MASK --> SM["Softmax"]
    SM --> MUL["Ã— V"]
    V --> MUL
    MUL --> OUT["Attention Output"]

    style Q fill:#e17055,stroke:#fff,color:#fff
    style K fill:#0984e3,stroke:#fff,color:#fff
    style V fill:#00b894,stroke:#fff,color:#fff
    style OUT fill:#6c5ce7,stroke:#fff,color:#fff
```

### 7.3 Multi-Head Attention (MHA) â€” á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º head á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ á€œá€­á€¯á€œá€²?

Head **á€á€…á€ºá€á€¯á€á€Šá€ºá€¸**á€†á€­á€¯á€›á€„á€º relationship type **á€á€…á€ºá€™á€»á€­á€¯á€¸**á€•á€² á€–á€™á€ºá€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ Head **á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸**á€†á€­á€¯á€›á€„á€º relationship **á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸** á€–á€™á€ºá€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º:

| Head | á€–á€™á€ºá€¸á€”á€­á€¯á€„á€ºá€á€²á€· Relationship á€¥á€•á€™á€¬ |
|:----:|:---|
| Head 1 | Syntax â€” subject-verb agreement |
| Head 2 | Semantics â€” meaning similarity |
| Head 3 | Positional â€” nearby words |
| Head 4 | Coreference â€” "he" â†’ "John" |

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

### 7.4 Attention Variants â€” MHA vs MQA vs GQA vs MLA

Model á€á€½á€±á€¡á€€á€¼á€¬á€¸á€™á€¾á€¬ attention mechanism á€€á€½á€¬á€á€¼á€¬á€¸á€•á€«á€á€šá€º:

```mermaid
graph TB
    subgraph MHA ["MHA â€” Multi-Head Attention<br/>(GPT-2, GPT-3)"]
        direction LR
        Q1["Qâ‚ - Kâ‚ - Vâ‚"]
        Q2["Qâ‚‚ - Kâ‚‚ - Vâ‚‚"]
        Q3["Qâ‚ƒ - Kâ‚ƒ - Vâ‚ƒ"]
        Q4["Qâ‚„ - Kâ‚„ - Vâ‚„"]
    end

    subgraph MQA ["MQA â€” Multi-Query Attention<br/>(PaLM)"]
        direction LR
        QA["Qâ‚"] --- KS["K<br/>(shared)"]
        QB["Qâ‚‚"] --- KS
        QC["Qâ‚ƒ"] --- KS
        QD["Qâ‚„"] --- KS
        KS --- VS["V<br/>(shared)"]
    end

    subgraph GQA ["GQA â€” Grouped-Query Attention<br/>(Llama, Mistral, Granite)"]
        direction LR
        QG1["Qâ‚ â”€â”"]
        QG2["Qâ‚‚ â”€â”˜â”€ Kâ‚Vâ‚"]
        QG3["Qâ‚ƒ â”€â”"]
        QG4["Qâ‚„ â”€â”˜â”€ Kâ‚‚Vâ‚‚"]
    end

    style MHA fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style MQA fill:#2d3436,stroke:#e17055,stroke-width:2px,color:#dfe6e9
    style GQA fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

| Variant | Q Heads | KV Heads | KV Cache | Quality | Speed |
|:-------:|:-------:|:--------:|:--------:|:-------:|:-----:|
| **MHA** | h | h | âŒ á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ | âœ… á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ | âŒ á€”á€¾á€±á€¸ |
| **MQA** | h | 1 | âœ… á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸ | âš ï¸ quality á€€á€» | âœ… á€¡á€™á€¼á€”á€ºá€†á€¯á€¶á€¸ |
| **GQA** | h | g (1 < g < h) | âœ… á€•á€­á€¯á€”á€Šá€ºá€¸ | âœ… MHA á€”á€®á€¸á€•á€«á€¸á€€á€±á€¬á€„á€ºá€¸ | âœ… á€•á€­á€¯á€™á€¼á€”á€º |
| **MLA** | h | compressed | âœ…âœ… compress á€‘á€¬á€¸ | âœ… á€€á€±á€¬á€„á€ºá€¸ | âœ… á€•á€­á€¯á€™á€¼á€”á€º |

### 7.5 GQA â€” Llama 3.2 á€¥á€•á€™á€¬

Llama 3.2-1B á€›á€²á€· GQA configuration:

- **Q heads = 32**, **KV heads = 8**
- Group size = 32 Ã· 8 = **4** (Q head 4 á€á€¯ â†’ KV head 1 á€á€¯ share)

$$Q \in \mathbb{R}^{n \times 32 \times 64}, \quad K \in \mathbb{R}^{n \times 8 \times 64}, \quad V \in \mathbb{R}^{n \times 8 \times 64}$$

**KV Cache savings:**

| | MHA (32 KV heads) | GQA (8 KV heads) | Savings |
|:-|:---:|:---:|:---:|
| KV params/layer | 8,388,608 | 2,097,152 | **75%** |
| Cache (16 layers, seq=1024, FP16) | 134.22 MB | 33.55 MB | **75%** |

### 7.6 MLA â€” DeepSeek á€›á€²á€· Compressed Attention

DeepSeek-V2 á€€ KV á€€á€­á€¯ **compress** á€œá€¯á€•á€ºá€•á€¼á€®á€¸ latent vector $c_{KV}$ á€¡á€”á€±á€”á€²á€· cache á€•á€«á€á€šá€º:

$$c_{KV} = W^{DKV} h \in \mathbb{R}^{d_c} \quad (d_c \ll n_h \times d_h)$$
$$K = W^{UK} c_{KV}, \quad V = W^{UV} c_{KV}$$

> GQA á€‘á€€á€º memory á€•á€­á€¯á€á€€á€ºá€á€¬á€•á€±á€™á€²á€· decompression computation á€•á€­á€¯á€œá€­á€¯á€•á€«á€á€šá€ºá‹

---

## 8. Causal Masking â€” á€¡á€”á€¬á€‚á€á€ºá€€á€­á€¯ á€™á€€á€¼á€Šá€·á€ºá€› Rule

Decoder model á€Ÿá€¬ text á€€á€­á€¯ **á€˜á€šá€ºá€€á€”á€± á€Šá€¬** (left-to-right) generate á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€€á€¼á€±á€¬á€„á€·á€º token á€á€…á€ºá€á€¯á€€ **á€á€°á€·á€”á€±á€¬á€€á€ºá€€á€œá€¬á€™á€šá€·á€º tokens** á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€á€½á€„á€·á€º **á€™á€›á€¾á€­á€•á€«á€˜á€°á€¸**á‹ á€’á€«á€€á€­á€¯ **Causal Masking** (Autoregressive Masking) á€œá€­á€¯á€·á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

### 8.1 Causal Mask á€˜á€šá€ºá€œá€­á€¯á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€œá€²?

"The cat sat" á€†á€­á€¯á€á€²á€· sequence á€¡á€á€½á€€á€º:

```
Attention Matrix (mask applied):

              The    cat    sat
    The    [  âœ…     âŒ     âŒ  ]    â† "The" á€€ á€€á€­á€¯á€šá€·á€ºá€€á€­á€¯á€šá€ºá€€á€­á€¯á€•á€² á€€á€¼á€Šá€·á€ºá€›
    cat    [  âœ…     âœ…     âŒ  ]    â† "cat" á€€ "The" á€›á€±á€¬ á€€á€­á€¯á€šá€·á€ºá€€á€­á€¯á€šá€ºá€€á€­á€¯á€›á€±á€¬ á€€á€¼á€Šá€·á€ºá€›
    sat    [  âœ…     âœ…     âœ…  ]    â† "sat" á€€ á€¡á€€á€¯á€”á€ºá€œá€¯á€¶á€¸ á€€á€¼á€Šá€·á€ºá€›
```

$$\text{Mask} = \begin{pmatrix} 0 & -\infty & -\infty \\ 0 & 0 & -\infty \\ 0 & 0 & 0 \end{pmatrix}$$

$-\infty$ á€€á€­á€¯ score á€™á€¾á€¬ á€•á€±á€«á€„á€ºá€¸á€œá€­á€¯á€€á€ºá€›á€„á€º softmax á€•á€¼á€®á€¸á€á€²á€·á€¡á€á€« $e^{-\infty} = 0$ á€–á€¼á€…á€ºá€•á€«á€á€šá€º â†’ future tokens á€€á€­á€¯ **completely ignore** á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

$$\text{CausalAttention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + \text{Mask}\right) V$$

```mermaid
graph LR
    subgraph Bidirectional ["Bidirectional (BERT)"]
        direction TB
        B1["The"] <--> B2["cat"]
        B2 <--> B3["sat"]
        B1 <--> B3
    end

    subgraph Causal ["Causal / Autoregressive (GPT, Llama)"]
        direction TB
        C1["The"] --> C2["cat"]
        C1 --> C3["sat"]
        C2 --> C3
    end

    style Bidirectional fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style Causal fill:#2d3436,stroke:#e94560,stroke-width:2px,color:#dfe6e9
```

> **á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Mask á€œá€­á€¯á€œá€²?** Training time á€™á€¾á€¬ sequence á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ parallel process á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Mask á€™á€›á€¾á€­á€›á€„á€º model á€€ "cheating" â€” answer á€€á€­á€¯ á€€á€¼á€­á€¯á€™á€¼á€„á€ºá€•á€¼á€®á€¸á€á€¬á€¸ á€–á€¼á€…á€ºá€á€½á€¬á€¸á€™á€¾á€¬á€•á€«á‹

---

## 9. Positional Encoding â€” á€á€Šá€ºá€”á€±á€›á€¬ á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º

Self-Attention á€Ÿá€¬ **position á€€á€­á€¯ á€™á€á€­á€•á€«á€˜á€°á€¸** (permutation invariant) â€” "cat sat" á€›á€±á€¬ "sat cat" á€›á€±á€¬ attention score á€á€°á€Šá€®á€•á€«á€á€šá€ºá‹ á€’á€«á€€á€¼á€±á€¬á€„á€·á€º **position information** á€€á€­á€¯ explicitly á€‘á€Šá€·á€ºá€•á€±á€¸á€›á€•á€«á€á€šá€ºá‹

### 9.1 Positional Encoding Methods

```mermaid
graph TD
    PE["Positional Encoding"] --> ABS["Absolute"]
    PE --> REL["Relative"]

    ABS --> LEARN["Learned<br/>(GPT-2)"]
    ABS --> SIN["Sinusoidal<br/>(Original Transformer)"]

    REL --> ROPE["RoPE<br/>(Llama, Mistral, Granite)"]
    REL --> ALIBI["ALiBi<br/>(Bloom, MPT)"]
    REL --> YARN["RoPE + YaRN<br/>(DeepSeek, Llama 3.1)"]

    style PE fill:#0f3460,stroke:#e94560,color:#fff
    style ROPE fill:#00b894,stroke:#fff,color:#fff
```

### 9.2 Learned Absolute â€” GPT-2

$$h_0 = W_E[\text{token}] + W_P[\text{position}]$$

- $W_P \in \mathbb{R}^{L_{\max} \times d}$ â€” position embedding á€€á€­á€¯ learn
- **á€¡á€¬á€¸á€”á€Šá€ºá€¸á€á€»á€€á€º:** Maximum length ($L_{\max}$) á€‘á€€á€º á€›á€¾á€Šá€ºá€›á€„á€º handle á€™á€›

### 9.3 RoPE (Rotary Position Embedding) â€” Modern Standard

Llama, Mistral, Granite, DeepSeek á€…á€á€Šá€ºá€–á€¼á€„á€·á€º modern models á€¡á€¬á€¸á€œá€¯á€¶á€¸á€”á€®á€¸á€•á€«á€¸ RoPE á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹

**Core Idea:** Q, K vectors á€€á€­á€¯ **rotation** (á€œá€¾á€Šá€·á€ºá€á€¼á€„á€ºá€¸) á€œá€¯á€•á€ºá€•á€¼á€®á€¸ position encode:

$$\tilde{q}_m = R_\Theta(m) \cdot q_m, \quad \tilde{k}_n = R_\Theta(n) \cdot k_n$$

2D pair á€á€…á€ºá€á€¯á€¡á€á€½á€€á€º rotation:

$$R_\theta(m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}$$

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Rotation á€€á€±á€¬á€„á€ºá€¸á€œá€²?**

Dot product á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€›á€„á€º:

$$\tilde{q}_m^T \tilde{k}_n = q_m^T R_\Theta(n - m) k_n$$

Attention score á€á€Šá€º **relative position $(n-m)$** á€•á€±á€«á€ºá€™á€¾á€¬á€•á€² depend á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Position 3 á€”á€²á€· 5 á€€á€¼á€¬á€¸á€€ relationship á€Ÿá€¬ Position 100 á€”á€²á€· 102 á€€á€¼á€¬á€¸á€€ relationship á€”á€²á€· **á€†á€„á€ºá€á€°á€•á€«á€á€šá€º**á‹

```mermaid
graph LR
    subgraph RoPE_Process ["RoPE â€” Query/Key vector á€€á€­á€¯ Rotate"]
        direction TB
        Q["q vector (d=64)"] --> PAIR["32 pairs á€á€½á€²<br/>(qâ‚€,qâ‚), (qâ‚‚,qâ‚ƒ), ..."]
        PAIR --> ROT["2D rotation apply<br/>angle = position Ã— Î¸áµ¢"]
        ROT --> Q_OUT["qÌƒ (rotated query)"]
    end

    subgraph Benefit ["Benefit"]
        B1["âœ… Relative position á€›"]
        B2["âœ… Length extrapolation á€€á€±á€¬á€„á€ºá€¸"]
        B3["âœ… Extra parameters á€™á€œá€­á€¯"]
        B4["âœ… V á€€á€­á€¯ rotate á€™á€œá€¯á€•á€º<br/> â†’ content representation á€™á€•á€»á€€á€º"]
    end

    style RoPE_Process fill:#2d3436,stroke:#6c5ce7,stroke-width:2px,color:#dfe6e9
    style Benefit fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

### 9.4 Cross-Model Positional Encoding Comparison

| Method | Model | Relative? | Extrapolation | Applied to |
|:------:|:-----:|:---------:|:-------------:|:----------:|
| Learned Absolute | GPT-2 | âŒ | âŒ Poor | Embedding layer |
| Sinusoidal | Original Transformer | âŒ | âš ï¸ Limited | Embedding layer |
| **RoPE** | Llama, Mistral, Granite | âœ… | âœ… Good | Q, K only |
| RoPE + SWA | Mistral | âœ… | âœ… Excellent | Q, K only |
| ALiBi | Bloom, MPT | âœ… | âœ… Good | Attention bias |
| RoPE + YaRN | DeepSeek, Llama 3.1 | âœ… | âœ… Excellent | Q, K only |

---

## 10. Feed-Forward Network (MLP) â€” á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€€á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸

Attention á€•á€¼á€®á€¸á€á€²á€·á€¡á€á€« token á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€€á€­á€¯ **Feed-Forward Network (FFN / MLP)** á€€á€­á€¯ á€–á€¼á€á€ºá€•á€­á€¯á€·á€•á€«á€á€šá€ºá‹ á€’á€® layer á€€ **feature transformation** á€œá€¯á€•á€ºá€•á€¼á€®á€¸ model á€›á€²á€· "thinking" / "reasoning" capacity á€€á€­á€¯ á€á€­á€¯á€¸á€•á€±á€¸á€•á€«á€á€šá€ºá‹

### 10.1 FFN Evolution

```mermaid
graph LR
    R["ReLU FFN<br/>(Original)"] --> G["GELU FFN<br/>(GPT-2/3)"]
    G --> S["SwiGLU<br/>(Llama, Mistral,<br/>DeepSeek, Granite)"]
    S --> M["SwiGLU + MoE<br/>(DeepSeek, Mixtral)"]

    style R fill:#2d3436,stroke:#e17055,color:#fff
    style G fill:#2d3436,stroke:#fdcb6e,color:#fff
    style S fill:#2d3436,stroke:#00b894,color:#fff
    style M fill:#2d3436,stroke:#6c5ce7,color:#fff
```

### 10.2 Standard FFN â€” GPT-2/3

$$\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 x + b_1) + b_2$$

- Dimension á€€á€­á€¯ expand â†’ activation â†’ shrink

### 10.3 SwiGLU â€” Modern Standard (Llama, Mistral, Granite)

$$\text{SwiGLU}(x) = W_{\text{down}} \cdot \left[\text{SiLU}(W_{\text{gate}} x) \odot (W_{\text{up}} x)\right]$$

$$\text{SiLU}(x) = \frac{x}{1 + e^{-x}}$$

```mermaid
graph LR
    X["x<br/>(d=2048)"] --> GATE["W_gate<br/>2048â†’8192"]
    X --> UP["W_up<br/>2048â†’8192"]
    GATE --> SILU["SiLU activation"]
    SILU --> MUL["âŠ™ Element-wise<br/>Multiply"]
    UP --> MUL
    MUL --> DOWN["W_down<br/>8192â†’2048"]
    DOWN --> OUT["output<br/>(d=2048)"]

    style X fill:#2d3436,stroke:#00b894,color:#fff
    style MUL fill:#e17055,stroke:#fff,color:#fff
    style OUT fill:#2d3436,stroke:#00b894,color:#fff
```

**SwiGLU á€›á€²á€· Gating Mechanism â€” á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€…á€½á€¬ á€›á€¾á€„á€ºá€¸á€•á€¼á€á€»á€€á€º:**

| Gate Value | á€–á€¼á€…á€ºá€›á€•á€º |
|:---:|:---|
| â‰ˆ 0 | Information á€€á€­á€¯ **block** (á€™á€›á€±á€¸á€•á€«á€”á€²á€· á€•á€­á€á€ºá€•á€±á€¸á€™á€šá€º) |
| â‰ˆ x | Information á€€á€­á€¯ **pass through** (á€–á€¼á€á€ºá€á€½á€„á€·á€ºá€•á€±á€¸á€™á€šá€º) |

Gate á€€ input-dependent á€–á€¼á€…á€ºá€œá€­á€¯á€· **dynamic feature selection** â€” token á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€¡á€á€½á€€á€º á€˜á€šá€º features á€€á€­á€¯ emphasize / suppress á€œá€¯á€•á€ºá€™á€œá€² á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€•á€«á€á€šá€ºá‹

### 10.4 á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º SwiGLU á€€ á€€á€±á€¬á€„á€ºá€¸á€œá€²?

| Feature | ReLU | GELU | SwiGLU |
|:--------|:----:|:----:|:------:|
| Dead neurons | âŒ á€›á€¾á€­ | âš ï¸ á€”á€Šá€ºá€¸ | âœ… á€™á€›á€¾á€­ |
| Smooth gradient | âŒ | âœ… | âœ… |
| Gating mechanism | âŒ | âŒ | âœ… Dynamic |
| Matrices per layer | 2 | 2 | 3 (gate, up, down) |
| Empirical quality | â”€â”€ | â”€â”€ | **Best** |

> **Trade-off:** SwiGLU á€™á€¾á€¬ matrix 3 á€á€¯ á€á€¯á€¶á€¸á€›á€œá€­á€¯á€· parameters ~50% á€•á€­á€¯á€™á€»á€¬á€¸á€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€²á€· quality improvement á€€ á€•á€­á€¯á€á€”á€ºá€–á€­á€¯á€¸á€›á€¾á€­á€•á€«á€á€šá€ºá‹

### 10.5 Mixture of Experts (MoE) â€” DeepSeek, Mixtral

MoE á€Ÿá€¬ FFN layer á€™á€¾á€¬ **expert network** á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸á€‘á€¬á€¸á€•á€¼á€®á€¸ token á€á€…á€ºá€á€¯á€¡á€á€½á€€á€º experts á€¡á€”á€Šá€ºá€¸á€„á€šá€ºá€€á€­á€¯á€•á€² activate á€•á€«á€á€šá€º:

```mermaid
graph TB
    H["hidden state x"] --> ROUTER["Router Network<br/>softmax(W_r Â· x)"]
    ROUTER --> |"top-k select"| E1["Expert 1 (SwiGLU)"]
    ROUTER --> |"top-k select"| E2["Expert 2 (SwiGLU)"]
    ROUTER --> DOTS["..."]
    ROUTER --> EN["Expert N (SwiGLU)"]

    E1 --> SUM["Weighted Sum<br/>Î£ gáµ¢ Â· Expertáµ¢(x)"]
    E2 --> SUM
    EN --> SUM
    SUM --> OUT["output"]

    style ROUTER fill:#e17055,stroke:#fff,color:#fff
    style SUM fill:#00b894,stroke:#fff,color:#fff
```

$$\text{MoE-FFN}(x) = \sum_{i \in \text{Top-}k} g_i(x) \cdot E_i(x)$$

> **Advantage:** Total parameters 236B á€›á€¾á€­á€•á€±á€™á€²á€· token á€á€…á€ºá€á€¯á€¡á€á€½á€€á€º 21B á€•á€² activate â†’ **speed á€•á€­á€¯á€™á€¼á€”á€º**

---

## 11. Normalization â€” á€á€”á€ºá€–á€­á€¯á€¸á€™á€»á€¬á€¸ á€Šá€¾á€­á€á€¼á€„á€ºá€¸

Deep neural network á€™á€¾á€¬ layer á€á€½á€± á€•á€­á€¯á€™á€»á€¬á€¸á€œá€¬á€á€¬á€”á€²á€· values á€á€½á€± á€€á€¼á€®á€¸á€œá€½á€”á€ºá€¸ (explode) / á€á€±á€¸á€œá€½á€”á€ºá€¸ (vanish) á€á€á€ºá€•á€«á€á€šá€ºá‹ **Normalization** á€€ á€’á€®á€•á€¼á€¿á€”á€¬á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

### 11.1 LayerNorm â€” GPT-2, GPT-3

$$\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta$$

- $\mu$ = mean (á€•á€»á€™á€ºá€¸á€™á€»á€¾), $\sigma^2$ = variance (á€–á€¼á€”á€·á€ºá€€á€¼á€€á€º)
- $\gamma$ = scale (learnable), $\beta$ = shift (learnable)
- Operations: **Mean â†’ Variance â†’ Normalize â†’ Scale & Shift**

### 11.2 RMSNorm â€” Llama, Mistral, DeepSeek, Granite (Modern Standard)

$$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_i^2 + \epsilon}} \cdot \gamma$$

- Mean á€€á€­á€¯ **á€™á€á€½á€€á€ºá€•á€«** â€” RMS (Root Mean Square) á€•á€² á€á€½á€€á€º
- Bias ($\beta$) **á€™á€›á€¾á€­á€•á€«** â€” scale ($\gamma$) á€•á€² á€›á€¾á€­
- Operations: **RMS â†’ Normalize â†’ Scale** (á€•á€­á€¯á€”á€Šá€ºá€¸, á€•á€­á€¯á€™á€¼á€”á€º)

```mermaid
graph LR
    subgraph LN ["LayerNorm (GPT)"]
        direction TB
        A1["â‘  Mean (Î¼) á€á€½á€€á€º"] --> A2["â‘¡ Variance (ÏƒÂ²) á€á€½á€€á€º"]
        A2 --> A3["â‘¢ Normalize: (x-Î¼)/âˆš(ÏƒÂ²+Îµ)"]
        A3 --> A4["â‘£ Scale (Î³) & Shift (Î²)"]
    end

    subgraph RMS ["RMSNorm (Llama, Modern)"]
        direction TB
        B1["â‘  RMS = âˆš(Î£xÂ²/d) á€á€½á€€á€º"] --> B2["â‘¡ Normalize: x/RMS"]
        B2 --> B3["â‘¢ Scale (Î³) only"]
    end

    style LN fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style RMS fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

### 11.3 á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º RMSNorm á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€œá€²?

| Feature | LayerNorm | RMSNorm |
|:--------|:---------:|:-------:|
| Mean computation | âœ… á€œá€­á€¯ | âŒ á€™á€œá€­á€¯ |
| Bias parameter ($\beta$) | âœ… á€›á€¾á€­ | âŒ á€™á€›á€¾á€­ |
| Speed | Baseline | **~33% á€•á€­á€¯á€™á€¼á€”á€º** |
| Training stability | á€€á€±á€¬á€„á€ºá€¸ | **á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸** |
| Learnable params | $2d$ | $d$ |

> **Research finding:** "Re-centering (mean subtraction) is not as important as re-scaling" â€” Zhang & Sennrich (2019)

### 11.4 Pre-Norm vs Post-Norm

Normalization á€€á€­á€¯ attention/FFN á€›á€²á€· **á€¡á€›á€„á€º** (Pre) á€œá€¯á€•á€ºá€™á€œá€¬á€¸ **á€¡á€•á€¼á€®á€¸** (Post) á€œá€¯á€•á€ºá€™á€œá€¬á€¸ á€†á€­á€¯á€á€¬ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€•á€«á€á€šá€º:

| Position | Formula | Model |
|:--------:|:--------|:------|
| **Post-Norm** | $\text{Norm}(x + \text{SubLayer}(x))$ | GPT-2, Original Transformer |
| **Pre-Norm** âœ… | $x + \text{SubLayer}(\text{Norm}(x))$ | Llama, Mistral, GPT-3, DeepSeek, Granite |

> **Pre-Norm á€€á€±á€¬á€„á€ºá€¸á€á€²á€· á€¡á€€á€¼á€±á€¬á€„á€ºá€¸:** Residual connection á€€á€”á€± gradient á€Ÿá€¬ normalization á€€á€­á€¯ skip á€•á€¼á€®á€¸ direct flow á€–á€¼á€…á€ºá€œá€­á€¯á€· **deep network train á€œá€¯á€•á€ºá€› á€•á€­á€¯á€œá€½á€šá€º**á€•á€«á€á€šá€ºá‹

---

## 12. Residual Connection â€” Gradient á€œá€™á€ºá€¸á€€á€¼á€±á€¬á€„á€ºá€¸

**Residual (Skip) Connection** á€Ÿá€¬ input á€€á€­á€¯ layer output á€”á€²á€· **á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€º**á€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

$$\text{output} = x + \text{SubLayer}(x)$$

### 12.1 á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Residual Connection á€œá€­á€¯á€œá€²?

```mermaid
graph LR
    subgraph Without_Residual ["Without Residual âŒ"]
        direction LR
        A1["x"] --> L1["Layer 1"]
        L1 --> L2["Layer 2"]
        L2 --> L3["...Layer 16"]
        L3 --> O1["output"]
    end

    subgraph With_Residual ["With Residual âœ…"]
        direction LR
        B1["x"] --> BL1["Layer 1"]
        BL1 --> PLUS1["+"]
        B1 --> PLUS1
        PLUS1 --> BL2["Layer 2"]
        BL2 --> PLUS2["+"]
        PLUS1 --> PLUS2
        PLUS2 --> O2["...output"]
    end

    style Without_Residual fill:#2d3436,stroke:#e17055,stroke-width:2px,color:#dfe6e9
    style With_Residual fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

| Problem | Without Residual | With Residual |
|:--------|:---:|:---:|
| **Vanishing Gradient** | Layer á€™á€»á€¬á€¸á€›á€„á€º gradient â†’ 0 á€–á€¼á€…á€ºá€á€á€º | Gradient shortcut á€› |
| **Training Difficulty** | Deep network train á€á€€á€º | Layer 100+ á€•á€²á€–á€¼á€…á€ºá€–á€¼á€…á€º train á€› |
| **Information Loss** | Layer á€á€½á€±á€€á€”á€± info á€•á€»á€±á€¬á€€á€ºá€á€á€º | Skip connection á€€á€”á€± original info á€‘á€­á€”á€ºá€¸ |

### 12.2 Residual Stream Perspective

Modern LLMs á€€á€­á€¯ **"residual stream"** (main highway) perspective á€”á€²á€· á€€á€¼á€Šá€·á€ºá€›á€„á€º:

```mermaid
graph LR
    X0["xâ‚€<br/>(embedding)"] --> |"+attnâ‚"| X1["xâ‚"]
    X1 --> |"+mlpâ‚"| X2["xâ‚‚"]
    X2 --> |"+attnâ‚‚"| X3["xâ‚ƒ"]
    X3 --> |"+mlpâ‚‚"| X4["xâ‚„"]
    X4 --> |"..."| XN["xâ‚ƒâ‚‚"]
    XN --> |"Norm"| OUT["logits"]

    style X0 fill:#2d3436,stroke:#00b894,color:#fff
    style XN fill:#2d3436,stroke:#e17055,color:#fff
```

> **Intuition:** Hidden state $x$ á€Ÿá€¬ "highway" á€•á€±á€«á€ºá€™á€¾á€¬ á€…á€®á€¸á€†á€„á€ºá€¸á€”á€±á€•á€¼á€®á€¸ layer á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€€ information á€€á€­á€¯ **á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€º** (additive update) á€•á€«á€á€šá€ºá‹ Layer á€€ information á€€á€­á€¯ **replace** á€™á€œá€¯á€•á€ºá€˜á€² **refine** á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

$$x_{l+1} = x_l + \text{Attn}(\text{Norm}(x_l)) + \text{MLP}(\text{Norm}(\ldots))$$

---

## 13. Output Layer & Token Generation

Decoder layers á€¡á€€á€¯á€”á€ºá€œá€¯á€¶á€¸ á€–á€¼á€á€ºá€•á€¼á€®á€¸á€á€²á€· hidden state á€€á€­á€¯ **vocabulary probabilities** á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ next token á€€á€­á€¯ á€›á€½á€±á€¸á€•á€«á€á€šá€ºá‹

### 13.1 Output Pipeline

```mermaid
graph LR
    H["Final Hidden State<br/>(batch, seq, d_model)"] --> NORM["Final<br/>RMSNorm"]
    NORM --> LM["lm_head (Linear)<br/>d_model â†’ vocab_size"]
    LM --> LOGITS["Logits<br/>(raw scores)"]
    LOGITS --> SM["Softmax"]
    SM --> PROB["Probabilities<br/>P(token | context)"]
    PROB --> SAMPLE["Sampling<br/>Strategy"]
    SAMPLE --> TOKEN["ğŸ¯ Next Token"]

    style H fill:#2d3436,stroke:#6c5ce7,color:#fff
    style TOKEN fill:#00b894,stroke:#fff,color:#fff
```

$$\text{logits} = W_{\text{lm\_head}} \cdot \text{RMSNorm}(h_{\text{final}}) \in \mathbb{R}^{|V|}$$

$$P(\text{token}_i) = \frac{e^{\text{logit}_i}}{\sum_j e^{\text{logit}_j}}$$

### 13.2 Sampling Strategies â€” Next Token á€›á€½á€±á€¸á€”á€Šá€ºá€¸á€™á€»á€¬á€¸

| Strategy | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | Use Case |
|:--------:|:-------------|:---------|
| **Greedy** | Probability á€¡á€™á€¼á€„á€·á€ºá€†á€¯á€¶á€¸ token á€€á€­á€¯á€•á€² á€›á€½á€±á€¸ | Deterministic, á€•á€¯á€¶á€á€± output |
| **Top-k** | Probability á€¡á€™á€¼á€„á€·á€ºá€†á€¯á€¶á€¸ k á€á€¯á€‘á€²á€€á€•á€² á€›á€½á€±á€¸ | Creative output with limit |
| **Top-p (Nucleus)** | Cumulative probability p% á€›á€±á€¬á€€á€ºá€á€²á€· tokens á€‘á€²á€€á€•á€² | Dynamic vocabulary size |
| **Temperature** | Probability distribution á€€á€­á€¯ sharp/flat á€œá€¯á€•á€º | T < 1: focused, T > 1: creative |

**Temperature á€›á€²á€· á€¡á€€á€»á€­á€¯á€¸á€á€€á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯:**

$$P_i = \frac{e^{\text{logit}_i / T}}{\sum_j e^{\text{logit}_j / T}}$$

| Temperature | Effect |
|:-----------:|:-------|
| T â†’ 0 | Greedy â€” probability highest token á€•á€² á€‘á€½á€€á€º |
| T = 1 | Original distribution â€” balanced |
| T > 1 | Flatter distribution â€” á€•á€­á€¯á€›á€½á€±á€¸á€…á€›á€¬ á€™á€»á€¬á€¸, á€•á€­á€¯creative |

### 13.3 Autoregressive Generation Process

```mermaid
sequenceDiagram
    participant User as User
    participant Model as Decoder Model
    participant Output as Generated Text

    User ->> Model: "The cat"
    Model ->> Model: Process "The cat" â†’ P(next)
    Model ->> Output: "sat" (highest prob)

    Note over Model: Now input = "The cat sat"
    Model ->> Model: Process "The cat sat" â†’ P(next)
    Model ->> Output: "on" (highest prob)

    Note over Model: Now input = "The cat sat on"
    Model ->> Model: Process "The cat sat on" â†’ P(next)
    Model ->> Output: "the" (highest prob)

    Note over Output: Final: "The cat sat on the..."
```

> Token **á€á€…á€ºá€á€¯á€•á€¼á€®á€¸á€á€…á€ºá€á€¯** (one-by-one) generate â†’ á€’á€«á€€á€¼á€±á€¬á€„á€·á€º **"Autoregressive"** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

---

## 14. KV Cache â€” Inference Optimization

Autoregressive generation á€™á€¾á€¬ token á€¡á€á€…á€ºá€á€…á€ºá€á€¯ generate á€œá€¯á€•á€ºá€á€­á€¯á€„á€ºá€¸ previous tokens á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ recompute á€œá€¯á€•á€ºá€›á€•á€«á€á€šá€ºá‹ **KV Cache** á€€ á€’á€® redundant computation á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

### 14.1 Problem â€” KV Cache á€™á€›á€¾á€­á€›á€„á€º

| Step | Input | Compute K,V for |
|:----:|:------|:------|
| 1 | "The" | The |
| 2 | "The cat" | **The**, cat (The á€‘á€•á€ºá€á€½á€€á€º) |
| 3 | "The cat sat" | **The, cat**, sat (á€‘á€•á€ºá€á€½á€€á€º) |
| 4 | "The cat sat on" | **The, cat, sat**, on (á€‘á€•á€ºá€‘á€•á€ºá€á€½á€€á€º) |

### 14.2 Solution â€” KV Cache á€›á€¾á€­á€›á€„á€º

```mermaid
sequenceDiagram
    participant Gen as Generator
    participant Cache as KV Cache
    participant Attn as Attention

    Note over Gen: Step 1: "The"
    Gen ->> Cache: Store Kâ‚, Vâ‚
    Gen ->> Attn: qâ‚ Ã— [Kâ‚] â†’ attention

    Note over Gen: Step 2: "cat"
    Gen ->> Cache: Append Kâ‚‚, Vâ‚‚ (Kâ‚ á€‘á€•á€ºá€™á€á€½á€€á€º)
    Gen ->> Attn: qâ‚‚ Ã— [Kâ‚, Kâ‚‚] â†’ attention

    Note over Gen: Step 3: "sat"
    Gen ->> Cache: Append Kâ‚ƒ, Vâ‚ƒ (Kâ‚,Kâ‚‚ á€‘á€•á€ºá€™á€á€½á€€á€º)
    Gen ->> Attn: qâ‚ƒ Ã— [Kâ‚, Kâ‚‚, Kâ‚ƒ] â†’ attention

    Note over Cache: Cache grows linearly with sequence!
```

> Previous tokens á€›á€²á€· K, V á€€á€­á€¯ **cache á€‘á€¬á€¸á€•á€¼á€®á€¸** new token á€›á€²á€· K, V á€€á€­á€¯á€•á€² compute â†’ **á€¡á€á€»á€­á€”á€º á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ á€á€€á€ºá€á€¬**

### 14.3 KV Cache Memory Formula

$$\text{Cache Size} = 2 \times L \times n_{kv} \times d_h \times S \times \text{bytes\_per\_element}$$

- $L$ = number of layers
- $n_{kv}$ = KV heads count
- $d_h$ = head dimension
- $S$ = sequence length

### 14.4 Cross-Model KV Cache Comparison (seq=4096, FP16)

| Model | KV Heads | Layers | Cache Size |
|:------|:--------:|:------:|:----------:|
| **Llama 3.2-1B** (GQA) | 8 | 16 | **64 MB** |
| **GPT-3 (175B)** (MHA) | 96 | 96 | **9,216 MB** |
| **Mistral 7B** (GQA) | 8 | 32 | **256 MB** |
| **Mistral 7B + SWA** | 8 | 32 | **~64 MB** |
| **DeepSeek-V2** (MLA) | compressed | 60 | **~60 MB** |

> **GQA á€›á€²á€· advantage:** KV heads á€€á€­á€¯ share á€œá€­á€¯á€· cache size **75%** á€œá€»á€¾á€±á€¬á€·á€á€»á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

## 15. Training â€” Model á€€á€­á€¯ á€˜á€šá€ºá€œá€­á€¯á€á€„á€ºá€•á€±á€¸á€œá€²

### 15.1 Pre-training â€” á€¡á€á€¼á€±á€á€¶á€á€„á€º

Internet á€•á€±á€«á€ºá€€ text (books, web pages, code) á€€á€­á€¯ **next token prediction** task á€”á€²á€· train:

$$\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t \mid x_1, ..., x_{t-1}; \theta)$$

```mermaid
graph TD
    subgraph Pretraining ["Pre-training"]
        direction TB
        D["ğŸ“š Massive Text Data<br/>(Trillions of tokens)"] --> M["Decoder Model"]
        M --> PRED["Next Token Prediction"]
        PRED --> LOSS["Cross-Entropy Loss"]
        LOSS --> |"Backpropagation"| M
    end

    subgraph SFT ["Supervised Fine-Tuning (SFT)"]
        direction TB
        INST["ğŸ“ Instruction-Response Pairs<br/>(Human-curated)"] --> M2["Pre-trained Model"]
        M2 --> RESP["Follow Instructions"]
    end

    subgraph RLHF ["RLHF / DPO"]
        direction TB
        PREF["ğŸ‘ğŸ‘ Human Preferences"] --> M3["SFT Model"]
        M3 --> ALIGN["Align with Human Values"]
    end

    Pretraining --> SFT --> RLHF

    style Pretraining fill:#1a1a2e,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style SFT fill:#1a1a2e,stroke:#fdcb6e,stroke-width:2px,color:#dfe6e9
    style RLHF fill:#1a1a2e,stroke:#00b894,stroke-width:2px,color:#dfe6e9
```

### 15.2 Training Stages

| Stage | Data | á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º | á€¥á€•á€™á€¬ |
|:------|:-----|:-------------|:------|
| **Pre-training** | Web text, Books, Code (Trillions of tokens) | Language understanding & generation | "The cat sat on the ___" â†’ "mat" |
| **SFT** | (Instruction, Response) pairs | Instructions á€€á€­á€¯ follow á€á€á€ºá€¡á€±á€¬á€„á€º | "Summarize this..." â†’ summary |
| **RLHF** | Human preference rankings | Human values á€”á€²á€· align | âŒ harmful content â† âœ… helpful content |
| **DPO** | Chosen vs Rejected pairs | RLHF á€‘á€€á€º á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€á€²á€· alignment | reward model á€™á€œá€­á€¯ |

### 15.3 Backpropagation â€” á€¡á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€†á€¯á€¶á€¸ á€›á€¾á€„á€ºá€¸á€•á€¼á€á€»á€€á€º

| Step | á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€»á€€á€º |
|:----:|:-------------|
| â‘  | Input text á€€á€­á€¯ model á€‘á€² á€–á€¼á€á€º â†’ predicted token á€› (Forward Pass) |
| â‘¡ | Predicted vs Actual token á€šá€¾á€‰á€º â†’ Loss (error) á€á€½á€€á€º |
| â‘¢ | Loss á€€á€”á€± gradient (á€˜á€šá€º weight á€€á€­á€¯ á€˜á€šá€ºá€œá€±á€¬á€€á€º á€•á€¼á€„á€ºá€›á€™á€œá€²) á€•á€¼á€”á€ºá€á€½á€€á€º (Backward Pass) |
| â‘£ | Weights á€€á€­á€¯ gradient á€¡á€á€­á€¯á€„á€ºá€¸ update (Optimizer â€” Adam, AdamW) |
| â‘¤ | Data batch á€¡á€á€…á€ºá€”á€²á€· â‘  á€€á€”á€± á€•á€¼á€”á€ºá€…á€œá€¯á€•á€º |

> **"Instruct" model** (e.g., Llama-3.2-1B-**Instruct**) á€†á€­á€¯á€á€¬ Pre-training + SFT + RLHF/DPO á€¡á€†á€„á€·á€ºá€á€½á€± á€¡á€€á€¯á€”á€º á€•á€¼á€®á€¸á€•á€¼á€®á€¸á€á€¬á€¸ model á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 16. Modern Decoder Models á€šá€¾á€‰á€ºá€á€»á€€á€º

### 16.1 Full Architecture Comparison

| Feature | Llama 3.2-1B | GPT-2 (124M) | GPT-3 (175B) | Mistral 7B | DeepSeek-V2 (236B) | Granite 3B |
|:--------|:---:|:---:|:---:|:---:|:---:|:---:|
| **Parameters** | 1.24B | 124M | 175B | 7.3B | 236B (21B active) | 3B |
| **Layers** | 16 | 12 | 96 | 32 | 60 | 32 |
| **Hidden Size** | 2,048 | 768 | 12,288 | 4,096 | 5,120 | 2,560 |
| **Attention** | GQA | MHA | MHA | GQA+SWA | MLA | GQA |
| **Q/KV Heads** | 32/8 | 12/12 | 96/96 | 32/8 | 128/â€” | 32/8 |
| **Pos Encoding** | RoPE | Learned | Learned | RoPE+SWA | RoPE+YaRN | RoPE |
| **FFN** | SwiGLU | GELU | GELU | SwiGLU | SwiGLU+MoE | SwiGLU |
| **Normalization** | RMSNorm | LayerNorm | LayerNorm | RMSNorm | RMSNorm | RMSNorm |
| **Norm Position** | Pre | Post | Pre | Pre | Pre | Pre |
| **Vocab Size** | 128K | 50K | 100K | 32K | 102K | 49K |
| **Max Context** | 131K | 1K | 2-32K | 32K | 128K | 4-128K |
| **Weight Tying** | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… |

### 16.2 Architecture Evolution Timeline

```mermaid
timeline
    title Transformer Decoder Architecture Evolution
    2017 : Original Transformer
         : Sinusoidal PE
         : LayerNorm (Post)
         : ReLU FFN, MHA
    2019 : GPT-2
         : Learned PE
         : LayerNorm (Post)
         : GELU FFN, MHA
    2020 : GPT-3
         : Learned PE
         : LayerNorm (Pre)
         : GELU FFN, MHA
    2023 : Llama 2
         : RoPE, RMSNorm (Pre)
         : SwiGLU, GQA
    2023 : Mistral 7B
         : RoPE + Sliding Window
         : SwiGLU, GQA
    2024 : DeepSeek-V2
         : RoPE + YaRN
         : SwiGLU + MoE, MLA
    2024 : Llama 3.2
         : RoPE (Î¸=500K)
         : SwiGLU, GQA
    2024 : Granite 3
         : RoPE, RMSNorm (Pre)
         : SwiGLU, GQA
```

### 16.3 Modern LLM Trends

```mermaid
graph TD
    subgraph Attention_Trend ["ğŸ¯ Attention Evolution"]
        MHA_T["MHA (GPT)"] --> GQA_T["GQA (Llama, Mistral)"]
        GQA_T --> MLA_T["MLA (DeepSeek)"]
        MHA_T --> SWA_T["+ Sliding Window (Mistral)"]
    end

    subgraph FFN_Trend ["âš¡ FFN Evolution"]
        RELU_T["ReLU"] --> GELU_T["GELU (GPT)"]
        GELU_T --> SWIGLU_T["SwiGLU (Modern)"]
        SWIGLU_T --> MOE_T["+ MoE (DeepSeek, Mixtral)"]
    end

    subgraph Norm_Trend ["ğŸ“ Normalization Evolution"]
        LN_T["LayerNorm Post (GPT-2)"] --> LN_PRE_T["LayerNorm Pre (GPT-3)"]
        LN_PRE_T --> RMS_T["RMSNorm Pre (Modern)"]
    end

    subgraph Pos_Trend ["ğŸ“ Position Encoding Evolution"]
        LEARN_T["Learned Absolute (GPT)"] --> ROPE_T["RoPE (Llama)"]
        ROPE_T --> ROPE_EXT["RoPE + Scaling (YaRN)"]
        ROPE_T --> ROPE_SWA["RoPE + SWA (Mistral)"]
    end

    style Attention_Trend fill:#1a1a2e,stroke:#e94560,stroke-width:2px,color:#dfe6e9
    style FFN_Trend fill:#1a1a2e,stroke:#00b894,stroke-width:2px,color:#dfe6e9
    style Norm_Trend fill:#1a1a2e,stroke:#0984e3,stroke-width:2px,color:#dfe6e9
    style Pos_Trend fill:#1a1a2e,stroke:#fdcb6e,stroke-width:2px,color:#dfe6e9
```

---

## 17. Key Takeaways

### 17.1 Decoder Model á€›á€²á€· Complete Forward Pass

Llama 3.2-1B-Instruct á€€á€­á€¯ á€¥á€•á€™á€¬ á€šá€°á€•á€¼á€®á€¸ â€” input á€€á€”á€± output á€‘á€­ step-by-step:

```mermaid
graph TD
    A["1ï¸âƒ£ Input: 'Hello world'"] --> B["2ï¸âƒ£ Tokenize â†’ [15496, 995]"]
    B --> C["3ï¸âƒ£ Embed â†’ vectors (2Ã—2048)"]
    C --> D["4ï¸âƒ£ + RoPE (position info)"]
    D --> E["5ï¸âƒ£ RMSNorm"]
    E --> F["6ï¸âƒ£ GQA Attention (causal masked)"]
    F --> G["7ï¸âƒ£ + Residual"]
    G --> H["8ï¸âƒ£ RMSNorm"]
    H --> I["9ï¸âƒ£ SwiGLU MLP"]
    I --> J["ğŸ”Ÿ + Residual"]
    J --> K{"Steps 5-10 Ã—16 layers"}
    K --> L["1ï¸âƒ£1ï¸âƒ£ Final RMSNorm"]
    L --> M["1ï¸âƒ£2ï¸âƒ£ lm_head â†’ logits (128,256)"]
    M --> N["1ï¸âƒ£3ï¸âƒ£ Softmax â†’ probabilities"]
    N --> O["1ï¸âƒ£4ï¸âƒ£ Sample â†’ next token"]

    style A fill:#0f3460,stroke:#e94560,color:#fff
    style O fill:#00b894,stroke:#fff,color:#fff
    style K fill:#0f3460,stroke:#fdcb6e,color:#fff
```

### 17.2 Mathematical Summary

$$\boxed{
\begin{aligned}
h_0 &= W_E[\text{tokens}] \\
\text{For layer } l &= 0, \ldots, L-1: \\
\quad \hat{h}_l &= \text{RMSNorm}(h_l) \\
\quad h_l' &= h_l + \text{GQA}(\hat{h}_l; \text{RoPE}) \\
\quad \hat{h}_l' &= \text{RMSNorm}(h_l') \\
\quad h_{l+1} &= h_l' + \text{SwiGLU}(\hat{h}_l') \\
\text{logits} &= W_{\text{lm\_head}} \cdot \text{RMSNorm}(h_L)
\end{aligned}
}$$

### 17.3 á€á€­á€á€„á€·á€ºá€á€²á€· Core Concepts Summary

| # | Concept | á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€á€²á€· á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º |
|:-:|:--------|:---|
| 1 | **Tokenization** | Text â†’ numbers (BPE) |
| 2 | **Embedding** | Numbers â†’ vectors (lookup table) |
| 3 | **Self-Attention** | Token á€á€½á€± á€¡á€á€»á€„á€ºá€¸á€á€»á€„á€ºá€¸ á€†á€€á€ºá€…á€•á€ºá€™á€¾á€¯ |
| 4 | **Causal Mask** | á€¡á€”á€¬á€‚á€á€º tokens á€€á€­á€¯ block |
| 5 | **Multi-Head** | Relationship á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸ á€–á€™á€ºá€¸ |
| 6 | **GQA** | KV heads share â†’ memory/speed á€€á€±á€¬á€„á€ºá€¸ |
| 7 | **RoPE** | Position info á€€á€­á€¯ rotation á€”á€²á€· encode |
| 8 | **SwiGLU** | Gated FFN â†’ dynamic feature selection |
| 9 | **RMSNorm** | Value normalize (LayerNorm á€‘á€€á€ºá€™á€¼á€”á€º) |
| 10 | **Residual Connection** | Gradient flow + info preservation |
| 11 | **KV Cache** | Inference time speed up |
| 12 | **Autoregressive** | Token-by-token generate |

### 17.4 á€†á€€á€ºá€œá€€á€ºá€œá€±á€·á€œá€¬á€›á€”á€º

- ğŸ““ [Llama 3.2-1B Analysis Notebook](llama_3_2_1B_instruct_analysis.ipynb) â€” Hands-on code analysis
- ğŸ““ [Decoder Model Analysis (Kaggle)](kaggle_tested/03-decoder-model-analysis.ipynb) â€” Kaggle GPU analysis
- ğŸ“– [Architecture Deep Dive](llama_3_2_1B_instruct_architecture_deep_dive.md) â€” Mathematical details

---

> **ğŸ“ Note:** á€’á€® document á€Ÿá€¬ Decoder-Only Transformer architecture á€€á€­á€¯ á€¡á€á€¼á€±á€á€¶á€€á€”á€± advanced level á€¡á€‘á€­ cover á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Hands-on practice á€¡á€á€½á€€á€º notebook files á€á€½á€±á€€á€­á€¯ á€–á€½á€„á€·á€ºá€•á€¼á€®á€¸ code á€á€½á€± run á€€á€¼á€Šá€·á€ºá€•á€«á‹

---

*Generated for learning purposes â€” Generative AI Decoder Model Study Guide*
