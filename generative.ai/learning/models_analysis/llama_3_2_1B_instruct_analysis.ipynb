{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52eeef0",
   "metadata": {},
   "source": [
    "# ğŸ¦™ Meta-Llama 3.2-1B-Instruct â€” Decoder Model Internal Structure Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Model:** `meta-llama/Llama-3.2-1B-Instruct`  \n",
    "**Architecture:** Decoder-Only Transformer (Causal Language Model)  \n",
    "**Parameters:** ~1.24 Billion  \n",
    "**Purpose:** á€’á€® notebook á€™á€¾á€¬ Llama 3.2-1B-Instruct model á€›á€²á€· decoder architecture á€¡á€á€½á€„á€ºá€¸á€•á€­á€¯á€„á€ºá€¸ structure á€€á€­á€¯ á€¡á€á€±á€¸á€…á€­á€á€ºá€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€œá€±á€·á€œá€¬á€•á€«á€™á€šá€ºá‹\n",
    "\n",
    "### ğŸ“‹ Table of Contents\n",
    "\n",
    "1. **Environment Setup & Model Loading**\n",
    "2. **High-Level Architecture Overview**\n",
    "3. **Configuration Analysis**\n",
    "4. **Tokenizer Deep Dive**\n",
    "5. **Embedding Layer Analysis**\n",
    "6. **Decoder Layer Structure**\n",
    "   - 6.1 Self-Attention Mechanism (Grouped-Query Attention)\n",
    "   - 6.2 RoPE (Rotary Position Embedding)\n",
    "   - 6.3 Feed-Forward Network (MLP â€” SwiGLU)\n",
    "   - 6.4 RMSNorm (Root Mean Square Layer Normalization)\n",
    "7. **Parameter Statistics & Memory Analysis**\n",
    "8. **Layer-by-Layer Weight Shape Inspection**\n",
    "9. **Attention Pattern Visualization**\n",
    "10. **Generation Pipeline & Inference Test**\n",
    "11. **Summary & Key Findings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb8bbe",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ğŸ”§ Environment Setup & Model Loading\n",
    "\n",
    "Kaggle á€•á€±á€«á€ºá€™á€¾á€¬ run á€–á€­á€¯á€· á€œá€­á€¯á€¡á€•á€ºá€á€²á€· libraries á€á€½á€± install á€œá€¯á€•á€ºá€•á€¼á€®á€¸ model á€€á€­á€¯ load á€•á€«á€™á€šá€ºá‹\n",
    "\n",
    "> **Kaggle Settings:** `Settings â†’ Accelerator â†’ GPU T4 x2` á€€á€­á€¯á€›á€½á€±á€¸á€•á€«á‹  \n",
    "> **Hugging Face Token:** Kaggle Secrets á€™á€¾á€¬ `HF_TOKEN` á€‘á€Šá€·á€ºá€‘á€¬á€¸á€•á€«á‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea522eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / Upgrade required packages\n",
    "!pip install -q transformers accelerate sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c22f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device      : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory      : {torch.cuda.get_device_properties(0).total_mem / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39037499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Hugging Face Authentication â”€â”€â”€\n",
    "# Kaggle á€™á€¾á€¬ Secrets > Add Secret > Label: HF_TOKEN, Value: your_token\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Method 1: Kaggle Secrets (Recommended) ---\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    print(\"âœ… HF Token loaded from Kaggle Secrets\")\n",
    "except Exception:\n",
    "    # --- Method 2: Environment Variable ---\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "    if hf_token:\n",
    "        print(\"âœ… HF Token loaded from environment variable\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No HF Token found. Set it in Kaggle Secrets or as env var.\")\n",
    "        hf_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Model & Tokenizer Loading â”€â”€â”€\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Config á€€á€­á€¯ á€á€®á€¸á€á€”á€·á€º load á€œá€¯á€•á€ºá€•á€«á€™á€šá€º (memory á€™á€á€¯á€¶á€¸á€˜á€² architecture á€€á€¼á€Šá€·á€ºá€–á€­á€¯á€·)\n",
    "config = AutoConfig.from_pretrained(MODEL_ID, token=hf_token)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token)\n",
    "\n",
    "# Full Model (float16 for memory efficiency)\n",
    "# NOTE: attn_implementation=\"eager\" is required to extract attention weights.\n",
    "# The default \"sdpa\" backend does not return attention matrices.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"\\nâœ… Model loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Device map : {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a3c90",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ğŸ—ï¸ High-Level Architecture Overview\n",
    "\n",
    "Llama 3.2-1B-Instruct á€›á€²á€· overall model structure á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«á€™á€šá€ºá‹  \n",
    "Decoder-Only Transformer á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º **Encoder á€™á€•á€«á€˜á€²** Decoder Layers á€á€½á€±á€”á€²á€·á€•á€² á€–á€½á€²á€·á€…á€Šá€ºá€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹\n",
    "\n",
    "```\n",
    "LlamaForCausalLM\n",
    "â”œâ”€â”€ model (LlamaModel)\n",
    "â”‚   â”œâ”€â”€ embed_tokens          â†’ Token Embedding Layer\n",
    "â”‚   â”œâ”€â”€ layers[0..N-1]        â†’ Stacked Decoder Layers\n",
    "â”‚   â”‚   â”œâ”€â”€ self_attn         â†’ Grouped-Query Attention (GQA)\n",
    "â”‚   â”‚   â”œâ”€â”€ mlp               â†’ SwiGLU Feed-Forward Network\n",
    "â”‚   â”‚   â”œâ”€â”€ input_layernorm   â†’ RMSNorm (Pre-Attention)\n",
    "â”‚   â”‚   â””â”€â”€ post_attention_layernorm â†’ RMSNorm (Pre-FFN)\n",
    "â”‚   â””â”€â”€ norm                  â†’ Final RMSNorm\n",
    "â””â”€â”€ lm_head                   â†’ Linear (Output Projection â†’ Vocabulary)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ad73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Full Model Architecture Print â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  LLAMA 3.2-1B-INSTRUCT â€” FULL ARCHITECTURE\")\n",
    "print(\"â•\" * 70)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefe62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Top-Level Module Names â”€â”€â”€\n",
    "print(\"\\nğŸ“¦ Top-Level Modules:\")\n",
    "print(\"-\" * 50)\n",
    "for name, module in model.named_children():\n",
    "    print(f\"  â”œâ”€â”€ {name}: {module.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Inside 'model' (LlamaModel):\")\n",
    "print(\"-\" * 50)\n",
    "for name, module in model.model.named_children():\n",
    "    if name == \"layers\":\n",
    "        print(f\"  â”œâ”€â”€ {name}: {module.__class__.__name__} ({len(module)} decoder layers)\")\n",
    "    else:\n",
    "        print(f\"  â”œâ”€â”€ {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853b360",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. âš™ï¸ Configuration Analysis\n",
    "\n",
    "Model á€›á€²á€· hyperparameters á€”á€²á€· architecture configuration á€€á€­á€¯ á€¡á€á€±á€¸á€…á€­á€á€ºá€€á€¼á€Šá€·á€ºá€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db02cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Model Configuration â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  MODEL CONFIGURATION\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "config_dict = config.to_dict()\n",
    "\n",
    "# Key architectural parameters\n",
    "key_params = [\n",
    "    (\"Model Type\", config_dict.get(\"model_type\")),\n",
    "    (\"Architecture\", config_dict.get(\"architectures\")),\n",
    "    (\"Hidden Size (d_model)\", config_dict.get(\"hidden_size\")),\n",
    "    (\"Intermediate Size (FFN)\", config_dict.get(\"intermediate_size\")),\n",
    "    (\"Num Hidden Layers\", config_dict.get(\"num_hidden_layers\")),\n",
    "    (\"Num Attention Heads (Q)\", config_dict.get(\"num_attention_heads\")),\n",
    "    (\"Num Key-Value Heads (KV)\", config_dict.get(\"num_key_value_heads\")),\n",
    "    (\"Head Dimension\", config_dict.get(\"head_dim\", config_dict.get(\"hidden_size\") // config_dict.get(\"num_attention_heads\"))),\n",
    "    (\"Vocab Size\", config_dict.get(\"vocab_size\")),\n",
    "    (\"Max Position Embeddings\", config_dict.get(\"max_position_embeddings\")),\n",
    "    (\"RMS Norm Epsilon\", config_dict.get(\"rms_norm_eps\")),\n",
    "    (\"Hidden Activation (FFN)\", config_dict.get(\"hidden_act\")),\n",
    "    (\"Rope Theta\", config_dict.get(\"rope_theta\")),\n",
    "    (\"Tie Word Embeddings\", config_dict.get(\"tie_word_embeddings\")),\n",
    "    (\"Use Cache\", config_dict.get(\"use_cache\")),\n",
    "    (\"Torch Dtype\", config_dict.get(\"torch_dtype\")),\n",
    "]\n",
    "\n",
    "for param_name, param_value in key_params:\n",
    "    print(f\"  {param_name:<35} : {param_value}\")\n",
    "\n",
    "# GQA ratio calculation\n",
    "n_heads = config_dict.get(\"num_attention_heads\")\n",
    "n_kv_heads = config_dict.get(\"num_key_value_heads\")\n",
    "gqa_ratio = n_heads // n_kv_heads\n",
    "print(f\"\\n  {'GQA Group Ratio':<35} : {gqa_ratio} (æ¯ {gqa_ratio} Q heads share 1 KV head)\")\n",
    "print(f\"  {'Attention Type':<35} : {'Grouped-Query Attention (GQA)' if n_kv_heads < n_heads else 'Multi-Head Attention (MHA)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b471c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ RoPE Scaling Configuration â”€â”€â”€\n",
    "print(\"\\nğŸ”„ RoPE (Rotary Position Embedding) Configuration:\")\n",
    "print(\"-\" * 50)\n",
    "rope_scaling = config_dict.get(\"rope_scaling\", None)\n",
    "if rope_scaling:\n",
    "    for key, value in rope_scaling.items():\n",
    "        print(f\"  {key:<30} : {value}\")\n",
    "else:\n",
    "    print(f\"  Base theta: {config_dict.get('rope_theta')}\")\n",
    "    print(\"  No additional scaling applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6758bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Full Config Dump (for reference) â”€â”€â”€\n",
    "import json\n",
    "print(\"\\nğŸ“„ Full Configuration JSON:\")\n",
    "print(json.dumps(config_dict, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744123f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ğŸ“ Tokenizer Deep Dive\n",
    "\n",
    "Llama 3.2 á€™á€¾á€¬ **BPE (Byte-Pair Encoding)** based tokenizer á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹  \n",
    "Tokenizer á€›á€²á€· vocabulary size, special tokens, encoding/decoding behavior á€á€½á€±á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Tokenizer Information â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  TOKENIZER ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "print(f\"  Tokenizer class       : {tokenizer.__class__.__name__}\")\n",
    "print(f\"  Vocabulary size       : {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Model max length      : {tokenizer.model_max_length:,}\")\n",
    "print(f\"  Padding side          : {tokenizer.padding_side}\")\n",
    "print(f\"  Truncation side       : {tokenizer.truncation_side}\")\n",
    "print(f\"  Is fast tokenizer     : {tokenizer.is_fast}\")\n",
    "\n",
    "print(\"\\nğŸ”‘ Special Tokens:\")\n",
    "print(\"-\" * 50)\n",
    "special_tokens = {\n",
    "    \"BOS (Begin of Sequence)\": (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "    \"EOS (End of Sequence)\": (tokenizer.eos_token, tokenizer.eos_token_id),\n",
    "    \"PAD (Padding)\": (tokenizer.pad_token, tokenizer.pad_token_id),\n",
    "    \"UNK (Unknown)\": (tokenizer.unk_token, getattr(tokenizer, 'unk_token_id', None)),\n",
    "}\n",
    "\n",
    "for name, (token, token_id) in special_tokens.items():\n",
    "    print(f\"  {name:<30} : token='{token}', id={token_id}\")\n",
    "\n",
    "# All special tokens\n",
    "print(\"\\nğŸ“‹ All Special Tokens:\")\n",
    "for i, token in enumerate(tokenizer.all_special_tokens[:20]):\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  [{i:>3}] id={token_id:<10} â†’ '{token}'\")\n",
    "if len(tokenizer.all_special_tokens) > 20:\n",
    "    print(f\"  ... and {len(tokenizer.all_special_tokens) - 20} more special tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd56dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Tokenization Examples â”€â”€â”€\n",
    "print(\"\\nğŸ”¤ Tokenization Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The capital of Myanmar is Naypyidaw.\",\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\",\n",
    "    \"ğŸ¦™ Llama model analysis!\",\n",
    "    \"á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«\",  # Burmese text\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(f\"\\n  Input   : '{text}'\")\n",
    "    print(f\"  Tokens  : {tokens}\")\n",
    "    print(f\"  IDs     : {token_ids}\")\n",
    "    print(f\"  Count   : {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Chat Template â”€â”€â”€\n",
    "print(\"\\nğŸ’¬ Chat Template:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(chat_text)\n",
    "\n",
    "chat_tokens = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "print(f\"\\nTotal tokens in chat prompt: {len(chat_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557a928",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ğŸ“Š Embedding Layer Analysis\n",
    "\n",
    "Model á€›á€²á€· input embedding layer á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«á€™á€šá€ºá‹  \n",
    "Llama 3.2 á€™á€¾á€¬ `embed_tokens` layer á€€á€á€¬ token embeddings á€€á€­á€¯ handle á€œá€¯á€•á€ºá€•á€¼á€®á€¸  \n",
    "**positional embeddings á€¡á€á€½á€€á€º RoPE** á€€á€­á€¯ attention layer á€‘á€²á€™á€¾á€¬ apply á€œá€¯á€•á€ºá€•á€«á€á€šá€º  \n",
    "(traditional absolute positional embedding á€™á€á€¯á€¶á€¸á€•á€«)á‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Embedding Layer Details â”€â”€â”€\n",
    "embed_layer = model.model.embed_tokens\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(\"  EMBEDDING LAYER ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "print(f\"  Layer type           : {embed_layer.__class__.__name__}\")\n",
    "print(f\"  Vocabulary size      : {embed_layer.num_embeddings:,}\")\n",
    "print(f\"  Embedding dimension  : {embed_layer.embedding_dim}\")\n",
    "print(f\"  Weight shape         : {embed_layer.weight.shape}\")\n",
    "print(f\"  Weight dtype         : {embed_layer.weight.dtype}\")\n",
    "print(f\"  Padding idx          : {embed_layer.padding_idx}\")\n",
    "\n",
    "# Parameter count\n",
    "embed_params = embed_layer.weight.numel()\n",
    "print(f\"  Total parameters     : {embed_params:,}\")\n",
    "print(f\"  Memory (FP16)        : {embed_params * 2 / 1e6:.2f} MB\")\n",
    "print(f\"  Memory (FP32)        : {embed_params * 4 / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc03c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Embedding Weight Statistics â”€â”€â”€\n",
    "embed_weight = embed_layer.weight.detach().float()\n",
    "\n",
    "print(\"\\nğŸ“Š Embedding Weight Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Mean               : {embed_weight.mean().item():.6f}\")\n",
    "print(f\"  Std                : {embed_weight.std().item():.6f}\")\n",
    "print(f\"  Min                : {embed_weight.min().item():.6f}\")\n",
    "print(f\"  Max                : {embed_weight.max().item():.6f}\")\n",
    "print(f\"  L2 Norm (avg/row)  : {embed_weight.norm(dim=1).mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52004761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Embedding Visualization: Weight Distribution â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Overall weight distribution\n",
    "axes[0].hist(embed_weight.cpu().numpy().flatten(), bins=200, color='steelblue', alpha=0.7, density=True)\n",
    "axes[0].set_title('Embedding Weight Distribution', fontsize=13)\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. L2 Norm per token\n",
    "norms = embed_weight.norm(dim=1).cpu().numpy()\n",
    "axes[1].hist(norms, bins=100, color='coral', alpha=0.7)\n",
    "axes[1].set_title('Token Embedding L2 Norms', fontsize=13)\n",
    "axes[1].set_xlabel('L2 Norm')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# 3. Heatmap of first 50 tokens x first 64 dims\n",
    "im = axes[2].imshow(embed_weight[:50, :64].cpu().numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[2].set_title('Embedding Heatmap (first 50 tokens Ã— 64 dims)', fontsize=13)\n",
    "axes[2].set_xlabel('Embedding Dimension')\n",
    "axes[2].set_ylabel('Token Index')\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b311c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Weight Tying Check: embed_tokens vs lm_head â”€â”€â”€\n",
    "print(\"\\nğŸ”— Weight Tying (embed_tokens â†” lm_head):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tie_flag = config_dict.get(\"tie_word_embeddings\", False)\n",
    "print(f\"  Config tie_word_embeddings : {tie_flag}\")\n",
    "\n",
    "if hasattr(model, 'lm_head') and model.lm_head.weight is not None:\n",
    "    same_data = model.lm_head.weight.data_ptr() == model.model.embed_tokens.weight.data_ptr()\n",
    "    print(f\"  Same memory (data_ptr)    : {same_data}\")\n",
    "    print(f\"  lm_head weight shape      : {model.lm_head.weight.shape}\")\n",
    "    print(f\"  embed_tokens weight shape  : {model.model.embed_tokens.weight.shape}\")\n",
    "    if same_data:\n",
    "        print(\"  âœ… Weights ARE tied â€” embed_tokens and lm_head share the same tensor\")\n",
    "    else:\n",
    "        print(\"  âŒ Weights are NOT tied â€” separate tensors\")\n",
    "else:\n",
    "    print(\"  lm_head not found or has no weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffef16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ğŸ” Decoder Layer Structure â€” Deep Dive\n",
    "\n",
    "Llama 3.2-1B-Instruct á€›á€²á€· decoder layer á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€›á€²á€· internal components á€á€½á€±á€€á€­á€¯ á€¡á€á€±á€¸á€…á€­á€á€ºá€á€½á€²á€€á€¼á€Šá€·á€ºá€•á€«á€™á€šá€ºá‹  \n",
    "Decoder Layer á€á€…á€ºá€á€¯á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬:\n",
    "\n",
    "1. **RMSNorm** (Pre-Attention Normalization)\n",
    "2. **Grouped-Query Self-Attention (GQA)**\n",
    "3. **RMSNorm** (Pre-FFN Normalization)\n",
    "4. **SwiGLU MLP** (Feed-Forward Network)\n",
    "5. **Residual Connections** (Add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9697911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Single Decoder Layer Breakdown â”€â”€â”€\n",
    "decoder_layers = model.model.layers\n",
    "num_layers = len(decoder_layers)\n",
    "layer_0 = decoder_layers[0]  # First decoder layer\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(f\"  DECODER LAYER STRUCTURE (Total: {num_layers} layers)\")\n",
    "print(\"â•\" * 70)\n",
    "print(f\"\\nğŸ“ Layer 0 Architecture:\")\n",
    "print(layer_0)\n",
    "\n",
    "print(f\"\\n\\nğŸ“¦ Sub-modules in each Decoder Layer:\")\n",
    "print(\"-\" * 50)\n",
    "for name, module in layer_0.named_children():\n",
    "    print(f\"  â”œâ”€â”€ {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0e144",
   "metadata": {},
   "source": [
    "### 6.1 ğŸ§  Self-Attention Mechanism â€” Grouped-Query Attention (GQA)\n",
    "\n",
    "Llama 3.2 á€™á€¾á€¬ **Grouped-Query Attention (GQA)** á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€º:\n",
    "- **Multi-Head Attention (MHA):** Q, K, V heads á€¡á€›á€±á€¡á€á€½á€€á€º á€¡á€á€°á€á€°\n",
    "- **Multi-Query Attention (MQA):** KV head 1 á€á€¯á€•á€²\n",
    "- **Grouped-Query Attention (GQA):** Q heads á€€á€­á€¯ groups á€á€½á€²á€•á€¼á€®á€¸ group á€á€…á€ºá€á€¯á€…á€®á€€á€­á€¯ KV head 1 á€á€¯á€”á€²á€· share\n",
    "\n",
    "GQA á€€á€¼á€±á€¬á€„á€·á€º KV cache memory á€€á€­á€¯ á€á€­á€á€­á€á€¬á€á€¬ á€œá€»á€¾á€±á€¬á€·á€á€»á€”á€­á€¯á€„á€ºá€•á€¼á€®á€¸ inference speed á€•á€­á€¯á€™á€¼á€”á€ºá€•á€«á€á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec109c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Attention Module Deep Dive â”€â”€â”€\n",
    "attn = layer_0.self_attn\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(\"  GROUPED-QUERY ATTENTION (GQA) ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "print(f\"  Attention class         : {attn.__class__.__name__}\")\n",
    "\n",
    "# Dimensions\n",
    "hidden_size = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads\n",
    "head_dim = hidden_size // num_heads\n",
    "\n",
    "print(f\"\\n  Hidden size (d_model)   : {hidden_size}\")\n",
    "print(f\"  Num Q heads             : {num_heads}\")\n",
    "print(f\"  Num KV heads            : {num_kv_heads}\")\n",
    "print(f\"  Head dimension          : {head_dim}\")\n",
    "print(f\"  GQA group size          : {num_heads // num_kv_heads}\")\n",
    "print(f\"  Q total dim             : {num_heads} Ã— {head_dim} = {num_heads * head_dim}\")\n",
    "print(f\"  K total dim             : {num_kv_heads} Ã— {head_dim} = {num_kv_heads * head_dim}\")\n",
    "print(f\"  V total dim             : {num_kv_heads} Ã— {head_dim} = {num_kv_heads * head_dim}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Projection Weight Shapes:\")\n",
    "print(\"-\" * 50)\n",
    "for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:\n",
    "    proj = getattr(attn, proj_name)\n",
    "    w_shape = proj.weight.shape\n",
    "    has_bias = proj.bias is not None\n",
    "    params = proj.weight.numel() + (proj.bias.numel() if has_bias else 0)\n",
    "    print(f\"  {proj_name:<10} : weight={str(w_shape):<20} bias={str(has_bias):<8} params={params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ GQA vs MHA Comparison Diagram â”€â”€â”€\n",
    "print(\"\\nğŸ“Š GQA vs MHA â€” Memory Savings Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MHA (if all heads had separate KV)\n",
    "mha_kv_params = 2 * num_heads * head_dim * hidden_size  # K + V projections\n",
    "# GQA (shared KV heads)\n",
    "gqa_kv_params = 2 * num_kv_heads * head_dim * hidden_size\n",
    "savings = (1 - gqa_kv_params / mha_kv_params) * 100\n",
    "\n",
    "print(f\"  MHA KV projection params : {mha_kv_params:>12,}\")\n",
    "print(f\"  GQA KV projection params : {gqa_kv_params:>12,}\")\n",
    "print(f\"  Parameter savings        : {savings:.1f}%\")\n",
    "\n",
    "# KV Cache savings per token\n",
    "seq_len = 1024\n",
    "mha_kv_cache = 2 * num_heads * head_dim * seq_len * 2  # fp16\n",
    "gqa_kv_cache = 2 * num_kv_heads * head_dim * seq_len * 2\n",
    "print(f\"\\n  KV Cache per layer @ seq_len={seq_len} (FP16):\")\n",
    "print(f\"    MHA: {mha_kv_cache / 1e6:.2f} MB  |  GQA: {gqa_kv_cache / 1e6:.2f} MB  |  Savings: {(1 - gqa_kv_cache/mha_kv_cache)*100:.1f}%\")\n",
    "print(f\"  Total KV Cache ({num_layers} layers):\")\n",
    "print(f\"    MHA: {mha_kv_cache * num_layers / 1e6:.2f} MB  |  GQA: {gqa_kv_cache * num_layers / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Attention Weight Statistics per Layer â”€â”€â”€\n",
    "print(\"\\nğŸ“Š Q/K/V/O Projection â€” Weight Statistics Across All Layers:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Layer':<8} {'Proj':<8} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} {'Norm':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for layer_idx in [0, num_layers // 4, num_layers // 2, 3 * num_layers // 4, num_layers - 1]:\n",
    "    layer = decoder_layers[layer_idx]\n",
    "    for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:\n",
    "        w = getattr(layer.self_attn, proj_name).weight.detach().float()\n",
    "        print(f\"  L{layer_idx:<5} {proj_name:<8} {w.mean().item():>10.6f} {w.std().item():>10.6f} \"\n",
    "              f\"{w.min().item():>10.6f} {w.max().item():>10.6f} {w.norm().item():>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b0265",
   "metadata": {},
   "source": [
    "### 6.2 ğŸ”„ RoPE â€” Rotary Position Embedding\n",
    "\n",
    "Llama 3.2 á€™á€¾á€¬ absolute positional embedding á€¡á€…á€¬á€¸ **RoPE (Rotary Position Embedding)** á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹\n",
    "\n",
    "**RoPE á€›á€²á€· á€¡á€“á€­á€€ á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸:**\n",
    "- Position information á€€á€­á€¯ attention computation á€‘á€²á€™á€¾á€¬ rotation matrix á€¡á€”á€±á€”á€²á€· inject á€œá€¯á€•á€º\n",
    "- Relative position information á€€á€­á€¯ naturally encode á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º\n",
    "- Sequence length extrapolation á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸\n",
    "- Q, K vectors á€•á€±á€«á€ºá€™á€¾á€¬á€á€¬ apply (V á€•á€±á€«á€ºá€™á€¾á€¬ apply á€™á€œá€¯á€•á€º)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ RoPE Analysis â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  RoPE (ROTARY POSITION EMBEDDING) ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# rope_theta may be a top-level attribute or nested inside rope_scaling depending on transformers version\n",
    "rope_theta = config_dict.get(\"rope_theta\", None)\n",
    "if rope_theta is None:\n",
    "    rope_scaling = config_dict.get(\"rope_scaling\", {}) or {}\n",
    "    rope_theta = rope_scaling.get(\"rope_theta\", 500_000.0)  # Llama 3.2 default\n",
    "\n",
    "print(f\"  Base theta             : {rope_theta:,.0f}\")\n",
    "print(f\"  Head dimension         : {head_dim}\")\n",
    "print(f\"  Max position           : {config.max_position_embeddings:,}\")\n",
    "\n",
    "# Compute RoPE frequencies\n",
    "dim = head_dim\n",
    "freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "print(f\"\\n  Number of frequency pairs: {len(freqs)}\")\n",
    "print(f\"  Frequency range: [{freqs.min().item():.2e}, {freqs.max().item():.2e}]\")\n",
    "\n",
    "# Wavelengths\n",
    "wavelengths = 2 * np.pi / freqs.numpy()\n",
    "print(f\"  Wavelength range: [{wavelengths.min():.2f}, {wavelengths.max():.2f}] positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d70aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ RoPE Frequency Visualization â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# 1. Inverse Frequencies\n",
    "axes[0].plot(freqs.numpy(), 'b-o', markersize=3)\n",
    "axes[0].set_title('RoPE Inverse Frequencies', fontsize=13)\n",
    "axes[0].set_xlabel('Dimension Pair Index')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Wavelengths (in positions)\n",
    "axes[1].plot(wavelengths, 'r-o', markersize=3)\n",
    "axes[1].set_title('RoPE Wavelengths (in positions)', fontsize=13)\n",
    "axes[1].set_xlabel('Dimension Pair Index')\n",
    "axes[1].set_ylabel('Wavelength')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. RoPE rotation angles for different positions\n",
    "positions = torch.arange(0, 128)\n",
    "angles = torch.outer(positions.float(), freqs)\n",
    "im = axes[2].imshow(angles.numpy()[:64, :], aspect='auto', cmap='twilight')\n",
    "axes[2].set_title('RoPE Rotation Angles (pos Ã— dim_pair)', fontsize=13)\n",
    "axes[2].set_xlabel('Dimension Pair Index')\n",
    "axes[2].set_ylabel('Position')\n",
    "plt.colorbar(im, ax=axes[2], label='Angle (radians)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b0589",
   "metadata": {},
   "source": [
    "### 6.3 ğŸ§® Feed-Forward Network â€” SwiGLU MLP\n",
    "\n",
    "Llama 3.2 á€›á€²á€· FFN (Feed-Forward Network) á€™á€¾á€¬ **SwiGLU activation** á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹\n",
    "\n",
    "**Traditional FFN:**  \n",
    "$\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot x)$\n",
    "\n",
    "**SwiGLU FFN (Llama):**  \n",
    "$\\text{FFN}(x) = W_{\\text{down}} \\cdot (\\text{SiLU}(W_{\\text{gate}} \\cdot x) \\odot (W_{\\text{up}} \\cdot x))$\n",
    "\n",
    "where $\\text{SiLU}(x) = x \\cdot \\sigma(x)$ and $\\odot$ is element-wise multiplication.\n",
    "\n",
    "3 weight matrices á€•á€«á€á€„á€º: `gate_proj`, `up_proj`, `down_proj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ MLP / FFN Analysis â”€â”€â”€\n",
    "mlp = layer_0.mlp\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(\"  SwiGLU MLP (FEED-FORWARD NETWORK) ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "print(f\"  MLP class             : {mlp.__class__.__name__}\")\n",
    "print(f\"  Activation function   : {mlp.act_fn.__class__.__name__} (SiLU / Swish)\")\n",
    "\n",
    "print(f\"\\nğŸ“ MLP Projection Shapes:\")\n",
    "print(\"-\" * 60)\n",
    "for proj_name in ['gate_proj', 'up_proj', 'down_proj']:\n",
    "    proj = getattr(mlp, proj_name)\n",
    "    w_shape = proj.weight.shape\n",
    "    has_bias = proj.bias is not None\n",
    "    params = proj.weight.numel()\n",
    "    print(f\"  {proj_name:<12} : weight={str(w_shape):<25} bias={str(has_bias):<8} params={params:,}\")\n",
    "\n",
    "intermediate_size = config.intermediate_size\n",
    "expansion_ratio = intermediate_size / hidden_size\n",
    "print(f\"\\n  Hidden size          : {hidden_size}\")\n",
    "print(f\"  Intermediate size    : {intermediate_size}\")\n",
    "print(f\"  Expansion ratio      : {expansion_ratio:.2f}x\")\n",
    "\n",
    "# SwiGLU has 3 matrices instead of 2, effective ratio:\n",
    "effective_params = 3 * hidden_size * intermediate_size\n",
    "standard_params = 2 * hidden_size * (int(hidden_size * 4))  # standard 4x expansion with 2 matrices\n",
    "print(f\"\\n  SwiGLU MLP params (per layer) : {effective_params:,}\")\n",
    "print(f\"  Standard FFN params (4x, 2 matrices): {standard_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ SwiGLU Activation Visualization â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "x = torch.linspace(-5, 5, 500)\n",
    "\n",
    "# SiLU (Swish)\n",
    "silu = x * torch.sigmoid(x)\n",
    "axes[0].plot(x.numpy(), silu.numpy(), 'b-', linewidth=2, label='SiLU(x) = xÂ·Ïƒ(x)')\n",
    "axes[0].plot(x.numpy(), torch.relu(x).numpy(), 'r--', linewidth=1.5, alpha=0.6, label='ReLU(x)')\n",
    "axes[0].plot(x.numpy(), torch.nn.functional.gelu(x).numpy(), 'g--', linewidth=1.5, alpha=0.6, label='GELU(x)')\n",
    "axes[0].set_title('SiLU vs ReLU vs GELU', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# SiLU derivative\n",
    "x_grad = x.clone().requires_grad_(True)\n",
    "silu_grad = torch.autograd.grad(\n",
    "    (x_grad * torch.sigmoid(x_grad)).sum(), x_grad\n",
    ")[0]\n",
    "axes[1].plot(x.numpy(), silu_grad.detach().numpy(), 'b-', linewidth=2)\n",
    "axes[1].set_title('SiLU Derivative', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Gate mechanism illustration\n",
    "gate_vals = torch.sigmoid(x)\n",
    "axes[2].plot(x.numpy(), gate_vals.numpy(), 'purple', linewidth=2, label='Ïƒ(gate)')\n",
    "axes[2].fill_between(x.numpy(), gate_vals.numpy(), alpha=0.2, color='purple')\n",
    "axes[2].set_title('Gating Mechanism: Ïƒ(Wgate Â· x)', fontsize=13)\n",
    "axes[2].set_xlabel('Gate Input')\n",
    "axes[2].set_ylabel('Gate Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cfed0",
   "metadata": {},
   "source": [
    "### 6.4 ğŸ“ RMSNorm â€” Root Mean Square Layer Normalization\n",
    "\n",
    "Llama 3.2 á€™á€¾á€¬ standard LayerNorm á€¡á€…á€¬á€¸ **RMSNorm** á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹\n",
    "\n",
    "**LayerNorm:**  \n",
    "$\\text{LN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$\n",
    "\n",
    "**RMSNorm:**  \n",
    "$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma$\n",
    "\n",
    "**RMSNorm á€›á€²á€· á€¡á€¬á€¸á€á€¬á€á€»á€€á€º:**\n",
    "- Mean subtraction á€™á€œá€­á€¯á€á€²á€·á€¡á€á€½á€€á€º computation á€•á€­á€¯á€™á€¼á€”á€º\n",
    "- Bias term ($\\beta$) á€™á€•á€«\n",
    "- Re-centering á€‘á€€á€º re-scaling á€€á€­á€¯á€•á€² focus á€œá€¯á€•á€º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ RMSNorm Analysis â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  RMSNorm ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Pre-attention norm\n",
    "input_norm = layer_0.input_layernorm\n",
    "post_attn_norm = layer_0.post_attention_layernorm\n",
    "final_norm = model.model.norm\n",
    "\n",
    "print(f\"  Per-layer RMSNorm locations:\")\n",
    "print(f\"    1. input_layernorm       : {input_norm.__class__.__name__}  (Before Self-Attention)\")\n",
    "print(f\"    2. post_attention_layernorm: {post_attn_norm.__class__.__name__}  (Before MLP/FFN)\")\n",
    "print(f\"  Final RMSNorm              : {final_norm.__class__.__name__}  (After all decoder layers)\")\n",
    "\n",
    "print(f\"\\nğŸ“ RMSNorm Parameters:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Weight (gamma) shape   : {input_norm.weight.shape}\")\n",
    "print(f\"  Epsilon                : {input_norm.variance_epsilon}\")\n",
    "print(f\"  Learnable params       : {input_norm.weight.numel():,} (per norm layer)\")\n",
    "\n",
    "total_rmsnorm_count = num_layers * 2 + 1  # 2 per layer + 1 final\n",
    "total_rmsnorm_params = total_rmsnorm_count * input_norm.weight.numel()\n",
    "print(f\"  Total RMSNorm layers   : {total_rmsnorm_count}\")\n",
    "print(f\"  Total RMSNorm params   : {total_rmsnorm_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69986512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ RMSNorm Gamma (Weight) Distribution Across Layers â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Collect gamma values\n",
    "input_norm_gammas = []\n",
    "post_attn_norm_gammas = []\n",
    "for i in range(num_layers):\n",
    "    input_norm_gammas.append(decoder_layers[i].input_layernorm.weight.detach().float().cpu().numpy())\n",
    "    post_attn_norm_gammas.append(decoder_layers[i].post_attention_layernorm.weight.detach().float().cpu().numpy())\n",
    "\n",
    "input_norm_gammas = np.array(input_norm_gammas)\n",
    "post_attn_norm_gammas = np.array(post_attn_norm_gammas)\n",
    "\n",
    "# Heatmap â€” input_layernorm\n",
    "im1 = axes[0].imshow(input_norm_gammas, aspect='auto', cmap='viridis')\n",
    "axes[0].set_title('input_layernorm Î³ (Pre-Attention)', fontsize=13)\n",
    "axes[0].set_xlabel('Hidden Dimension')\n",
    "axes[0].set_ylabel('Layer Index')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Heatmap â€” post_attention_layernorm\n",
    "im2 = axes[1].imshow(post_attn_norm_gammas, aspect='auto', cmap='viridis')\n",
    "axes[1].set_title('post_attention_layernorm Î³ (Pre-FFN)', fontsize=13)\n",
    "axes[1].set_xlabel('Hidden Dimension')\n",
    "axes[1].set_ylabel('Layer Index')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a91f26",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ğŸ“ˆ Parameter Statistics & Memory Analysis\n",
    "\n",
    "Model á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€›á€²á€· parameter count, memory footprint, component-wise breakdown á€á€½á€±á€€á€­á€¯ á€á€½á€€á€ºá€á€»á€€á€ºá€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4959f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Total Parameter Count â”€â”€â”€\n",
    "print(\"â•\" * 70)\n",
    "print(\"  PARAMETER STATISTICS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable = total_params - trainable_params\n",
    "\n",
    "print(f\"  Total parameters       : {total_params:>15,}  ({total_params/1e9:.3f} B)\")\n",
    "print(f\"  Trainable parameters   : {trainable_params:>15,}  ({trainable_params/1e9:.3f} B)\")\n",
    "print(f\"  Non-trainable params   : {non_trainable:>15,}\")\n",
    "\n",
    "# Memory estimation\n",
    "print(f\"\\nğŸ’¾ Memory Estimation:\")\n",
    "print(\"-\" * 50)\n",
    "for dtype_name, bytes_per_param in [(\"FP32\", 4), (\"FP16/BF16\", 2), (\"INT8\", 1), (\"INT4\", 0.5)]:\n",
    "    mem_gb = total_params * bytes_per_param / 1e9\n",
    "    print(f\"  {dtype_name:<12} : {mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Component-wise Parameter Breakdown â”€â”€â”€\n",
    "print(\"\\nğŸ“Š Component-wise Parameter Breakdown:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "component_params = OrderedDict()\n",
    "\n",
    "# Embedding\n",
    "embed_p = sum(p.numel() for p in model.model.embed_tokens.parameters())\n",
    "component_params['Embedding (embed_tokens)'] = embed_p\n",
    "\n",
    "# Per-layer breakdown (using layer 0 as representative)\n",
    "attn_params_per_layer = sum(p.numel() for p in layer_0.self_attn.parameters())\n",
    "mlp_params_per_layer = sum(p.numel() for p in layer_0.mlp.parameters())\n",
    "norm_params_per_layer = (\n",
    "    sum(p.numel() for p in layer_0.input_layernorm.parameters()) +\n",
    "    sum(p.numel() for p in layer_0.post_attention_layernorm.parameters())\n",
    ")\n",
    "\n",
    "component_params[f'Self-Attention (Ã—{num_layers})'] = attn_params_per_layer * num_layers\n",
    "component_params[f'MLP/FFN (Ã—{num_layers})'] = mlp_params_per_layer * num_layers\n",
    "component_params[f'RMSNorm (Ã—{num_layers}Ã—2 + 1)'] = norm_params_per_layer * num_layers + sum(p.numel() for p in model.model.norm.parameters())\n",
    "\n",
    "# lm_head\n",
    "if not config.tie_word_embeddings:\n",
    "    lm_head_p = sum(p.numel() for p in model.lm_head.parameters())\n",
    "    component_params['LM Head (lm_head)'] = lm_head_p\n",
    "else:\n",
    "    component_params['LM Head (tied with embedding)'] = 0\n",
    "\n",
    "for name, count in component_params.items():\n",
    "    pct = count / total_params * 100\n",
    "    bar = 'â–ˆ' * int(pct / 2) + 'â–‘' * (50 - int(pct / 2))\n",
    "    print(f\"  {name:<35} : {count:>12,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n  {'TOTAL':<35} : {total_params:>12,}\")\n",
    "\n",
    "# Sub-breakdown per layer\n",
    "print(f\"\\n  ğŸ“ Per Decoder Layer:\")\n",
    "print(f\"    Self-Attention : {attn_params_per_layer:>10,} ({attn_params_per_layer/1e6:.2f}M)\")\n",
    "print(f\"    MLP/FFN        : {mlp_params_per_layer:>10,} ({mlp_params_per_layer/1e6:.2f}M)\")\n",
    "print(f\"    RMSNorm (Ã—2)   : {norm_params_per_layer:>10,} ({norm_params_per_layer/1e6:.4f}M)\")\n",
    "layer_total = attn_params_per_layer + mlp_params_per_layer + norm_params_per_layer\n",
    "print(f\"    Layer Total    : {layer_total:>10,} ({layer_total/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3297da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Parameter Distribution Pie Chart â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Overall component distribution\n",
    "labels = [k for k, v in component_params.items() if v > 0]\n",
    "sizes = [v for v in component_params.values() if v > 0]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "explode = [0.05] * len(sizes)\n",
    "\n",
    "axes[0].pie(sizes, explode=explode, labels=labels, colors=colors[:len(sizes)],\n",
    "            autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "            textprops={'fontsize': 9})\n",
    "axes[0].set_title('Parameter Distribution by Component', fontsize=14)\n",
    "\n",
    "# 2. Per-layer breakdown\n",
    "layer_labels = ['Self-Attention\\n(Q,K,V,O proj)', 'MLP/FFN\\n(gate,up,down)', 'RMSNorm\\n(Ã—2)']\n",
    "layer_sizes = [attn_params_per_layer, mlp_params_per_layer, norm_params_per_layer]\n",
    "layer_colors = ['#FF6B6B', '#4ECDC4', '#96CEB4']\n",
    "\n",
    "axes[1].pie(layer_sizes, labels=layer_labels, colors=layer_colors,\n",
    "            autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "            textprops={'fontsize': 10})\n",
    "axes[1].set_title('Per Decoder Layer â€” Parameter Breakdown', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c029ada",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ğŸ”¬ Layer-by-Layer Weight Shape Inspection\n",
    "\n",
    "Model á€›á€²á€· named parameters á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ layer-by-layer á€…á€…á€ºá€†á€±á€¸á€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f518ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ All Named Parameters â”€â”€â”€\n",
    "print(\"â•\" * 90)\n",
    "print(\"  ALL NAMED PARAMETERS â€” SHAPES & SIZES\")\n",
    "print(\"â•\" * 90)\n",
    "print(f\"{'Name':<60} {'Shape':<25} {'Params':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "prev_layer = None\n",
    "for name, param in model.named_parameters():\n",
    "    # Group by layer\n",
    "    parts = name.split('.')\n",
    "    if 'layers' in parts:\n",
    "        layer_idx = parts[parts.index('layers') + 1]\n",
    "        if layer_idx != prev_layer:\n",
    "            if prev_layer is not None and int(layer_idx) > 1:\n",
    "                print(f\"  {'... (similar structure for layers 1-' + str(int(layer_idx)-1) + ')':<60}\")\n",
    "            if int(layer_idx) == 0 or int(layer_idx) == num_layers - 1:\n",
    "                prev_layer = layer_idx\n",
    "            else:\n",
    "                prev_layer = layer_idx\n",
    "                continue\n",
    "        elif int(layer_idx) not in [0, num_layers - 1]:\n",
    "            continue\n",
    "\n",
    "    print(f\"  {name:<60} {str(list(param.shape)):<25} {param.numel():>12,}\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"  {'TOTAL':<60} {'':<25} {total_params:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Weight Norm Across Layers â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Collect norms per layer\n",
    "q_norms, k_norms, v_norms, o_norms = [], [], [], []\n",
    "gate_norms, up_norms, down_norms = [], [], []\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer = decoder_layers[i]\n",
    "    q_norms.append(layer.self_attn.q_proj.weight.detach().float().norm().item())\n",
    "    k_norms.append(layer.self_attn.k_proj.weight.detach().float().norm().item())\n",
    "    v_norms.append(layer.self_attn.v_proj.weight.detach().float().norm().item())\n",
    "    o_norms.append(layer.self_attn.o_proj.weight.detach().float().norm().item())\n",
    "    gate_norms.append(layer.mlp.gate_proj.weight.detach().float().norm().item())\n",
    "    up_norms.append(layer.mlp.up_proj.weight.detach().float().norm().item())\n",
    "    down_norms.append(layer.mlp.down_proj.weight.detach().float().norm().item())\n",
    "\n",
    "x = range(num_layers)\n",
    "\n",
    "# Attention projections\n",
    "axes[0, 0].plot(x, q_norms, 'r-o', markersize=4, label='Q')\n",
    "axes[0, 0].plot(x, k_norms, 'g-s', markersize=4, label='K')\n",
    "axes[0, 0].plot(x, v_norms, 'b-^', markersize=4, label='V')\n",
    "axes[0, 0].plot(x, o_norms, 'm-D', markersize=4, label='O')\n",
    "axes[0, 0].set_title('Attention Projection Weight Norms', fontsize=13)\n",
    "axes[0, 0].set_xlabel('Layer')\n",
    "axes[0, 0].set_ylabel('Frobenius Norm')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MLP projections\n",
    "axes[0, 1].plot(x, gate_norms, 'r-o', markersize=4, label='gate_proj')\n",
    "axes[0, 1].plot(x, up_norms, 'g-s', markersize=4, label='up_proj')\n",
    "axes[0, 1].plot(x, down_norms, 'b-^', markersize=4, label='down_proj')\n",
    "axes[0, 1].set_title('MLP Projection Weight Norms', fontsize=13)\n",
    "axes[0, 1].set_xlabel('Layer')\n",
    "axes[0, 1].set_ylabel('Frobenius Norm')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight std per layer\n",
    "q_stds = [decoder_layers[i].self_attn.q_proj.weight.detach().float().std().item() for i in range(num_layers)]\n",
    "mlp_stds = [decoder_layers[i].mlp.gate_proj.weight.detach().float().std().item() for i in range(num_layers)]\n",
    "axes[1, 0].plot(x, q_stds, 'r-o', markersize=4, label='Q proj std')\n",
    "axes[1, 0].plot(x, mlp_stds, 'b-s', markersize=4, label='gate_proj std')\n",
    "axes[1, 0].set_title('Weight Standard Deviation Across Layers', fontsize=13)\n",
    "axes[1, 0].set_xlabel('Layer')\n",
    "axes[1, 0].set_ylabel('Std')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count per layer\n",
    "layer_param_counts = []\n",
    "for i in range(num_layers):\n",
    "    layer_p = sum(p.numel() for p in decoder_layers[i].parameters())\n",
    "    layer_param_counts.append(layer_p)\n",
    "\n",
    "axes[1, 1].bar(x, [p / 1e6 for p in layer_param_counts], color='steelblue', alpha=0.8)\n",
    "axes[1, 1].set_title('Parameters per Decoder Layer (Millions)', fontsize=13)\n",
    "axes[1, 1].set_xlabel('Layer')\n",
    "axes[1, 1].set_ylabel('Params (M)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837b0cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ğŸ‘ï¸ Attention Pattern Visualization\n",
    "\n",
    "Model á€€á€­á€¯ actual text input á€•á€±á€¸á€•á€¼á€®á€¸ attention weights (patterns) á€€á€­á€¯ visualize á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹  \n",
    "Decoder model á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º **causal (lower-triangular) attention mask** á€€á€­á€¯ á€á€¯á€¶á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Extract Attention Weights â”€â”€â”€\n",
    "test_input = \"Artificial intelligence is transforming the world of technology and science.\"\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "tokens_list = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "print(f\"Input text  : '{test_input}'\")\n",
    "print(f\"Token count : {len(tokens_list)}\")\n",
    "print(f\"Tokens      : {tokens_list}\")\n",
    "\n",
    "# Forward pass with attention outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "# outputs.attentions: tuple of (batch, num_heads, seq_len, seq_len) for each layer\n",
    "print(f\"\\nNumber of attention layers returned: {len(outputs.attentions)}\")\n",
    "print(f\"Attention shape per layer: {outputs.attentions[0].shape}\")\n",
    "print(f\"  â†’ (batch_size, num_heads, seq_len, seq_len)\")\n",
    "\n",
    "# Hidden states\n",
    "print(f\"\\nNumber of hidden states: {len(outputs.hidden_states)} (input_embed + {num_layers} layers)\")\n",
    "print(f\"Hidden state shape: {outputs.hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Attention Heatmaps: First & Last Layers â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "\n",
    "for row, layer_idx in enumerate([0, num_layers - 1]):\n",
    "    attn_weights = outputs.attentions[layer_idx][0].detach().float().cpu()  # (num_heads, seq, seq)\n",
    "\n",
    "    for col, head_idx in enumerate([0, 1, 2, 3]):\n",
    "        if head_idx < attn_weights.shape[0]:\n",
    "            ax = axes[row, col]\n",
    "            im = ax.imshow(attn_weights[head_idx].numpy(), cmap='Blues', vmin=0)\n",
    "            ax.set_title(f'Layer {layer_idx}, Head {head_idx}', fontsize=11)\n",
    "            ax.set_xticks(range(len(tokens_list)))\n",
    "            ax.set_yticks(range(len(tokens_list)))\n",
    "\n",
    "            display_tokens = [t[:10] for t in tokens_list]\n",
    "            ax.set_xticklabels(display_tokens, rotation=90, fontsize=7)\n",
    "            ax.set_yticklabels(display_tokens, fontsize=7)\n",
    "            plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "fig.suptitle('Attention Patterns â€” First Layer (top) vs Last Layer (bottom)', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Average Attention Pattern Across Heads per Layer â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "\n",
    "# Select 8 evenly spaced layers\n",
    "layer_indices = np.linspace(0, num_layers - 1, 8, dtype=int)\n",
    "\n",
    "for idx, layer_idx in enumerate(layer_indices):\n",
    "    row, col = idx // 4, idx % 4\n",
    "    attn_weights = outputs.attentions[layer_idx][0].detach().float().cpu()\n",
    "    avg_attn = attn_weights.mean(dim=0)  # Average across heads\n",
    "\n",
    "    ax = axes[row, col]\n",
    "    im = ax.imshow(avg_attn.numpy(), cmap='Blues', vmin=0)\n",
    "    ax.set_title(f'Layer {layer_idx} (Avg over {attn_weights.shape[0]} heads)', fontsize=11)\n",
    "\n",
    "    display_tokens = [t[:10] for t in tokens_list]\n",
    "    ax.set_xticks(range(len(tokens_list)))\n",
    "    ax.set_yticks(range(len(tokens_list)))\n",
    "    ax.set_xticklabels(display_tokens, rotation=90, fontsize=7)\n",
    "    ax.set_yticklabels(display_tokens, fontsize=7)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "fig.suptitle('Average Attention Patterns Across Layers', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Hidden State Norms Across Layers â”€â”€â”€\n",
    "hidden_norms = []\n",
    "for h in outputs.hidden_states:\n",
    "    # h shape: (batch, seq_len, hidden_size)\n",
    "    norm = h[0].detach().float().norm(dim=-1).mean().cpu().item()\n",
    "    hidden_norms.append(norm)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(range(len(hidden_norms)), hidden_norms, 'o-', color='steelblue', markersize=6)\n",
    "ax.set_title('Hidden State L2 Norm Across Layers', fontsize=14)\n",
    "ax.set_xlabel('Layer (0 = embedding output)')\n",
    "ax.set_ylabel('Average L2 Norm')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, len(hidden_norms), max(1, len(hidden_norms) // 16)))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8623985",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ğŸš€ Generation Pipeline & Inference Test\n",
    "\n",
    "Model á€€á€­á€¯ actual text generation task á€•á€±á€¸á€•á€¼á€®á€¸ inference behavior á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ff58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Text Generation Tests â”€â”€â”€\n",
    "def generate_response(messages, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate response using chat template.\"\"\"\n",
    "    chat_out = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    input_ids = chat_out[\"input_ids\"].to(model.device)\n",
    "    attention_mask = chat_out.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated part\n",
    "    generated = output_ids[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    return response, len(generated)\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(\"  GENERATION TESTS\")\n",
    "print(\"â•\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ada5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Test 1: General Knowledge â”€â”€â”€\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a transformer model in deep learning? Explain in 3 sentences.\"},\n",
    "]\n",
    "\n",
    "response, num_tokens = generate_response(messages, max_new_tokens=150)\n",
    "print(f\"\\nğŸ§ª Test 1 â€” General Knowledge:\")\n",
    "print(f\"Q: What is a transformer model in deep learning?\")\n",
    "print(f\"A: {response}\")\n",
    "print(f\"   (Generated {num_tokens} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Test 2: Code Generation â”€â”€â”€\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Python coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to check if a number is prime.\"},\n",
    "]\n",
    "\n",
    "response, num_tokens = generate_response(messages, max_new_tokens=200)\n",
    "print(f\"\\nğŸ§ª Test 2 â€” Code Generation:\")\n",
    "print(f\"Q: Write a Python function to check if a number is prime.\")\n",
    "print(f\"A: {response}\")\n",
    "print(f\"   (Generated {num_tokens} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Test 3: Reasoning â”€â”€â”€\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and give away 1, then buy 5 more, how many apples do I have?\"},\n",
    "]\n",
    "\n",
    "response, num_tokens = generate_response(messages, max_new_tokens=100)\n",
    "print(f\"\\nğŸ§ª Test 3 â€” Reasoning:\")\n",
    "print(f\"Q: If I have 3 apples and give away 1, then buy 5 more, how many?\")\n",
    "print(f\"A: {response}\")\n",
    "print(f\"   (Generated {num_tokens} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d758318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Token Probability Analysis â”€â”€â”€\n",
    "print(\"\\nğŸ“Š Next-Token Probability Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_prompt = \"The meaning of life is\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]  # Last token's logits\n",
    "    probs = torch.softmax(logits.float(), dim=-1)\n",
    "\n",
    "# Top-20 predictions\n",
    "top_k = 20\n",
    "top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(f\"\\nPrompt: '{test_prompt}'\")\n",
    "print(f\"\\nTop-{top_k} Next Token Predictions:\")\n",
    "print(f\"{'Rank':<6} {'Token':<20} {'Probability':>12} {'Log Prob':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(top_k):\n",
    "    token = tokenizer.decode(top_indices[i].item())\n",
    "    prob = top_probs[i].item()\n",
    "    log_prob = torch.log(top_probs[i]).item()\n",
    "    bar = 'â–ˆ' * int(prob * 100)\n",
    "    print(f\"  {i+1:<4} '{token}'{'':>{18-len(token)}} {prob:>10.4f}   {log_prob:>8.4f}  {bar}\")\n",
    "\n",
    "# Entropy\n",
    "entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
    "print(f\"\\n  Entropy of distribution: {entropy:.4f} nats\")\n",
    "print(f\"  Perplexity: {np.exp(entropy):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c360c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. ğŸ“‹ Summary & Key Findings\n",
    "\n",
    "Llama 3.2-1B-Instruct model á€›á€²á€· decoder architecture á€€á€­á€¯ analysis á€œá€¯á€•á€ºá€•á€¼á€®á€¸á€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€· key findings á€á€½á€±á€€á€­á€¯ á€¡á€”á€¾á€…á€ºá€á€»á€¯á€•á€ºá€•á€«á€™á€šá€ºá‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Architecture Summary Table â”€â”€â”€\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘\" + \"  LLAMA 3.2-1B-INSTRUCT â€” ARCHITECTURE SUMMARY\".center(68) + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 68 + \"â•£\")\n",
    "\n",
    "summary = [\n",
    "    (\"Model Type\", \"Decoder-Only Transformer (Causal LM)\"),\n",
    "    (\"Total Parameters\", f\"{total_params:,} ({total_params/1e9:.3f}B)\"),\n",
    "    (\"Hidden Size\", f\"{hidden_size}\"),\n",
    "    (\"Decoder Layers\", f\"{num_layers}\"),\n",
    "    (\"Attention Heads (Q)\", f\"{num_heads}\"),\n",
    "    (\"KV Heads\", f\"{num_kv_heads} (GQA ratio: {num_heads//num_kv_heads}:1)\"),\n",
    "    (\"Head Dimension\", f\"{head_dim}\"),\n",
    "    (\"FFN Intermediate Size\", f\"{intermediate_size} ({expansion_ratio:.2f}x expansion)\"),\n",
    "    (\"Vocabulary Size\", f\"{config.vocab_size:,}\"),\n",
    "    (\"Max Context Length\", f\"{config.max_position_embeddings:,}\"),\n",
    "    (\"Attention Type\", \"Grouped-Query Attention (GQA)\"),\n",
    "    (\"Position Encoding\", f\"RoPE (theta={rope_theta:,.0f})\"),\n",
    "    (\"FFN Activation\", \"SwiGLU (SiLU gate)\"),\n",
    "    (\"Normalization\", f\"RMSNorm (eps={config.rms_norm_eps})\"),\n",
    "    (\"Weight Tying\", f\"{config.tie_word_embeddings}\"),\n",
    "]\n",
    "\n",
    "for key, value in summary:\n",
    "    print(f\"â•‘  {key:<28} â”‚ {value:<37} â•‘\")\n",
    "\n",
    "print(\"â• \" + \"â•\" * 68 + \"â•£\")\n",
    "print(\"â•‘\" + \"  DATA FLOW (per decoder layer)\".center(68) + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 68 + \"â•£\")\n",
    "\n",
    "flow_steps = [\n",
    "    \"1. Input x\",\n",
    "    \"2. residual = x\",\n",
    "    \"3. x = RMSNorm(x)                    [input_layernorm]\",\n",
    "    \"4. x = GQA_SelfAttention(x)           [Q,K,V proj â†’ RoPE â†’ Attn â†’ O proj]\",\n",
    "    \"5. x = x + residual                   [Residual Connection #1]\",\n",
    "    \"6. residual = x\",\n",
    "    \"7. x = RMSNorm(x)                    [post_attention_layernorm]\",\n",
    "    \"8. x = SwiGLU_MLP(x)                 [gateâ†‘upâ†“ projections]\",\n",
    "    \"9. x = x + residual                   [Residual Connection #2]\",\n",
    "]\n",
    "\n",
    "for step in flow_steps:\n",
    "    print(f\"â•‘  {step:<66} â•‘\")\n",
    "\n",
    "print(\"â• \" + \"â•\" * 68 + \"â•£\")\n",
    "print(\"â•‘\" + \"  After all layers:\".center(68) + \"â•‘\")\n",
    "print(f\"â•‘  {'10. x = RMSNorm(x)        [final norm]':<66} â•‘\")\n",
    "print(f\"â•‘  {'11. logits = lm_head(x)   [project to vocab]':<66} â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Final Architecture Diagram â”€â”€â”€\n",
    "print(\"\\nğŸ—ï¸  Llama 3.2-1B-Instruct â€” Visual Architecture Flow\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "  Input Token IDs\n",
    "       â”‚\n",
    "       â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚   embed_tokens   â”‚  Vocabulary â†’ d_model ({hidden_size})\n",
    "  â”‚   ({vocab_size:,} Ã— {hidden_size})  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚         Decoder Layer Ã— {num_layers}           â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚  RMSNorm (input_layernorm)        â”‚  â”‚\n",
    "  â”‚  â”‚          â”‚                        â”‚  â”‚\n",
    "  â”‚  â”‚  Grouped-Query Attention (GQA)    â”‚  â”‚\n",
    "  â”‚  â”‚    Q: {num_heads} heads  KV: {num_kv_heads} heads          â”‚  â”‚\n",
    "  â”‚  â”‚    head_dim: {head_dim}   +RoPE            â”‚  â”‚\n",
    "  â”‚  â”‚          â”‚                        â”‚  â”‚\n",
    "  â”‚  â”‚  + Residual Connection            â”‚  â”‚\n",
    "  â”‚  â”‚          â”‚                        â”‚  â”‚\n",
    "  â”‚  â”‚  RMSNorm (post_attn_layernorm)    â”‚  â”‚\n",
    "  â”‚  â”‚          â”‚                        â”‚  â”‚\n",
    "  â”‚  â”‚  SwiGLU MLP                       â”‚  â”‚\n",
    "  â”‚  â”‚    gate: {hidden_size}â†’{intermediate_size}              â”‚  â”‚\n",
    "  â”‚  â”‚    up:   {hidden_size}â†’{intermediate_size}              â”‚  â”‚\n",
    "  â”‚  â”‚    down:  {intermediate_size}â†’{hidden_size}             â”‚  â”‚\n",
    "  â”‚  â”‚          â”‚                        â”‚  â”‚\n",
    "  â”‚  â”‚  + Residual Connection            â”‚  â”‚\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "                       â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚    RMSNorm (final)      â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚   lm_head (Linear)      â”‚  d_model â†’ vocab_size\n",
    "  â”‚   ({hidden_size} â†’ {vocab_size:,})    â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "         Output Logits\n",
    "     ({vocab_size:,} probabilities)\n",
    "\"\"\".format(\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=config.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    head_dim=head_dim,\n",
    "    intermediate_size=intermediate_size,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a340ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Clean up GPU memory â”€â”€â”€\n",
    "print(\"ğŸ§¹ Cleaning up...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory before cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"  GPU memory after cleanup : {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
