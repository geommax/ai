# ü¶ô Llama 3.2-1B-Instruct ‚Äî Architecture Deep Dive & Cross-Model Analysis

> **Model:** `meta-llama/Llama-3.2-1B-Instruct` ¬∑ **Parameters:** ~1.24B ¬∑ **Type:** Decoder-Only Causal LM  
> **·Äí·ÄÆ document ·Äô·Äæ·Ä¨** Llama 3.2 ·Äõ·Ä≤·Ä∑ internal architecture ·ÄÄ·Ä≠·ÄØ mathematical intuitions ·Äê·ÄΩ·Ä±·Äî·Ä≤·Ä∑ ·Ä°·Äû·Ä±·Ä∏·ÄÖ·Ä≠·Äê·Ä∫·Äõ·Äæ·ÄÑ·Ä∫·Ä∏·Äï·Äº·Äï·Äº·ÄÆ·Ä∏ GPT, Mistral, DeepSeek, Granite models ·Äê·ÄΩ·Ä±·Äî·Ä≤·Ä∑ ·Äö·Äæ·Äâ·Ä∫·Äï·Äº·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã

---

## üìã Table of Contents

1. [Overall Architecture Flow](#1-overall-architecture-flow)
2. [Token Embedding](#2-token-embedding)
3. [Self-Attention: GQA vs MHA vs MQA](#3-self-attention-gqa-vs-mha-vs-mqa)
4. [Positional Encoding: RoPE vs Alternatives](#4-positional-encoding-rope-vs-alternatives)
5. [Feed-Forward Network: SwiGLU vs Alternatives](#5-feed-forward-network-swiglu-vs-alternatives)
6. [Normalization: RMSNorm vs LayerNorm](#6-normalization-rmsnorm-vs-layernorm)
7. [KV Cache & Inference Optimization](#7-kv-cache--inference-optimization)
8. [Cross-Model Architecture Comparison Table](#8-cross-model-architecture-comparison-table)
9. [Key Takeaways](#9-key-takeaways)

---

## 1. Overall Architecture Flow

Llama 3.2-1B-Instruct ·Äû·Ää·Ä∫ **Decoder-Only Transformer** architecture ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã Encoder ·Äô·Äï·Ä´·Äò·Ä≤ Decoder blocks ·Äê·ÄΩ·Ä±·ÄÄ·Ä≠·ÄØ·Äû·Ä¨ stack ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 1.1 End-to-End Data Flow

```mermaid
graph TD
    A["üìù Input Text"] --> B["Tokenizer (BPE)"]
    B --> C["Token IDs<br/>(seq_len,)"]
    C --> D["embed_tokens<br/>128,256 √ó 2,048"]
    D --> E["Hidden States<br/>(batch, seq_len, 2048)"]

    E --> F{"Decoder Layer √ó 16"}

    subgraph DecoderLayer ["üîÅ Single Decoder Layer"]
        direction TB
        G["input_layernorm (RMSNorm)"] --> H["GQA Self-Attention<br/>Q:32 heads, KV:8 heads"]
        H --> I["+ Residual ‚ë†"]
        I --> J["post_attention_layernorm (RMSNorm)"]
        J --> K["SwiGLU MLP<br/>2048 ‚Üí 8192 ‚Üí 2048"]
        K --> L["+ Residual ‚ë°"]
    end

    F --> |"repeat 16√ó"| DecoderLayer
    DecoderLayer --> M["Final RMSNorm"]
    M --> N["lm_head (Linear)<br/>2,048 ‚Üí 128,256"]
    N --> O["Softmax ‚Üí Token Probabilities"]
    O --> P["üéØ Next Token Prediction"]

    style DecoderLayer fill:#1a1a2e,stroke:#e94560,stroke-width:2px
    style F fill:#0f3460,stroke:#e94560
```

### 1.2 Residual Stream Perspective

Modern LLMs ·ÄÄ·Ä≠·ÄØ "**residual stream**" perspective ·Äî·Ä≤·Ä∑ ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·ÄÑ·Ä∫ ·Äï·Ä≠·ÄØ·Äî·Ä¨·Ä∏·Äú·Ää·Ä∫·Äï·Ä´·Äê·Äö·Ä∫:

```mermaid
graph LR
    X0["x‚ÇÄ<br/>embedding"] --> |"+attn‚ÇÅ"| X1["x‚ÇÅ"]
    X1 --> |"+mlp‚ÇÅ"| X2["x‚ÇÇ"]
    X2 --> |"+attn‚ÇÇ"| X3["x‚ÇÉ"]
    X3 --> |"+mlp‚ÇÇ"| X4["x‚ÇÑ"]
    X4 --> |"..."| XN["x_2L"]
    XN --> |"RMSNorm"| OUT["logits"]

    style X0 fill:#2d3436,stroke:#00b894,color:#fff
    style XN fill:#2d3436,stroke:#e17055,color:#fff
```

**Intuition:** Hidden state $x$ ·Äû·Ää·Ä∫ "residual stream" ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨ ·ÄÖ·ÄÆ·Ä∏·ÄÜ·ÄÑ·Ä∫·Ä∏·Äî·Ä±·Äï·Äº·ÄÆ·Ä∏ layer ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·ÄÄ information ·ÄÄ·Ä≠·ÄØ **·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ää·Ä∑·Ä∫** (additive update) ·Äú·ÄØ·Äï·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã ·Äí·Ä´·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ gradient flow ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ deep networks ·ÄÄ·Ä≠·ÄØ train ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

$$x_{l+1} = x_l + \text{Attn}(\text{Norm}(x_l)) + \text{MLP}(\text{Norm}(x_l + \text{Attn}(\text{Norm}(x_l))))$$

---

## 2. Token Embedding

### 2.1 Embedding in Llama 3.2

| Property | Value |
|----------|-------|
| Vocab Size | 128,256 |
| Embedding Dim | 2,048 |
| Weight Tying | ‚úÖ Yes (`embed_tokens` == `lm_head`) |
| Parameters | 128,256 √ó 2,048 = **262,668,288** (~263M) |

**Weight Tying Intuition:**

$$\text{embed-tokens}: \quad e = W_E[t] \quad (t \to \mathbb{R}^{d})$$

$$\text{lm-head}: \quad \text{logits} = W_E^T \cdot h \quad (\mathbb{R}^{d} \to \mathbb{R}^{|V|})$$

Same matrix $W_E$ ·ÄÄ·Ä≠·ÄØ embed_tokens (lookup) ·Äî·Ä≤·Ä∑ lm_head (projection) ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ·Äú·ÄØ·Ä∂·Ä∏·Äô·Äæ·Ä¨ share ·Äû·ÄØ·Ä∂·Ä∏·Äê·Ä¨ ‚Äî parameter count ·ÄÄ·Ä≠·ÄØ ~263M ·Äû·ÄÄ·Ä∫·Äû·Ä¨·ÄÖ·Ä±·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 2.2 Cross-Model Embedding Comparison

| Model | Vocab Size | Embed Dim | Weight Tying |
|-------|-----------|-----------|:------------:|
| **Llama 3.2-1B** | 128,256 | 2,048 | ‚úÖ |
| **GPT-2** | 50,257 | 768‚Äì1,600 | ‚úÖ |
| **GPT-3/4** | 100,256+ | 12,288+ | ‚ùå |
| **Mistral 7B** | 32,000 | 4,096 | ‚ùå |
| **DeepSeek-V2** | 102,400 | 5,120 | ‚ùå |
| **Granite 3B** | 49,152 | 2,560 | ‚úÖ |

> **Key Insight:** Llama 3 series ·ÄÄ vocab size ·ÄÄ·Ä≠·ÄØ 128K+ ·Äë·Ä≠ ·ÄÅ·Äª·Ä≤·Ä∑·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã ·Äí·Ä´·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ embedding layer ·Äõ·Ä≤·Ä∑ parameter count ·ÄÄ model total ·Äõ·Ä≤·Ä∑ ~21% ·ÄÄ·Ä≠·ÄØ ·Äö·Ä∞·Äï·Ä´·Äê·Äö·Ä∫·Åã Weight tying ·Äü·Ä¨ 1B-class small model ·Äô·Äæ·Ä¨ parameter efficiency ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äõ·Ä±·Ä∏·ÄÄ·Äº·ÄÆ·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã

---

## 3. Self-Attention: GQA vs MHA vs MQA

### 3.1 Standard Multi-Head Attention (MHA) ‚Äî GPT Series

GPT-2/3 ·Äô·Äæ·Ä¨ ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äê·Ä≤·Ä∑ classic MHA:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

Head ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Äô·Äæ·Ä¨:

$$Q_i = xW^Q_i, \quad K_i = xW^K_i, \quad V_i = xW^V_i \quad \text{for } i = 1, \ldots, h$$

```mermaid
graph TB
    subgraph MHA ["Multi-Head Attention (GPT)"]
        direction LR
        Q1["Q‚ÇÅ"] --- K1["K‚ÇÅ"] --- V1["V‚ÇÅ"]
        Q2["Q‚ÇÇ"] --- K2["K‚ÇÇ"] --- V2["V‚ÇÇ"]
        Q3["Q‚ÇÉ"] --- K3["K‚ÇÉ"] --- V3["V‚ÇÉ"]
        Q4["Q‚ÇÑ"] --- K4["K‚ÇÑ"] --- V4["V‚ÇÑ"]
    end

    subgraph MQA ["Multi-Query Attention"]
        direction LR
        QA["Q‚ÇÅ"] --- KA["K (shared)"]
        QB["Q‚ÇÇ"] --- KA
        QC["Q‚ÇÉ"] --- KA
        QD["Q‚ÇÑ"] --- KA
        KA --- VA["V (shared)"]
    end

    subgraph GQA ["Grouped-Query Attention (Llama)"]
        direction LR
        QG1["Q‚ÇÅ"] --- KG1["K‚ÇÅ"]
        QG2["Q‚ÇÇ"] --- KG1
        KG1 --- VG1["V‚ÇÅ"]
        QG3["Q‚ÇÉ"] --- KG2["K‚ÇÇ"]
        QG4["Q‚ÇÑ"] --- KG2
        KG2 --- VG2["V‚ÇÇ"]
    end

    style MHA fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#fff
    style MQA fill:#2d3436,stroke:#e17055,stroke-width:2px,color:#fff
    style GQA fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#fff
```

### 3.2 Grouped-Query Attention (GQA) ‚Äî Llama 3.2

Llama 3.2-1B ·Äô·Äæ·Ä¨:
- **Query heads:** 32
- **KV heads:** 8
- **Group size:** 32 √∑ 8 = 4 (Q heads 4 ·ÄÅ·ÄØ·ÄÄ KV head 1 ·ÄÅ·ÄØ·ÄÄ·Ä≠·ÄØ share)

**Math:**

$$Q \in \mathbb{R}^{n \times 32 \times 64}, \quad K \in \mathbb{R}^{n \times 8 \times 64}, \quad V \in \mathbb{R}^{n \times 8 \times 64}$$

GQA ·Äô·Äæ·Ä¨ K, V ·ÄÄ·Ä≠·ÄØ broadcast/repeat ·Äú·ÄØ·Äï·Ä∫·Äï·Äº·ÄÆ·Ä∏ Q groups ·Äî·Ä≤·Ä∑ match ·Äï·Ä´·Äê·Äö·Ä∫:

$$K_{\text{expanded}} = \text{repeat-kv}(K, \text{n-rep}=4) \in \mathbb{R}^{n \times 32 \times 64}$$

$$\text{Attn}_i = \text{softmax}\left(\frac{Q_i K_{\lfloor i/4 \rfloor}^T}{\sqrt{64}}\right) V_{\lfloor i/4 \rfloor}$$

### 3.3 Parameter & KV Cache Savings

**Projection Parameters per Layer:**

| | MHA (32 KV heads) | GQA (8 KV heads) | Savings |
|-|---|---|---|
| **Q proj** | 2,048 √ó 2,048 | 2,048 √ó 2,048 | ‚Äî |
| **K proj** | 2,048 √ó 2,048 | 2,048 √ó 512 | **75%** |
| **V proj** | 2,048 √ó 2,048 | 2,048 √ó 512 | **75%** |
| **O proj** | 2,048 √ó 2,048 | 2,048 √ó 2,048 | ‚Äî |
| **KV params total** | 8,388,608 | 2,097,152 | **75%** |

**KV Cache per Layer (seq_len=1024, FP16):**

$$\text{KV Cache} = 2 \times n_{\text{kv-heads}} \times d_{\text{head}} \times \text{seq-len} \times 2 \text{ bytes}$$

| | MHA | GQA | Savings |
|-|---|---|---|
| Per layer | 8.39 MB | 2.10 MB | 75% |
| Total (16 layers) | 134.22 MB | 33.55 MB | **75%** |

### 3.4 Cross-Model Attention Comparison

| Model | Attention Type | Q Heads | KV Heads | Head Dim | GQA Ratio |
|-------|:-------------:|--------:|--------:|---------:|----------:|
| **Llama 3.2-1B** | GQA | 32 | 8 | 64 | 4:1 |
| **Llama 3.1-8B** | GQA | 32 | 8 | 128 | 4:1 |
| **GPT-2 (124M)** | MHA | 12 | 12 | 64 | 1:1 |
| **GPT-3 (175B)** | MHA | 96 | 96 | 128 | 1:1 |
| **Mistral 7B** | GQA | 32 | 8 | 128 | 4:1 |
| **Mistral + Sliding Window** | GQA + SW | 32 | 8 | 128 | 4:1 |
| **DeepSeek-V2 (236B)** | **MLA** | 128 | ‚Äî | ‚Äî | N/A |
| **Granite 3B** | GQA | 32 | 8 | 80 | 4:1 |

### 3.5 DeepSeek's Multi-Head Latent Attention (MLA)

DeepSeek-V2 ·ÄÄ GQA ·Äë·ÄÄ·Ä∫ ·Äï·Ä≠·ÄØ·Äô·Ä≠·ÄØ·ÄÄ·Äª·ÄÖ·Ä∫·Äú·ÄÖ·Ä∫·Äê·Ä≤·Ä∑ **MLA** ·ÄÄ·Ä≠·ÄØ ·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·Äê·Äö·Ä∫:

```mermaid
graph TB
    subgraph GQA_Flow ["GQA (Llama/Mistral)"]
        direction TB
        H1["hidden (d=2048)"] --> WQ1["W_Q ‚Üí Q (32√ó64)"]
        H1 --> WK1["W_K ‚Üí K (8√ó64)"]
        H1 --> WV1["W_V ‚Üí V (8√ó64)"]
        WQ1 --> ATTN1["Attention"]
        WK1 --> ATTN1
        WV1 --> ATTN1
    end

    subgraph MLA_Flow ["MLA (DeepSeek-V2)"]
        direction TB
        H2["hidden (d=5120)"] --> C["W_DKV ‚Üí c_KV (compressed, d_c=512)"]
        C --> WK2["W_UK ‚Üí K (128 heads)"]
        C --> WV2["W_UV ‚Üí V (128 heads)"]
        H2 --> WQ2["W_DQ ‚Üí c_Q ‚Üí W_UQ ‚Üí Q"]
        WQ2 --> ATTN2["Attention"]
        WK2 --> ATTN2
        WV2 --> ATTN2
    end

    style GQA_Flow fill:#1a1a2e,stroke:#00b894,stroke-width:2px
    style MLA_Flow fill:#1a1a2e,stroke:#e94560,stroke-width:2px
```

**MLA Math:**

$$c_{KV} = W^{DKV} h \in \mathbb{R}^{d_c} \quad (d_c \ll n_h \times d_h)$$
$$K = W^{UK} c_{KV}, \quad V = W^{UV} c_{KV}$$

**KV Cache ·Äö·Äæ·Äâ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫:**

| Method | Cache per Token | Llama 3.2 (d=2048) | DeepSeek-V2 (d=5120) |
|--------|:-:|:-:|:-:|
| MHA | $2 \times n_h \times d_h$ | 4,096 | 32,768 |
| GQA | $2 \times n_{kv} \times d_h$ | 1,024 | ‚Äî |
| MLA | $d_c$ | ‚Äî | 512 |

> **Insight:** MLA ·ÄÄ compressed latent vector $c_{KV}$ ·ÄÄ·Ä≠·ÄØ·Äï·Ä≤ cache ·Äú·ÄØ·Äï·Ä∫·Äõ·Äê·Ä¨·Äñ·Äº·ÄÖ·Ä∫·Äú·Ä≠·ÄØ·Ä∑ GQA ·Äë·ÄÄ·Ä∫ memory ·Äï·Ä≠·ÄØ·Äû·ÄÄ·Ä∫·Äû·Ä¨·Äï·Ä´·Äê·Äö·Ä∫·Åã ·Äí·Ä´·Äï·Ä±·Äô·Ä≤·Ä∑ decompression computation ·Äï·Ä≠·ÄØ·Äú·Ä≠·ÄØ·Äï·Ä´·Äê·Äö·Ä∫·Åã

---

## 4. Positional Encoding: RoPE vs Alternatives

### 4.1 Why Position Encoding?

Transformer ·Äõ·Ä≤·Ä∑ self-attention ·Äû·Ää·Ä∫ **permutation invariant** ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫ ‚Äî token order ·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·Äñ·Ä≠·ÄØ·Ä∑ position information ·Äë·Ää·Ä∑·Ä∫·Äï·Ä±·Ä∏·Äõ·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 4.2 Absolute Positional Encoding ‚Äî GPT-2

GPT-2 ·Äô·Äæ·Ä¨ learnable absolute position embeddings:

$$h_0 = W_E[t] + W_P[\text{pos}]$$

- $W_P \in \mathbb{R}^{L_{\max} \times d}$ ‚Äî position ·ÄÄ·Ä≠·ÄØ direct learn
- **·Ä°·Ä¨·Ä∏·Äî·Ää·Ä∫·Ä∏·ÄÅ·Äª·ÄÄ·Ä∫:** $L_{\max}$ ·Äë·ÄÄ·Ä∫ ·Äõ·Äæ·Ää·Ä∫·Äê·Ä≤·Ä∑ sequences ·ÄÄ·Ä≠·ÄØ handle ·Äô·Äõ

### 4.3 Sinusoidal Positional Encoding ‚Äî Original Transformer

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

### 4.4 RoPE (Rotary Position Embedding) ‚Äî Llama, Mistral, DeepSeek, Granite

Llama 3.2 ·Äô·Äæ·Ä¨ ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äê·Ä≤·Ä∑ RoPE:

**Core Idea:** Position information ·ÄÄ·Ä≠·ÄØ Q, K vectors ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨ **rotation** ·Ä°·Äî·Ä±·Äî·Ä≤·Ä∑ apply:

$$\tilde{q}_m = R_\Theta(m) \cdot q_m, \quad \tilde{k}_n = R_\Theta(n) \cdot k_n$$

Rotation matrix (2D pair ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫):

$$R_\theta(m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}$$

**·Äò·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ Rotation?**

Dot product ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·ÄÑ·Ä∫:

$$\tilde{q}_m^T \tilde{k}_n = q_m^T R_\Theta(m)^T R_\Theta(n) k_n = q_m^T R_\Theta(n - m) k_n$$

Attention score ·Äû·Ää·Ä∫ **relative position** $(n - m)$ ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨·Äï·Ä≤ depend ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫! ·Äí·Ä´·ÄÄ RoPE ·Äõ·Ä≤·Ä∑ elegance ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

**Llama 3.2 RoPE Frequencies:**

$$\theta_i = \frac{1}{\theta_{\text{base}}^{2i/d}} \quad \text{where } \theta_{\text{base}} = 500{,}000, \; d = 64$$

```mermaid
graph LR
    subgraph RoPE_Apply ["RoPE Application (per head)"]
        Q["q (d=64)"] --> PAIR["Split into 32 pairs<br/>(q‚ÇÄ,q‚ÇÅ), (q‚ÇÇ,q‚ÇÉ), ..."]
        PAIR --> ROT["Apply 2D rotation<br/>angle = pos √ó Œ∏·µ¢"]
        ROT --> Q_OUT["qÃÉ (rotated)"]
    end

    subgraph Freq ["Frequency Spectrum"]
        F1["Œ∏‚ÇÄ = 1.0<br/>(high freq, local)"]
        F2["Œ∏‚ÇÅ‚ÇÖ ‚âà 0.001<br/>(medium freq)"]
        F3["Œ∏‚ÇÉ‚ÇÅ ‚âà 1e-6<br/>(low freq, global)"]
    end

    style RoPE_Apply fill:#2d3436,stroke:#6c5ce7,stroke-width:2px,color:#fff
    style Freq fill:#2d3436,stroke:#fdcb6e,stroke-width:2px,color:#fff
```

### 4.5 Mistral's Sliding Window Attention + RoPE

Mistral 7B ·ÄÄ RoPE ·Ä°·Äï·Äº·ÄÑ·Ä∫ **Sliding Window Attention (SWA)** ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·Äê·Äö·Ä∫:

$$\text{Attn}_{ij} = \begin{cases} \text{softmax}(\frac{q_i k_j^T}{\sqrt{d}}) & \text{if } i - j \leq W \\ 0 & \text{otherwise} \end{cases}$$

Window size $W = 4096$ ·Äñ·Äº·ÄÖ·Ä∫·Äï·Äº·ÄÆ·Ä∏ layer stacking ·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ effective receptive field ·ÄÄ $W \times L$ (L = num layers) ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 4.6 Positional Encoding Comparison

| Method | Model | Relative? | Extrapolation | Applied to |
|--------|-------|:---------:|:-------------:|:----------:|
| **Learned Absolute** | GPT-2 | ‚ùå | ‚ùå Poor | embed layer |
| **Sinusoidal** | Original Transformer | ‚ùå | ‚ö†Ô∏è Limited | embed layer |
| **RoPE** | Llama, Mistral, Granite | ‚úÖ | ‚úÖ Good | Q, K only |
| **RoPE + SWA** | Mistral | ‚úÖ | ‚úÖ Excellent | Q, K only |
| **ALiBi** | Bloom, MPT | ‚úÖ | ‚úÖ Good | attn bias |
| **RoPE + YaRN** | DeepSeek, Llama 3.1 | ‚úÖ | ‚úÖ Excellent | Q, K only |

> **RoPE ·Äõ·Ä≤·Ä∑ Key Advantages:**
> 1. Relative position ·ÄÄ·Ä≠·ÄØ naturally encode (rotation property)
> 2. V ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨ apply ·Äô·Äú·ÄØ·Äï·Ä∫·Äú·Ä≠·ÄØ·Ä∑ value representation ·ÄÄ·Ä≠·ÄØ corrupt ·Äô·Äú·ÄØ·Äï·Ä∫
> 3. Sequence length extrapolation ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ (especially with scaling)
> 4. No additional parameters (Œ∏_base ·Äï·Ä≤ hyperparameter)

---

## 5. Feed-Forward Network: SwiGLU vs Alternatives

### 5.1 Original FFN ‚Äî GPT-2

$$\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2$$

$$W_1 \in \mathbb{R}^{d \times 4d}, \quad W_2 \in \mathbb{R}^{4d \times d}$$

Parameters: $2 \times d \times 4d = 8d^2$

### 5.2 GELU FFN ‚Äî GPT-3

$$\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 x)$$

$$\text{GELU}(x) = x \cdot \Phi(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{2/\pi}(x + 0.044715x^3)\right]\right)$$

### 5.3 SwiGLU ‚Äî Llama, Mistral, DeepSeek, Granite

$$\text{SwiGLU}(x) = W_{\text{down}} \cdot \left[\text{SiLU}(W_{\text{gate}} x) \odot (W_{\text{up}} x)\right]$$

where:

$$\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$

```mermaid
graph LR
    X["x<br/>(d=2048)"] --> GATE["W_gate<br/>2048‚Üí8192"]
    X --> UP["W_up<br/>2048‚Üí8192"]
    GATE --> SILU["SiLU(¬∑)"]
    SILU --> MUL["‚äô element-wise"]
    UP --> MUL
    MUL --> DOWN["W_down<br/>8192‚Üí2048"]
    DOWN --> OUT["output<br/>(d=2048)"]

    style X fill:#2d3436,stroke:#00b894,color:#fff
    style MUL fill:#e17055,stroke:#fff,color:#fff
    style OUT fill:#2d3436,stroke:#00b894,color:#fff
```

**Gating Mechanism Intuition:**

$\text{SiLU}(W_{\text{gate}} x)$ ·ÄÄ "gate" ·Äõ·Ä≤·Ä∑ role ·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫:
- Gate value ‚âà 0 ‚Üí information ·ÄÄ·Ä≠·ÄØ block
- Gate value ‚âà x ‚Üí information ·ÄÄ·Ä≠·ÄØ pass through
- Gate ·ÄÄ input-dependent ·Äñ·Äº·ÄÖ·Ä∫·Äú·Ä≠·ÄØ·Ä∑ **dynamic feature selection** ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫

**·Äò·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ SwiGLU ·ÄÄ ReLU/GELU ·Äë·ÄÄ·Ä∫ ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤?**

1. **Gating ‚Üí Expressiveness:** Matrix 2 ·ÄÅ·ÄØ (gate + up) ·Äõ·Ä≤·Ä∑ interaction ·ÄÄ richer representations ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫
2. **SiLU smoothness:** ReLU ·Äõ·Ä≤·Ä∑ dead neuron problem ·Äô·Äõ·Äæ·Ä≠ (gradient ‚â† 0 for negative inputs)
3. **Empirical gains:** PaLM paper (Google, 2022) ·Äô·Äæ·Ä¨ SwiGLU ·ÄÄ ReLU/GELU ·Äë·ÄÄ·Ä∫ consistent ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ prove ·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫

### 5.4 Parameter Count Comparison

Llama 3.2-1B: $d = 2048$, intermediate = $8192$

| FFN Type | Matrices | Formula | Params per Layer |
|----------|:--------:|---------|:----------------:|
| **ReLU/GELU (GPT)** | 2 | $2 \times d \times 4d$ | $2 \times 2048 \times 8192 = 33.6M$ |
| **SwiGLU (Llama)** | 3 | $3 \times d \times d_{\text{ff}}$ | $3 \times 2048 \times 8192 = 50.3M$ |

> ‚ö†Ô∏è SwiGLU ·ÄÄ matrix 3 ·ÄÅ·ÄØ (gate, up, down) ·Äû·ÄØ·Ä∂·Ä∏·Äú·Ä≠·ÄØ·Ä∑ parameter count 50% ·Äï·Ä≠·ÄØ·Äô·Äª·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã ·Äí·Ä´·Äï·Ä±·Äô·Ä≤·Ä∑ quality improvement ·ÄÄ parameter cost ·ÄÄ·Ä≠·ÄØ worth ·Äñ·Äº·ÄÖ·Ä∫·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ research papers ·Äê·ÄΩ·Ä±·Äô·Äæ·Ä¨ consistently ·Äï·Äº·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã Intermediate size ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Äª·Ä≠·Äî·Ä∫·Ää·Äæ·Ä≠·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Äñ·Äº·ÄÑ·Ä∑·Ä∫ (e.g., $\frac{2}{3} \times 4d$ instead of $4d$) parameter parity ·Äë·Ä≠·Äî·Ä∫·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 5.5 Cross-Model FFN Comparison

| Model | FFN Type | Activation | Expansion Ratio | Intermediate |
|-------|----------|:----------:|:---------------:|-------------:|
| **Llama 3.2-1B** | SwiGLU | SiLU | 4.0√ó | 8,192 |
| **GPT-2 (124M)** | Standard | GELU | 4.0√ó | 3,072 |
| **GPT-3 (175B)** | Standard | GELU | 4.0√ó | 49,152 |
| **Mistral 7B** | SwiGLU | SiLU | 3.5√ó | 14,336 |
| **DeepSeek-V2** | SwiGLU + **MoE** | SiLU | varies | **expert routing** |
| **Granite 3B** | SwiGLU | SiLU | 4.0√ó | 10,240 |

### 5.6 DeepSeek's Mixture of Experts (MoE) FFN

DeepSeek-V2 ·ÄÄ FFN layer ·Äô·Äæ·Ä¨ **Mixture of Experts** ·ÄÄ·Ä≠·ÄØ ·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·Äê·Äö·Ä∫:

```mermaid
graph TB
    H["hidden state x"] --> ROUTER["Router Network<br/>softmax(W_r ¬∑ x)"]
    ROUTER --> |"top-k selection"| E1["Expert 1<br/>SwiGLU"]
    ROUTER --> |"top-k selection"| E2["Expert 2<br/>SwiGLU"]
    ROUTER --> E_DOTS["..."]
    ROUTER --> EN["Expert N<br/>SwiGLU"]

    E1 --> COMBINE["Weighted Sum<br/>Œ£ g·µ¢ ¬∑ Expert·µ¢(x)"]
    E2 --> COMBINE
    EN --> COMBINE
    COMBINE --> OUT["output"]

    style ROUTER fill:#e17055,stroke:#fff,color:#fff
    style COMBINE fill:#00b894,stroke:#fff,color:#fff
```

$$\text{MoE-FFN}(x) = \sum_{i \in \text{Top-}k} g_i(x) \cdot E_i(x)$$

**Advantage:** Total parameters ·Äô·Äª·Ä¨·Ä∏·Äï·Ä±·Äô·Ä≤·Ä∑ token ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ experts ·Ä°·Äî·Ää·Ä∫·Ä∏·ÄÑ·Äö·Ä∫·Äï·Ä≤ activate ·Äú·Ä≠·ÄØ·Ä∑ **compute ·Äï·Ä≠·ÄØ·Äô·Äº·Äî·Ä∫**·Äï·Ä´·Äê·Äö·Ä∫·Åã

---

## 6. Normalization: RMSNorm vs LayerNorm

### 6.1 Layer Normalization ‚Äî GPT-2/3

$$\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta$$

where:

$$\mu = \frac{1}{d}\sum_{i=1}^{d} x_i, \quad \sigma^2 = \frac{1}{d}\sum_{i=1}^{d}(x_i - \mu)^2$$

- **Re-centering** (mean subtraction) + **Re-scaling** (variance normalization)
- Learnable parameters: $\gamma$ (scale) + $\beta$ (shift) ‚Üí $2d$ params

### 6.2 RMSNorm ‚Äî Llama, Mistral, DeepSeek, Granite

$$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_i^2 + \epsilon}} \cdot \gamma$$

- **Re-scaling only** (no mean subtraction, no bias)
- Learnable parameters: $\gamma$ (scale only) ‚Üí $d$ params

### 6.3 Mathematical Comparison

```mermaid
graph LR
    subgraph LN ["LayerNorm (GPT)"]
        direction TB
        A1["1. Compute mean Œº"] --> A2["2. Compute variance œÉ¬≤"]
        A2 --> A3["3. Normalize: (x-Œº)/‚àö(œÉ¬≤+Œµ)"]
        A3 --> A4["4. Scale & Shift: Œ≥xÃÇ + Œ≤"]
    end

    subgraph RMS ["RMSNorm (Llama)"]
        direction TB
        B1["1. Compute RMS = ‚àö(Œ£x¬≤/d)"] --> B2["2. Normalize: x/RMS"]
        B2 --> B3["3. Scale: Œ≥xÃÇ"]
    end

    style LN fill:#2d3436,stroke:#0984e3,stroke-width:2px,color:#fff
    style RMS fill:#2d3436,stroke:#00b894,stroke-width:2px,color:#fff
```

**·Äò·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ RMSNorm ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤?**

1. **Speed:** Mean computation ·ÄÄ·Ä≠·ÄØ skip (operations ~33% ·Äï·Ä≠·ÄØ·Äî·Ää·Ä∫·Ä∏)
2. **Theory:** "Re-centering is not as important as re-scaling" ‚Äî Zhang & Sennrich (2019)
3. **No bias ($\beta$):** Parameter ·Äï·Ä≠·ÄØ·Äî·Ää·Ä∫·Ä∏, training ·Äï·Ä≠·ÄØstable

### 6.4 Pre-Norm vs Post-Norm

| Position | Formula | Used by |
|----------|---------|---------|
| **Post-Norm** | $x + \text{Attn}(\text{Norm}(x))$ ... wait, ‚Üì | GPT-2, Original Transformer |
| **Post-Norm (actual)** | $\text{Norm}(x + \text{Attn}(x))$ | GPT-2, Original Transformer |
| **Pre-Norm** ‚úÖ | $x + \text{Attn}(\text{Norm}(x))$ | Llama, Mistral, DeepSeek, GPT-3, Granite |

Llama 3.2 (Pre-Norm):

$$x_{l+1} = x_l + \text{Attn}(\text{RMSNorm}(x_l))$$
$$x_{l+2} = x_{l+1} + \text{MLP}(\text{RMSNorm}(x_{l+1}))$$

> **Pre-Norm Advantage:** Gradient flow ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ deep networks ·ÄÄ·Ä≠·ÄØ stable ·Äñ·Äº·ÄÖ·Ä∫·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ train ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã Residual connection ·ÄÄ·Äî·Ä± gradient ·ÄÄ normalization layer ·ÄÄ·Ä≠·ÄØ skip ·Äï·Äº·ÄÆ·Ä∏ direct flow ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

### 6.5 Cross-Model Normalization

| Model | Norm Type | Position | Has Bias ($\beta$) | Epsilon |
|-------|-----------|:--------:|:---------:|--------:|
| **Llama 3.2-1B** | RMSNorm | Pre-Norm | ‚ùå | 1e-5 |
| **GPT-2** | LayerNorm | Post-Norm | ‚úÖ | 1e-5 |
| **GPT-3** | LayerNorm | Pre-Norm | ‚úÖ | 1e-5 |
| **Mistral 7B** | RMSNorm | Pre-Norm | ‚ùå | 1e-5 |
| **DeepSeek-V2** | RMSNorm | Pre-Norm | ‚ùå | 1e-6 |
| **Granite 3B** | RMSNorm | Pre-Norm | ‚ùå | 1e-5 |

---

## 7. KV Cache & Inference Optimization

### 7.1 Autoregressive Generation Problem

Decoder model ·Äô·Äæ·Ä¨ token generate ·Äú·ÄØ·Äï·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ previous tokens ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ K, V ·ÄÄ·Ä≠·ÄØ recompute ·Äú·ÄØ·Äï·Ä∫·Äõ·Äï·Ä´·Äê·Äö·Ä∫:

$$\text{Step } t: \quad \text{Attn}(q_t, [k_1, ..., k_t], [v_1, ..., v_t])$$

**KV Cache:** Previous steps ·Äõ·Ä≤·Ä∑ K, V values ·ÄÄ·Ä≠·ÄØ cache ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ new token ·Äõ·Ä≤·Ä∑ K, V ·ÄÄ·Ä≠·ÄØ·Äï·Ä≤ compute:

```mermaid
sequenceDiagram
    participant Gen as Generator
    participant Cache as KV Cache
    participant Attn as Attention

    Note over Gen: Step 1: "The"
    Gen->>Cache: Store K‚ÇÅ, V‚ÇÅ
    Gen->>Attn: q‚ÇÅ √ó [K‚ÇÅ] ‚Üí attn‚ÇÅ

    Note over Gen: Step 2: "cat"
    Gen->>Cache: Append K‚ÇÇ, V‚ÇÇ
    Gen->>Attn: q‚ÇÇ √ó [K‚ÇÅ,K‚ÇÇ] ‚Üí attn‚ÇÇ

    Note over Gen: Step 3: "sat"
    Gen->>Cache: Append K‚ÇÉ, V‚ÇÉ
    Gen->>Attn: q‚ÇÉ √ó [K‚ÇÅ,K‚ÇÇ,K‚ÇÉ] ‚Üí attn‚ÇÉ

    Note over Cache: Cache grows linearly!
```

### 7.2 KV Cache Memory Formula

$$\text{Cache Memory} = 2 \times L \times n_{kv} \times d_h \times S \times \text{bytes}$$

where $L$ = layers, $n_{kv}$ = KV heads, $d_h$ = head dim, $S$ = seq length

### 7.3 Cross-Model KV Cache Comparison (seq_len=4096, FP16)

| Model | KV Heads | Head Dim | Layers | Cache Size |
|-------|:--------:|:--------:|:------:|:----------:|
| **Llama 3.2-1B** | 8 | 64 | 16 | **64 MB** |
| **GPT-3 (175B)** | 96 | 128 | 96 | **9,216 MB** |
| **Mistral 7B** | 8 | 128 | 32 | **256 MB** |
| **Mistral 7B + SWA** | 8 | 128 | 32 | **~64 MB** (window=4096) |
| **DeepSeek-V2 (MLA)** | ‚Äî | ‚Äî | 60 | **~60 MB** (compressed) |
| **Granite 3B** | 8 | 80 | 32 | **160 MB** |

> **Llama 3.2-1B ·Äõ·Ä≤·Ä∑ KV Cache Advantage:** 1B model ·Äñ·Äº·ÄÖ·Ä∫·Äú·Ä≠·ÄØ·Ä∑ layers ·Äî·Ää·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ GQA ·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ cache size ·ÄÄ·Ä≠·ÄØ·Äú·Ää·Ä∫·Ä∏ 75% ·Äú·Äª·Äæ·Ä±·Ä¨·Ä∑·Äë·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã Edge devices / mobile deployment ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ practical ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã

---

## 8. Cross-Model Architecture Comparison Table

### 8.1 Full Architecture Summary

| Feature | Llama 3.2-1B | GPT-2 (124M) | GPT-3 (175B) | Mistral 7B | DeepSeek-V2 (236B) | Granite 3B |
|---------|:------------:|:------------:|:------------:|:----------:|:------------------:|:----------:|
| **Params** | 1.24B | 124M | 175B | 7.3B | 236B (21B active) | 3B |
| **Layers** | 16 | 12 | 96 | 32 | 60 | 32 |
| **Hidden** | 2,048 | 768 | 12,288 | 4,096 | 5,120 | 2,560 |
| **Attention** | GQA | MHA | MHA | GQA+SWA | MLA | GQA |
| **Q/KV Heads** | 32/8 | 12/12 | 96/96 | 32/8 | 128/‚Äî | 32/8 |
| **Pos Encoding** | RoPE | Learned | Learned | RoPE+SWA | RoPE+YaRN | RoPE |
| **FFN** | SwiGLU | GELU | GELU | SwiGLU | SwiGLU+MoE | SwiGLU |
| **Norm** | RMSNorm | LayerNorm | LayerNorm | RMSNorm | RMSNorm | RMSNorm |
| **Norm Position** | Pre | Post | Pre | Pre | Pre | Pre |
| **Vocab** | 128K | 50K | 100K | 32K | 102K | 49K |
| **Context** | 131K | 1K | 2-32K | 32K | 128K | 4-128K |
| **Weight Tying** | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |

### 8.2 Architecture Evolution Timeline

```mermaid
timeline
    title Transformer Architecture Evolution
    2017 : Original Transformer
         : Sinusoidal PE
         : LayerNorm (Post)
         : ReLU FFN
    2019 : GPT-2
         : Learned PE
         : LayerNorm (Post)
         : GELU FFN
    2020 : GPT-3
         : Learned PE
         : LayerNorm (Pre)
         : GELU FFN
    2023 : Llama 2
         : RoPE
         : RMSNorm (Pre)
         : SwiGLU, GQA
    2023 : Mistral 7B
         : RoPE + Sliding Window
         : RMSNorm (Pre)
         : SwiGLU, GQA
    2024 : DeepSeek-V2
         : RoPE + YaRN
         : RMSNorm (Pre)
         : SwiGLU + MoE, MLA
    2024 : Llama 3.2
         : RoPE (Œ∏=500K)
         : RMSNorm (Pre)
         : SwiGLU, GQA
    2024 : Granite 3
         : RoPE
         : RMSNorm (Pre)
         : SwiGLU, GQA
```

---

## 9. Key Takeaways

### 9.1 Llama 3.2-1B ·Äõ·Ä≤·Ä∑ Design Philosophy

1. **Efficiency First:** 1B parameters ·Äî·Ä≤·Ä∑ maximum performance ·Äõ·Äñ·Ä≠·ÄØ·Ä∑ GQA + Weight Tying + RMSNorm
2. **Proven Components:** SwiGLU, RoPE, Pre-Norm ‚Äî research-backed choices
3. **Edge-Ready:** Small model size + efficient KV cache ‚Üí mobile/edge deployment suitable

### 9.2 Modern LLM Architecture Trends

```mermaid
graph TD
    subgraph Attention_Trend ["Attention Evolution"]
        MHA_T["MHA (GPT)"] --> GQA_T["GQA (Llama, Mistral)"]
        GQA_T --> MLA_T["MLA (DeepSeek)"]
        MHA_T --> SWA_T["+ Sliding Window (Mistral)"]
    end

    subgraph FFN_Trend ["FFN Evolution"]
        RELU_T["ReLU"] --> GELU_T["GELU (GPT-3)"]
        GELU_T --> SWIGLU_T["SwiGLU (Llama, all modern)"]
        SWIGLU_T --> MOE_T["+ MoE (DeepSeek, Mixtral)"]
    end

    subgraph Norm_Trend ["Normalization Evolution"]
        LN_T["LayerNorm Post"] --> LN_PRE_T["LayerNorm Pre (GPT-3)"]
        LN_PRE_T --> RMS_T["RMSNorm Pre (all modern)"]
    end

    subgraph Pos_Trend ["Position Encoding Evolution"]
        LEARN_T["Learned Absolute (GPT)"] --> ROPE_T["RoPE (Llama)"]
        ROPE_T --> ROPE_EXT["RoPE + Scaling (YaRN, NTK)"]
        ROPE_T --> ROPE_SWA["RoPE + SWA (Mistral)"]
    end

    style Attention_Trend fill:#1a1a2e,stroke:#e94560,stroke-width:2px
    style FFN_Trend fill:#1a1a2e,stroke:#00b894,stroke-width:2px
    style Norm_Trend fill:#1a1a2e,stroke:#0984e3,stroke-width:2px
    style Pos_Trend fill:#1a1a2e,stroke:#fdcb6e,stroke-width:2px
```

### 9.3 Summary Formula ‚Äî Llama 3.2-1B Forward Pass

$$\boxed{
\begin{aligned}
h_0 &= W_E[\text{tokens}] \\
\text{For layer } l &= 0, \ldots, 15: \\
\quad \hat{h}_l &= \text{RMSNorm}(h_l) \\
\quad h_l' &= h_l + \text{GQA}(\hat{h}_l; W^Q_l, W^K_l, W^V_l, W^O_l, \text{RoPE}) \\
\quad \hat{h}_l' &= \text{RMSNorm}(h_l') \\
\quad h_{l+1} &= h_l' + W^{\text{down}}_l \left[\text{SiLU}(W^{\text{gate}}_l \hat{h}_l') \odot W^{\text{up}}_l \hat{h}_l'\right] \\
\text{logits} &= W_E^T \cdot \text{RMSNorm}(h_{16})
\end{aligned}
}$$

---

*Document generated from analysis of meta-llama/Llama-3.2-1B-Instruct notebook.*
