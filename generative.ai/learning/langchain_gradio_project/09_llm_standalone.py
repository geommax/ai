"""
09 - LLM Standalone (Without RAG Pipeline)
RAG pipeline á€™á€á€½á€²á€á€„á€º LLM á€›á€²á€· raw knowledge á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€–á€­á€¯á€· standalone scriptá‹

á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º:
  - LLM á€€á€­á€¯ á€á€®á€¸á€á€”á€·á€º test á€œá€¯á€•á€ºá€•á€¼á€®á€¸ domain knowledge á€›á€¾á€­/á€™á€›á€¾á€­ á€…á€…á€ºá€†á€±á€¸á€á€šá€º
  - RAG á€‘á€Šá€·á€ºá€–á€­á€¯á€· á€œá€­á€¯á€¡á€•á€ºá€á€œá€¬á€¸ evaluate á€œá€¯á€•á€ºá€á€šá€º
  - Generation parameters (temperature, top_p, top_k, etc.) á€€á€­á€¯ tune á€œá€¯á€•á€ºá€œá€­á€¯á€·á€›á€á€šá€º

Usage:
  python 09_llm_standalone.py
"""

import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


# â”€â”€ Model Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MODEL_ID = "Qwen/Qwen2.5-3B-Instruct"

print(f"Loading model: {MODEL_ID} ...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    local_files_only=True,
)
print("Model loaded successfully!\n")


# â”€â”€ Generation Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_response(
    prompt: str,
    do_sample: bool,
    temperature: float,
    top_p: float,
    top_k: int,
    max_new_tokens: int,
) -> str:
    """
    LLM á€†á€®á€€á€­á€¯ prompt á€•á€­á€¯á€·á€•á€¼á€®á€¸ response á€•á€¼á€”á€ºá€šá€°á€á€šá€ºá‹
    RAG context á€™á€•á€«á€˜á€² model á€›á€²á€· built-in knowledge á€€á€­á€¯á€•á€² á€á€¯á€¶á€¸á€á€šá€ºá‹
    """
    if not prompt.strip():
        return "âš ï¸ Please enter a prompt."

    gen_kwargs: dict = {
        "max_new_tokens": int(max_new_tokens),
        "do_sample": do_sample,
    }

    # do_sample=True á€™á€¾á€á€¬ sampling parameters á€á€½á€± á€á€€á€ºá€†á€­á€¯á€„á€ºá€á€šá€º
    if do_sample:
        gen_kwargs["temperature"] = temperature
        gen_kwargs["top_p"] = top_p
        gen_kwargs["top_k"] = int(top_k)

    text_gen = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False,
        **gen_kwargs,
    )

    result = text_gen(prompt)
    return result[0]["generated_text"]


# â”€â”€ Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_standalone_interface() -> gr.Blocks:
    """LLM standalone testing á€¡á€á€½á€€á€º Gradio interface á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€šá€ºá‹"""

    with gr.Blocks(title="LLM Standalone Test") as demo:
        gr.Markdown("# ğŸ§  LLM Standalone Test")
        gr.Markdown(
            "RAG pipeline á€™á€•á€«á€˜á€² LLM á€›á€²á€· **raw knowledge** á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€á€²á€· tool á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹\n\n"
            f"**Model:** `{MODEL_ID}`\n\n"
            "á€’á€®á€™á€¾á€¬ question á€™á€±á€¸á€€á€¼á€Šá€·á€ºá€•á€¼á€®á€¸ model á€€ á€˜á€¬á€á€½á€±á€á€­á€œá€²áŠ "
            "á€€á€­á€¯á€šá€·á€º domain knowledge á€•á€«á€á€œá€¬á€¸ á€…á€…á€ºá€†á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹"
        )

        with gr.Row():
            # â”€â”€ Left: Input / Output â”€â”€
            with gr.Column(scale=3):
                prompt_input = gr.Textbox(
                    label="Prompt",
                    placeholder="Ask the LLM anything to test its raw knowledge...",
                    lines=4,
                )
                generate_btn = gr.Button("ğŸš€ Generate", variant="primary")
                response_output = gr.Textbox(
                    label="LLM Response",
                    lines=12,
                    interactive=False,
                )

            # â”€â”€ Right: Generation Parameters â”€â”€
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ Generation Parameters")

                do_sample = gr.Checkbox(
                    label="do_sample",
                    value=False,
                    info="True = sampling (creative), False = greedy (deterministic)",
                )
                temperature = gr.Slider(
                    minimum=0.01,
                    maximum=2.0,
                    value=0.7,
                    step=0.01,
                    label="Temperature",
                    info="Higher = more random, Lower = more focused",
                    interactive=True,
                )
                top_p = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.9,
                    step=0.01,
                    label="Top-p (nucleus sampling)",
                    info="Cumulative probability threshold",
                    interactive=True,
                )
                top_k = gr.Slider(
                    minimum=1,
                    maximum=100,
                    value=50,
                    step=1,
                    label="Top-k",
                    info="Top-k tokens to sample from",
                    interactive=True,
                )
                max_new_tokens = gr.Slider(
                    minimum=16,
                    maximum=2048,
                    value=512,
                    step=16,
                    label="Max New Tokens",
                    info="Generate á€œá€¯á€•á€ºá€™á€šá€·á€º max token á€¡á€›á€±á€¡á€á€½á€€á€º",
                    interactive=True,
                )

        # â”€â”€ Toggle sampling params visibility â”€â”€
        def toggle_sampling_params(is_sampling: bool):
            interactive = gr.update(interactive=is_sampling)
            return interactive, interactive, interactive

        do_sample.change(
            fn=toggle_sampling_params,
            inputs=do_sample,
            outputs=[temperature, top_p, top_k],
        )

        # â”€â”€ Generate button action â”€â”€
        generate_btn.click(
            fn=generate_response,
            inputs=[prompt_input, do_sample, temperature, top_p, top_k, max_new_tokens],
            outputs=response_output,
        )

        # â”€â”€ Enter key shortcut â”€â”€
        prompt_input.submit(
            fn=generate_response,
            inputs=[prompt_input, do_sample, temperature, top_p, top_k, max_new_tokens],
            outputs=response_output,
        )

        gr.Markdown("---")
        gr.Markdown(
            "ğŸ’¡ **Tip:** `do_sample=False` (greedy) á€†á€­á€¯á€›á€„á€º temperature, top_p, top_k á€á€½á€± "
            "effect á€™á€›á€¾á€­á€•á€«á€˜á€°á€¸á‹ Creative/diverse responses á€œá€­á€¯á€á€»á€„á€ºá€›á€„á€º `do_sample` á€€á€­á€¯ âœ… á€–á€½á€„á€·á€ºá€•á€«á‹"
        )

    return demo


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    demo = build_standalone_interface()
    demo.launch(theme=gr.themes.Soft())
