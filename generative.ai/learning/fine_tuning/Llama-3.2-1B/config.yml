# ============================================================
# Full Fine-Tuning Config: Llama-3.2-1B
# GPU: RTX 3050 12GB — TIGHT FIT ⚠️
# Expected VRAM: ~9-11 GB
# ============================================================

# Model
base_model: meta-llama/Llama-3.2-1B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# Tokenizer — pad_token setting
special_tokens:
  pad_token: "<|finetune_right_pad_id|>"

# Full Fine-Tuning
load_in_8bit: false
load_in_4bit: false

# Dataset
datasets:
  - path: /workspace/data/Llama-3.2-1B/train.jsonl
    type: alpaca

dataset_prepared_path: /workspace/data/Llama-3.2-1B/prepared
val_set_size: 0.1

# Training — Memory ချွေတာရန် conservative settings
output_dir: /workspace/data/Llama-3.2-1B/output

sequence_len: 512                   # ← 1024 ဆိုရင် OOM ဖြစ်နိုင်
num_epochs: 3
micro_batch_size: 1                 # ← 1 ပဲ သုံးပါ (OOM prevention)
gradient_accumulation_steps: 8      # ← effective batch = 8
eval_batch_size: 1

# Optimizer — memory-efficient optimizer
learning_rate: 2e-5
optimizer: adamw_torch_fused        # ← fused version, slightly less memory
lr_scheduler: cosine
weight_decay: 0.01
warmup_ratio: 0.1

# Precision
bf16: auto
tf32: true

# Memory Optimization — ALL enabled
gradient_checkpointing: true        # ← VRAM ~40% save (MUST for 1B FFT)
flash_attention: true               # ← VRAM + Speed boost
sample_packing: true
pad_to_sequence_len: true

# Logging
logging_steps: 1
eval_steps: 10
save_steps: 100
save_total_limit: 1                 # ← disk space save ဖို့

# Misc
seed: 42
strict: false
