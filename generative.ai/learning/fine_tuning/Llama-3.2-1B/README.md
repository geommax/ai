# Llama-3.2-1B â€” Full Fine-Tuning (Maximum)

## Overview

| Item | Detail |
|---|---|
| **Model** | [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) |
| **Parameters** | 1.24B |
| **Training Type** | Full Fine-Tuning (FFT) |
| **Expected VRAM** | ~9-11 GB âš ï¸ |
| **GPU** | RTX 3050 12GB |
| **á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º** | 12GB VRAM limit test (tight fit) |

## âš ï¸ Warning

á€’á€® model á€€ **12GB VRAM á€›á€²á€· limit á€”á€¬á€¸á€€á€•á€º** á€•á€«á€á€šá€ºá‹ OOM á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€¼á€± á€›á€¾á€­á€•á€«á€á€šá€ºá‹

- OOM á€–á€¼á€…á€ºá€›á€„á€º â†’ `sequence_len` á€€á€­á€¯ 256 á€‘á€­ á€œá€»á€¾á€±á€¬á€·á€•á€«
- á€’á€«á€œá€Šá€ºá€¸ á€™á€›á€›á€„á€º â†’ Qwen2.5-0.5B á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«
- **Llama 3.2 á€Ÿá€¬ gated model á€–á€¼á€…á€ºá€•á€«á€á€šá€º** â€” HuggingFace á€™á€¾á€¬ access request á€œá€¯á€•á€ºá€•á€¼á€®á€¸ accept á€›á€•á€«á€™á€šá€º

## Folder Structure

```
Llama-3.2-1B/
â”œâ”€â”€ README.md          â† á€’á€®á€–á€­á€¯á€„á€º
â”œâ”€â”€ config.yml         â† Axolotl training config
â”œâ”€â”€ train.jsonl        â† Test dataset (10 examples)
â”œâ”€â”€ prepared/          â† (auto) Preprocessed data cache
â””â”€â”€ output/            â† (auto) Trained model output
```

## Prerequisites

### Llama 3.2 Access Request

Llama 3.2 á€Ÿá€¬ gated model á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º HuggingFace á€™á€¾á€¬ access request á€œá€­á€¯á€•á€«á€á€šá€º:

1. https://huggingface.co/meta-llama/Llama-3.2-1B á€á€½á€¬á€¸á€•á€«
2. "Access Request" button á€”á€¾á€­á€•á€ºá€•á€«
3. License agreement accept á€œá€¯á€•á€ºá€•á€«
4. Approval á€›á€–á€­á€¯á€· á€…á€±á€¬á€„á€·á€ºá€•á€« (usually instant)

### HuggingFace Login

```bash
# Container á€‘á€²á€™á€¾á€¬ login á€œá€¯á€•á€ºá€•á€«
huggingface-cli login
# OR
hf auth login --token $HF_TOKEN
```

## Quick Start

### 1. Docker Container á€…á€á€„á€ºá€á€¼á€„á€ºá€¸

```bash
docker run --gpus '"all"' --rm -it \
  --shm-size=4g \
  -v /home/mr_cobot/Desktop/dev_projects/ai/generative.ai/learning/fine_tuning:/workspace/data \
  -v /home/mr_cobot/.cache/huggingface:/root/.cache/huggingface \
  axolotlai/axolotl:main-latest
```

### 2. HuggingFace Login (REQUIRED â€” gated model)

```bash
huggingface-cli login
```

### 3. Preprocess (Optional)

```bash
accelerate launch -m axolotl.cli.preprocess /workspace/data/Llama-3.2-1B/config.yml
```

### 4. Training Run

```bash
accelerate launch -m axolotl.cli.train /workspace/data/Llama-3.2-1B/config.yml
```

### 5. VRAM Monitoring (âš ï¸ Recommended â€” OOM á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º)

```bash
# á€”á€±á€¬á€€á€ºá€‘á€•á€º terminal á€™á€¾á€¬ â€” VRAM á€€á€­á€¯ closely monitor á€œá€¯á€•á€ºá€•á€«
docker ps
docker exec -it <container_id> bash
watch -n 1 nvidia-smi
```

### 6. Inference Test

```bash
accelerate launch -m axolotl.cli.inference /workspace/data/Llama-3.2-1B/config.yml
```

## Config Key Settings

```yaml
base_model: meta-llama/Llama-3.2-1B
special_tokens:
  pad_token: "<|finetune_right_pad_id|>"   # â† Llama 3 native pad token
sequence_len: 512                           # â† 1024 á€†á€­á€¯á€›á€„á€º OOM á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º
micro_batch_size: 1                         # â† absolute minimum
gradient_accumulation_steps: 8
optimizer: adamw_torch_fused                # â† memory-efficient optimizer
gradient_checkpointing: true                # â† MUST for 1B FFT on 12GB
flash_attention: true
sample_packing: true
bf16: auto
```

## VRAM Estimate

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Llama-3.2-1B FFT â€” VRAM Breakdown    âš ï¸ TIGHT  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model Weights (bf16):       ~2.5 GB             â”‚
â”‚ Gradients:                  ~2.5 GB             â”‚
â”‚ Optimizer States (AdamW):   ~5.0 GB             â”‚
â”‚ Activations + Overhead:     ~1.0-2.0 GB         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total:                      ~9-11 GB / 12 GB    â”‚
â”‚ Headroom:                   ~1-3 GB âš ï¸          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## OOM á€–á€¼á€…á€ºá€›á€„á€º á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€Šá€ºá€¸

á€¡á€±á€¬á€€á€ºá€•á€« settings á€á€½á€±á€€á€­á€¯ **á€¡á€…á€‰á€ºá€œá€­á€¯á€€á€º** á€•á€¼á€„á€ºá€•á€«:

### Fix 1: Sequence Length á€œá€»á€¾á€±á€¬á€·

```yaml
sequence_len: 256       # 512 â†’ 256
```

### Fix 2: Sample Packing á€•á€­á€á€º

```yaml
sample_packing: false
```

### Fix 3: Optimizer á€•á€¼á€±á€¬á€„á€ºá€¸

```yaml
# Adafactor â€” optimizer states memory ~50% save
optimizer: adafactor

# OR 8-bit AdamW
optimizer: adamw_bnb_8bit
```

### Fix 4: á€’á€«á€á€½á€± á€¡á€¬á€¸á€œá€¯á€¶á€¸ á€™á€›á€›á€„á€º

â†’ **Qwen2.5-0.5B** folder á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€« (VRAM ~5-6GB á€•á€² á€€á€¯á€”á€ºá€•á€«á€á€šá€º)

## Notes

- âš ï¸ **VRAM limit** á€”á€¬á€¸á€€á€•á€ºá€•á€«á€á€šá€º â€” monitor closely
- ğŸ”‘ **Gated model** â€” HF login + access approval á€œá€­á€¯á€•á€«á€á€šá€º
- SmolLM2-135M â†’ Qwen2.5-0.5B â†’ á€’á€® model (**á€¡á€…á€‰á€ºá€œá€­á€¯á€€á€º run** á€•á€«)
- Production á€¡á€á€½á€€á€º 1B model FFT á€‘á€€á€º **QLoRA + 7B-8B model** á€€á€­á€¯ recommend á€•á€«á€á€šá€º
