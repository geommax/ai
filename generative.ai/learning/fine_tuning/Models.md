# AI Model á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸ á€”á€¾á€„á€·á€º Suffix á€€á€¼á€Šá€·á€ºá€•á€¼á€®á€¸ á€á€½á€²á€á€¼á€¬á€¸á€”á€Šá€ºá€¸

## Model á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸ (Categories)

AI Models á€á€½á€±á€€á€­á€¯ á€¡á€“á€­á€€ **á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ á… á€™á€»á€­á€¯á€¸** á€á€½á€²á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

| Category | Description | Examples |
|---|---|---|
| **LLM** (Large Language Model) | Text generation, reasoning, chat | LLaMA, Mistral, GPT |
| **VLM** (Vision-Language Model) | Image + Text understanding | LLaVA, Qwen-VL, InternVL |
| **Speech/Audio Model** | Speech recognition, TTS, audio understanding | Whisper, Bark, SeamlessM4T |
| **Vision Model** | Image classification, detection, segmentation | ViT, DINO, SAM |
| **Multimodal Model** | Multiple modalities (text + image + audio + video) | GPT-4o, Gemini, Any-to-Any |

---

## Model Suffix á€€á€¼á€Šá€·á€ºá€•á€¼á€®á€¸ Type á€á€½á€²á€á€¼á€¬á€¸á€”á€Šá€ºá€¸

Model name á€›á€²á€· **suffix** (á€”á€±á€¬á€€á€ºá€†á€€á€º) á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€›á€¯á€¶á€”á€²á€· model á€›á€²á€· training stage, purpose, quantization level á€€á€­á€¯ á€á€½á€²á€á€¼á€¬á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

### ğŸ·ï¸ 1. Training Stage Suffixes

Model á€€á€­á€¯ á€˜á€šá€º training stage á€¡á€‘á€­ á€œá€¯á€•á€ºá€‘á€¬á€¸á€œá€² á€•á€¼á€á€²á€· suffixes:

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| **(suffix á€™á€›á€¾á€­)** | Base / Pretrained Model | Raw pretrained model, next-token prediction á€á€¬ train á€‘á€¬á€¸ | `meta-llama/Llama-3.1-8B` |
| `-base` | Base Model | Pretrained model á€–á€¼á€…á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ explicitly á€•á€¼á€‘á€¬á€¸ | `Qwen/Qwen2.5-7B-Base` |
| `-Instruct` | Instruction-tuned | Instruction following á€¡á€á€½á€€á€º fine-tune á€‘á€¬á€¸á€•á€¼á€®á€¸ | `meta-llama/Llama-3.1-8B-Instruct` |
| `-Chat` | Chat-optimized | Multi-turn chat conversation á€¡á€á€½á€€á€º optimize á€‘á€¬á€¸ | `Qwen/Qwen2-7B-Chat` |
| `-it` | Instruction-tuned | `-Instruct` á€›á€²á€· á€¡á€á€­á€¯á€€á€±á€¬á€€á€º (Google models) | `google/gemma-2-9b-it` |
| `-hf` | Hugging Face format | HF Transformers library á€”á€²á€· compatible format | `tiiuae/falcon-7b-hf` |

---

### ğŸ·ï¸ 2. Alignment / Safety Suffixes

Model á€€á€­á€¯ alignment / safety training á€˜á€šá€ºá€œá€±á€¬á€€á€ºá€‘á€­ á€œá€¯á€•á€ºá€‘á€¬á€¸á€œá€² á€•á€¼á€á€²á€· suffixes:

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| `-RLHF` | Reinforcement Learning from Human Feedback | Human preference data á€”á€²á€· align á€‘á€¬á€¸ | `Llama-2-7b-chat-RLHF` |
| `-DPO` | Direct Preference Optimization | RLHF á€›á€²á€· simpler alternative á€”á€²á€· align á€‘á€¬á€¸ | `NousResearch/Hermes-2-Pro-Llama-3-8B-DPO` |
| `-ORPO` | Odds Ratio Preference Optimization | SFT + alignment á€€á€­á€¯ single stage á€™á€¾á€¬ á€œá€¯á€•á€º | `mlabonne/OrpoLlama-3-8B` |
| `-KTO` | Kahneman-Tversky Optimization | Unpaired preference data á€”á€²á€· align á€‘á€¬á€¸ | `model-kto` |
| `-PPO` | Proximal Policy Optimization | Classic RL algorithm á€”á€²á€· align á€‘á€¬á€¸ | `model-ppo` |
| `-SimPO` | Simple Preference Optimization | Reference-free preference optimization | `model-simpo` |

---

### ğŸ·ï¸ 3. Fine-Tuning Method Suffixes

á€˜á€šá€º fine-tuning method á€á€¯á€¶á€¸á€‘á€¬á€¸á€œá€² á€•á€¼á€á€²á€· suffixes:

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| `-SFT` | Supervised Fine-Tuning | Labeled data á€”á€²á€· supervised train á€‘á€¬á€¸ | `model-7B-SFT` |
| `-LoRA` | LoRA adapter | LoRA fine-tune á€‘á€¬á€¸á€á€²á€· adapter weights | `model-7B-LoRA` |
| `-QLoRA` | Quantized LoRA | 4-bit quantized + LoRA | `model-7B-QLoRA` |
| `-merged` | Merged adapter | LoRA adapter á€€á€­á€¯ base model á€‘á€² merge á€‘á€¬á€¸á€•á€¼á€®á€¸ | `model-7B-LoRA-merged` |
| `-FT` | Fine-Tuned | Full fine-tuning á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€²á€· model | `model-7B-FT` |
| `-adapter` | Adapter weights only | Adapter weights á€á€®á€¸á€á€”á€·á€º (base model á€™á€•á€«) | `model-7B-adapter` |

---

### ğŸ·ï¸ 4. Quantization Suffixes

Model á€›á€²á€· precision / quantization level á€•á€¼á€á€²á€· suffixes:

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | Size Reduction | Quality | á€¥á€•á€™á€¬ |
|---|---|---|---|---|
| `-fp32` | 32-bit floating point | Baseline | Highest | `model-fp32` |
| `-fp16` | 16-bit floating point | 2Ã— smaller | Near-original | `model-fp16` |
| `-bf16` | Brain floating point 16 | 2Ã— smaller | Near-original (better range) | `model-bf16` |
| `-int8` | 8-bit integer | 4Ã— smaller | Slight loss | `model-int8` |
| `-int4` | 4-bit integer | 8Ã— smaller | Moderate loss | `model-int4` |
| `-GPTQ` | GPTQ quantization | 4-8Ã— smaller | Good (post-training quant) | `TheBloke/Llama-2-7B-GPTQ` |
| `-AWQ` | Activation-aware Weight Quantization | 4-8Ã— smaller | Better than GPTQ | `TheBloke/Llama-2-7B-AWQ` |
| `-GGUF` | GGML Universal Format | Variable | llama.cpp compatible | `model-Q4_K_M.gguf` |
| `-EXL2` | ExLlamaV2 format | Variable | ExLlamaV2 compatible | `model-EXL2` |
| `-bnb` / `-4bit` | BitsAndBytes quantization | 4-8Ã— smaller | Runtime quantization | `model-bnb-4bit` |

#### GGUF Quantization Levels

GGUF files á€á€½á€±á€™á€¾á€¬ quantization level á€€á€­á€¯ filename á€™á€¾á€¬ á€•á€¼á€•á€«á€á€šá€º:

| Quant Type | Bits | Quality | Size (7B Model) | á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€„á€·á€ºá€á€²á€· á€¡á€á€¼á€±á€¡á€”á€± |
|---|---|---|---|---|
| `Q2_K` | 2-bit | Low | ~2.8 GB | Memory á€¡á€›á€™á€ºá€¸á€”á€Šá€ºá€¸á€á€²á€·á€¡á€á€« |
| `Q3_K_S/M/L` | 3-bit | Fair | ~3.2-3.8 GB | Mobile / Edge devices |
| `Q4_0` | 4-bit | Good | ~3.8 GB | Standard quantization |
| `Q4_K_S/M` | 4-bit | Better | ~3.8-4.1 GB | **á€¡á€á€¯á€¶á€¸á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ (recommended)** |
| `Q5_0` | 5-bit | Very Good | ~4.6 GB | Quality á€¦á€¸á€…á€¬á€¸á€•á€±á€¸á€›á€„á€º |
| `Q5_K_S/M` | 5-bit | Very Good+ | ~4.6-4.8 GB | Quality + reasonable size |
| `Q6_K` | 6-bit | Excellent | ~5.5 GB | Near-original quality |
| `Q8_0` | 8-bit | Near-perfect | ~7.2 GB | Maximum quality quantization |
| `F16` | 16-bit | Original | ~14 GB | Full precision |

---

### ğŸ·ï¸ 5. Model Size Suffixes

Model parameter count á€•á€¼á€á€²á€· suffixes:

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€¥á€•á€™á€¬ |
|---|---|---|
| `-1B`, `-3B`, `-7B`, `-8B` | Billion parameters | `Llama-3.1-8B` |
| `-0.5B`, `-1.5B` | Sub-billion / small models | `Qwen2.5-0.5B` |
| `-13B`, `-14B` | Medium models | `Llama-2-13B` |
| `-30B`, `-34B`, `-35B` | Large models | `Yi-34B` |
| `-70B`, `-72B` | Very large models | `Llama-3.1-70B` |
| `-405B` | Ultra-large models | `Llama-3.1-405B` |
| `-MoE` | Mixture of Experts | Active parameters < total | `Mixtral-8x7B` |
| `-A14B` | Active parameters (MoE) | `14B` active out of total | `Qwen2.5-A14B` |

---

### ğŸ·ï¸ 6. Vision / Multimodal Suffixes

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| `-VL` | Vision-Language | Image + Text understanding | `Qwen/Qwen2-VL-7B-Instruct` |
| `-Vision` | Vision capable | Image understanding | `model-Vision` |
| `-LLaVA` | LLaVA architecture | Visual instruction tuning | `liuhaotian/llava-v1.6-mistral-7b` |
| `-MM` | Multimodal | Multiple modalities support | `model-MM` |
| `-Omni` | Omni-modal | Text + Image + Audio + Video | `model-Omni` |

---

### ğŸ·ï¸ 7. Special Purpose Suffixes

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| `-Coder` / `-Code` | Code generation | Code generation á€¡á€á€½á€€á€º specialized | `Qwen2.5-Coder-7B` |
| `-Math` | Mathematics | Math reasoning á€¡á€á€½á€€á€º | `Qwen2.5-Math-7B` |
| `-Med` / `-Medical` | Medical domain | Medical knowledge | `model-Med` |
| `-Legal` | Legal domain | Legal text processing | `model-Legal` |
| `-Finance` | Finance domain | Financial analysis | `model-Finance` |
| `-RP` | Roleplay | Roleplay/Character chat | `model-RP` |
| `-Uncensored` | Uncensored | Safety filters á€–á€¼á€¯á€á€ºá€‘á€¬á€¸ | `model-Uncensored` |
| `-Abliterated` | Abliterated | Refusal behavior á€–á€šá€ºá€›á€¾á€¬á€¸á€‘á€¬á€¸ | `model-abliterated` |
| `-Turbo` | Turbo/Fast | Speed optimized | `model-Turbo` |
| `-Mini` / `-Nano` | Small variant | Smaller, faster version | `Phi-3-mini` |
| `-Pro` / `-Plus` | Enhanced variant | Better performance version | `Gemma-2-Pro` |
| `-Preview` | Preview/Beta | Testing release | `model-Preview` |
| `-Long` | Long context | Extended context window | `model-Long` |

---

### ğŸ·ï¸ 8. Version Suffixes

| Suffix | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º | á€¥á€•á€™á€¬ |
|---|---|---|
| `-v1`, `-v1.5`, `-v2` | Version number | `llava-v1.6-mistral-7b` |
| `.1`, `.2`, `.3` | Sub-version (in model family) | `Llama-3.1`, `Qwen2.5` |
| `-2025xxxx` | Date-based version | `model-20250201` |

---

## Model Name Anatomy - á€¥á€•á€™á€¬á€”á€²á€· á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€á€¼á€„á€ºá€¸

### á€¥á€•á€™á€¬ á: `meta-llama/Llama-3.1-8B-Instruct`

```
meta-llama  /  Llama-3.1  -  8B      -  Instruct
â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€
Organization   Model v3.1   8 Billion   Instruction-tuned
                             params
```

### á€¥á€•á€™á€¬ á‚: `TheBloke/Llama-2-13B-Chat-GPTQ`

```
TheBloke  /  Llama-2  -  13B       -  Chat    -  GPTQ
â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€          â”€â”€â”€â”€       â”€â”€â”€â”€
Quantizer    Model v2    13 Billion   Chat-opt   GPTQ quantized
```

### á€¥á€•á€™á€¬ áƒ: `Qwen/Qwen2.5-72B-Instruct-AWQ`

```
Qwen  /  Qwen2.5  -  72B        -  Instruct        -  AWQ
â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€
Org      v2.5       72 Billion     Instruction-tuned   AWQ quantized
```

### á€¥á€•á€™á€¬ á„: `NousResearch/Hermes-2-Pro-Llama-3-8B-DPO`

```
NousResearch / Hermes-2-Pro - Llama-3 - 8B  - DPO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€   â”€â”€    â”€â”€â”€
Organization   Fine-tune       Base     Size  Alignment
               name            model          method
```

### á€¥á€•á€™á€¬ á…: `liuhaotian/llava-v1.6-mistral-7b-hf`

```
liuhaotian / llava-v1.6 - mistral - 7b - hf
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€   â”€â”€   â”€â”€
Org          VLM v1.6     Base LLM  Size HuggingFace
                          backbone       format
```

---

## LLM Models - á€¡á€á€±á€¸á€…á€­á€á€º á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸

### Open-Source LLM Families

| Model Family | Organization | Sizes | License | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|---|
| **LLaMA 3.1 / 3.2 / 3.3** | Meta | 1B, 3B, 8B, 70B, 405B | Llama License | á€¡á€€á€»á€šá€ºá€•á€¼á€”á€·á€ºá€†á€¯á€¶á€¸ open-source LLM |
| **Qwen 2.5** | Alibaba | 0.5B - 72B | Apache 2.0 / Qwen | Multilingual, Code, Math variants |
| **Mistral / Mixtral** | Mistral AI | 7B, 8x7B, 8x22B | Apache 2.0 | MoE architecture, efficient |
| **Gemma 2** | Google | 2B, 9B, 27B | Gemma License | Lightweight, efficient |
| **Phi-3 / Phi-4** | Microsoft | 3.8B, 7B, 14B | MIT | Small but powerful (SLM) |
| **Yi** | 01.AI | 6B, 9B, 34B | Apache 2.0 | Strong bilingual (EN/ZH) |
| **DeepSeek V3** | DeepSeek | 671B (MoE) | MIT | MoE, cost-efficient training |
| **Command R+** | Cohere | 35B, 104B | CC-BY-NC | RAG optimized |
| **OLMo** | AI2 | 1B, 7B, 13B | Apache 2.0 | Fully open (data + code + weights) |
| **Falcon** | TII | 7B, 40B, 180B | Apache 2.0 | Early open-source pioneer |
| **InternLM 2.5** | Shanghai AI Lab | 7B, 20B | Apache 2.0 | Strong reasoning |
| **StarCoder 2** | BigCode | 3B, 7B, 15B | BigCode OpenRAIL-M | Code generation specialized |

---

## Vision-Language Models (VLMs)

### VLM Architecture Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VLM Architecture               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Vision   â”‚â”€â”€â”€â†’â”‚ Connector â”‚â”€â”€â”€â†’â”‚    LLM    â”‚  â”‚
â”‚  â”‚ Encoder   â”‚    â”‚ (Bridge)  â”‚    â”‚ Backbone  â”‚  â”‚
â”‚  â”‚ (ViT etc) â”‚    â”‚           â”‚    â”‚           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚
â”‚  Image input      Feature        Text output      â”‚
â”‚                   alignment                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Open-Source VLMs

| Model | Organization | LLM Backbone | Vision Encoder | Sizes | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|---|---|
| **LLaVA 1.6 (NeXT)** | LLaVA Team | Mistral/Vicuna/LLaMA | CLIP ViT-L | 7B, 13B, 34B | Pioneer open VLM |
| **Qwen2-VL** | Alibaba | Qwen2 | ViT (native) | 2B, 7B, 72B | Video understanding á€•á€« |
| **InternVL 2.5** | Shanghai AI Lab | InternLM2 | InternViT-6B | 1B - 78B | Strong OCR + document |
| **Llama 3.2 Vision** | Meta | Llama 3.2 | ViT | 11B, 90B | Official Meta VLM |
| **Phi-3-Vision** | Microsoft | Phi-3 | CLIP ViT | 4.2B | Small but capable |
| **DeepSeek-VL 2** | DeepSeek | DeepSeek MoE | SigLIP | 4.5B, 28B | MoE VLM |
| **CogVLM2** | Zhipu AI | LLaMA2/ChatGLM | EVA2-CLIP | 19B | High-res understanding |
| **Idefics3** | Hugging Face | Llama 3.1 | SigLIP | 8B | Native HF integration |
| **MiniCPM-V** | OpenBMB | MiniCPM | SigLIP | 3B, 8B | Mobile-friendly VLM |
| **Pixtral** | Mistral AI | Mistral | Custom ViT | 12B | Mistral's VLM |

---

## Speech / Audio Models

### Speech Model Types

| Type | Description | Models |
|---|---|---|
| **ASR** (Automatic Speech Recognition) | Speech â†’ Text | Whisper, wav2vec2, Conformer |
| **TTS** (Text-to-Speech) | Text â†’ Speech | VITS, Bark, XTTS, F5-TTS |
| **Voice Cloning** | Voice replication | XTTS, OpenVoice, RVC |
| **Speech Translation** | Speech â†’ Translated text | SeamlessM4T, Whisper |
| **Audio Understanding** | Audio analysis + QA | Qwen2-Audio, SALMONN |
| **Music Generation** | Text â†’ Music | MusicGen, Stable Audio |
| **Sound Effect** | Text â†’ Sound effects | AudioGen, Make-An-Audio |

### Key Speech/Audio Models

| Model | Organization | Task | Sizes | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|---|
| **Whisper** | OpenAI | ASR + Translation | tiny - large-v3 | Multilingual ASR, 99 languages |
| **Seamless M4T v2** | Meta | Speech â†” Text Translation | 2.3B | Multimodal translation |
| **Bark** | Suno | TTS | 1.3B | Multilingual, music, sound effects |
| **XTTS v2** | Coqui | TTS + Voice Clone | ~1B | 17 languages, voice cloning |
| **Wav2Vec 2.0** | Meta | ASR | 300M | Self-supervised speech |
| **Qwen2-Audio** | Alibaba | Audio Understanding | 7B | Audio QA, multi-type audio |
| **VALL-E X** | Microsoft | TTS + Clone | - | Zero-shot voice synthesis |
| **F5-TTS** | Community | TTS | ~300M | Fast, high quality |
| **Parler-TTS** | Hugging Face | TTS | 600M, 2.3B | Describable TTS |

---

## Vision Models (Image-only)

### Vision Model Types

| Type | Description | Models |
|---|---|---|
| **Classification** | Image â†’ Label | ViT, ConvNeXt, EfficientNet |
| **Object Detection** | Image â†’ Bounding boxes | YOLO, DETR, RT-DETR |
| **Segmentation** | Image â†’ Pixel-level masks | SAM, Mask2Former |
| **Image Generation** | Text â†’ Image | Stable Diffusion, FLUX, DALL-E |
| **Image Editing** | Image modification | InstructPix2Pix |
| **Super Resolution** | Low-res â†’ High-res | Real-ESRGAN, SwinIR |
| **Depth Estimation** | Image â†’ Depth map | Depth Anything, MiDaS |
| **OCR** | Image â†’ Text extraction | TrOCR, PaddleOCR, EasyOCR |

### Key Vision Models

| Model | Organization | Task | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|
| **Stable Diffusion XL/3** | Stability AI | Image Generation | Open-source image gen |
| **FLUX** | Black Forest Labs | Image Generation | SD successor, high quality |
| **SAM 2** | Meta | Segmentation | Segment Anything (image + video) |
| **YOLO v11** | Ultralytics | Object Detection | Real-time detection |
| **DINOv2** | Meta | Visual Features | Self-supervised vision backbone |
| **ViT** | Google | Classification | Vision Transformer |
| **Depth Anything v2** | HKU | Depth Estimation | Monocular depth |
| **RT-DETR** | Baidu | Object Detection | Real-time DETR |

---

## Embedding Models

| Model | Organization | Dimensions | Max Tokens | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|---|
| **BGE-M3** | BAAI | 1024 | 8192 | Multilingual, multi-granularity |
| **E5-Mistral-7B** | Microsoft | 4096 | 32768 | LLM-based embedding |
| **GTE-Qwen2** | Alibaba | 768-1536 | 8192 | Strong multilingual |
| **Nomic-Embed-Text** | Nomic AI | 768 | 8192 | Open-source, efficient |
| **jina-embeddings-v3** | Jina AI | 1024 | 8192 | Task-specific embeddings |
| **Snowflake-Arctic-Embed** | Snowflake | 768-1024 | 512 | Retrieval optimized |

---

## Axolotl á€™á€¾á€¬ Train á€œá€­á€¯á€·á€›á€™á€šá€·á€º Model Types

### âœ… Fully Supported (Direct Training)

Axolotl á€Ÿá€¬ **Hugging Face Transformers** library á€•á€±á€«á€ºá€™á€¾á€¬ á€¡á€á€¼á€±á€á€¶á€‘á€¬á€¸á€á€²á€·á€¡á€á€½á€€á€º, Transformers compatible á€–á€¼á€…á€ºá€á€²á€· **causal LLM** (decoder-only) models á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ train á€œá€­á€¯á€·á€›á€•á€«á€á€šá€ºá‹

| Model Architecture | Models | Axolotl Support Level |
|---|---|---|
| **LlamaForCausalLM** | LLaMA 2/3/3.1/3.2/3.3, CodeLlama, Vicuna, Yi | âœ… Full (First-class) |
| **MistralForCausalLM** | Mistral 7B, Zephyr | âœ… Full |
| **MixtralForCausalLM** | Mixtral 8x7B, 8x22B | âœ… Full (MoE support) |
| **Qwen2ForCausalLM** | Qwen2, Qwen2.5 series | âœ… Full |
| **GemmaForCausalLM** | Gemma, Gemma 2 | âœ… Full |
| **Phi3ForCausalLM** | Phi-3, Phi-3.5 | âœ… Full |
| **GPTNeoXForCausalLM** | Pythia, RedPajama | âœ… Full |
| **FalconForCausalLM** | Falcon 7B, 40B, 180B | âœ… Full |
| **GPT2LMHeadModel** | GPT-2 series | âœ… Full |
| **MPTForCausalLM** | MPT-7B, MPT-30B | âœ… Full |
| **StableLMForCausalLM** | StableLM 2 | âœ… Full |
| **InternLM2ForCausalLM** | InternLM 2, 2.5 | âœ… Full |
| **DeepseekV2ForCausalLM** | DeepSeek V2, V3 | âœ… Supported (MoE) |
| **CohereForCausalLM** | Command R/R+ | âœ… Supported |
| **OlmoForCausalLM** | OLMo | âœ… Supported |
| **StarCoder2ForCausalLM** | StarCoder 2 | âœ… Supported |
| **Starcoder2ForCausalLM** | StarCoder 2 | âœ… Supported |

### âš ï¸ Partially Supported (VLMs - Multimodal)

Axolotl á€™á€¾á€¬ Vision-Language Models á€á€á€»á€­á€¯á€·á€€á€­á€¯ train á€œá€­á€¯á€· á€›á€•á€«á€á€šá€º (experimental/growing support):

| Model | Architecture | Support Status | á€™á€¾á€á€ºá€á€»á€€á€º |
|---|---|---|---|
| **LLaVA 1.5/1.6** | LlavaForConditionalGeneration | âš ï¸ Supported | Visual instruction tuning |
| **Qwen2-VL** | Qwen2VLForConditionalGeneration | âš ï¸ Experimental | Vision-Language training |
| **Pixtral** | PixtralForConditionalGeneration | âš ï¸ Experimental | Mistral VLM |
| **Llama 3.2 Vision** | MllamaForConditionalGeneration | âš ï¸ Experimental | Meta VLM |

#### Axolotl VLM Training Config Example

```yaml
base_model: llava-hf/llava-v1.6-mistral-7b-hf
model_type: LlavaForConditionalGeneration
adapter: lora
lora_r: 16
lora_alpha: 32

datasets:
  - path: dataset_path
    type: llava
```

### âŒ Not Supported (Direct Training)

| Model Type | Reason | Alternative |
|---|---|---|
| **Encoder-only** (BERT, RoBERTa) | Axolotl is for causal/autoregressive LMs | HF Trainer / custom script |
| **Encoder-Decoder** (T5, BART, mBART) | Architecture mismatch | HF Seq2SeqTrainer |
| **Speech Models** (Whisper, Wav2Vec) | Different modality | HF Trainer + custom data |
| **Diffusion Models** (SD, FLUX) | Completely different training paradigm | Kohya, diffusers library |
| **Embedding Models** (BGE, E5) | Different training objective | Sentence-transformers |
| **Vision-only** (ViT, YOLO, SAM) | Not language models | Timm, Ultralytics |
| **GGUF / GGML models** | Quantized inference format | Convert to HF format first |
| **GPTQ models** | Post-training quantized | á€¡á€á€€á€ºá€¡á€á€²á€›á€¾á€­ (QLoRA á€€á€­á€¯ base model á€”á€²á€· á€á€¯á€¶á€¸á€•á€«) |
| **AWQ models** | Post-training quantized | QLoRA with base model instead |
| **EXL2 models** | ExLlamaV2 inference format | Not trainable |

---

## Axolotl Model Selection Guide

### Training á€œá€¯á€•á€ºá€™á€šá€·á€º Model á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

```
á€˜á€¬ Task á€¡á€á€½á€€á€ºá€œá€²?
â”‚
â”œâ”€â”€ ğŸ’¬ General Chat / Instruction Following
â”‚   â”œâ”€â”€ GPU 8-16GB  â†’ Qwen2.5-3B / Phi-3-mini-4k / Gemma-2-2B
â”‚   â”œâ”€â”€ GPU 24GB    â†’ Llama-3.1-8B / Mistral-7B / Qwen2.5-7B
â”‚   â”œâ”€â”€ GPU 48GB    â†’ Qwen2.5-14B / InternLM2.5-20B
â”‚   â””â”€â”€ GPU 80GB+   â†’ Llama-3.1-70B / Qwen2.5-72B
â”‚
â”œâ”€â”€ ğŸ’» Code Generation
â”‚   â”œâ”€â”€ Small      â†’ Qwen2.5-Coder-3B / StarCoder2-3B
â”‚   â”œâ”€â”€ Medium     â†’ Qwen2.5-Coder-7B / CodeLlama-7B
â”‚   â””â”€â”€ Large      â†’ Qwen2.5-Coder-14B+ / DeepSeek-Coder-V2
â”‚
â”œâ”€â”€ ğŸ”¢ Math / Reasoning
â”‚   â”œâ”€â”€ Small      â†’ Qwen2.5-Math-1.5B
â”‚   â”œâ”€â”€ Medium     â†’ Qwen2.5-Math-7B
â”‚   â””â”€â”€ Large      â†’ Qwen2.5-Math-72B
â”‚
â”œâ”€â”€ ğŸ–¼ï¸ Vision + Language (VLM)
â”‚   â”œâ”€â”€ Small      â†’ MiniCPM-V-2.5 / Phi-3-Vision
â”‚   â”œâ”€â”€ Medium     â†’ LLaVA-v1.6-7B / Qwen2-VL-7B
â”‚   â””â”€â”€ Large      â†’ InternVL2.5-78B / Qwen2-VL-72B
â”‚
â””â”€â”€ ğŸŒ Multilingual
    â”œâ”€â”€ Small      â†’ Qwen2.5-3B
    â”œâ”€â”€ Medium     â†’ Qwen2.5-7B / Llama-3.1-8B
    â””â”€â”€ Large      â†’ Qwen2.5-72B
```

### Base Model vs Instruct Model á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

| Scenario | á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€™á€šá€·á€º Model | á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€á€»á€€á€º |
|---|---|---|
| Custom chat style á€–á€”á€ºá€á€®á€¸á€á€»á€„á€ºá€›á€„á€º | **Base model** | Instruct training á€›á€²á€· bias á€™á€›á€¾á€­ |
| Domain-specific knowledge á€‘á€Šá€·á€ºá€á€»á€„á€ºá€›á€„á€º | **Base model** | Clean continued pretraining |
| Existing chat format á€€á€­á€¯ á€•á€¼á€„á€ºá€á€»á€„á€ºá€›á€„á€º | **Instruct model** | Chat capability á€›á€¾á€­á€•á€¼á€®á€¸á€á€¬á€¸ |
| Task-specific fine-tune á€á€»á€„á€ºá€›á€„á€º | **Instruct model** | Instruction following ability á€›á€¾á€­á€•á€¼á€®á€¸á€á€¬á€¸ |
| DPO/RLHF alignment á€œá€¯á€•á€ºá€á€»á€„á€ºá€›á€„á€º | **SFT model** | SFT á€•á€¼á€®á€¸á€á€¬á€¸ model á€€á€­á€¯ align |

---

## Model Format Compatibility

### Axolotl Compatible Formats

| Format | Trainable? | Load Method | á€™á€¾á€á€ºá€á€»á€€á€º |
|---|---|---|---|
| **HuggingFace safetensors** | âœ… Yes | `base_model: org/model` | **Recommended format** |
| **HuggingFace bin (pytorch)** | âœ… Yes | `base_model: org/model` | Legacy format |
| **BitsAndBytes 4-bit** | âœ… Yes (QLoRA) | `load_in_4bit: true` | Runtime quantization |
| **BitsAndBytes 8-bit** | âœ… Yes | `load_in_8bit: true` | Runtime quantization |
| **GPTQ** | âš ï¸ Limited | `gptq: true` | Training quality concerns |
| **AWQ** | âŒ No | - | Inference only format |
| **GGUF** | âŒ No | - | llama.cpp format |
| **EXL2** | âŒ No | - | ExLlamaV2 format |
| **ONNX** | âŒ No | - | Inference only format |
| **TensorRT** | âŒ No | - | NVIDIA inference only |

### Axolotl Inference/Serving Compatible Tools

Fine-tune á€•á€¼á€®á€¸á€á€²á€· model á€€á€­á€¯ serve/deploy á€œá€¯á€•á€ºá€–á€­á€¯á€·:

| Tool | Format Required | Speed | á€‘á€°á€¸á€á€¼á€¬á€¸á€á€»á€€á€º |
|---|---|---|---|
| **vLLM** | HF / AWQ / GPTQ | Fast | Production serving, batching |
| **llama.cpp** | GGUF | Medium | CPU/Metal inference |
| **TGI** | HF / AWQ / GPTQ | Fast | HuggingFace serving |
| **Ollama** | GGUF | Easy | Local deployment |
| **ExLlamaV2** | EXL2 / GPTQ | Very Fast | Consumer GPU optimized |
| **SGLang** | HF | Very Fast | Structured generation |

> ğŸ’¡ **Tip:** Axolotl á€”á€²á€· train á€•á€¼á€®á€¸á€á€²á€· LoRA adapter á€€á€­á€¯ base model á€‘á€² merge á€œá€¯á€•á€ºá€•á€¼á€®á€¸ â†’  á€€á€¼á€­á€¯á€€á€ºá€á€²á€· format (GGUF, AWQ, GPTQ, EXL2) á€€á€­á€¯ convert á€œá€¯á€•á€ºá€•á€¼á€®á€¸ deploy á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```bash
# Merge LoRA adapter into base model
python -m axolotl.cli.merge_lora your_config.yml --lora_model_dir="./outputs"

# Convert to GGUF for Ollama/llama.cpp
python convert_hf_to_gguf.py ./merged_model --outtype q4_k_m
```
