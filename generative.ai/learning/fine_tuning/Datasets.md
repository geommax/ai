# Dataset Format Preparation - Training Process Type á€¡á€œá€­á€¯á€€á€º

## Overview

Fine-tuning á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€« **Training Process Type** á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€¼á€®á€¸ dataset format á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€±á€¸á€›á€•á€«á€á€šá€ºá‹ Model type á€á€°á€á€°á€•á€² á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º training objective á€€á€½á€¬á€›á€„á€º dataset format á€€á€½á€¬á€•á€«á€á€šá€ºá‹

```
Dataset Format á€€á€­á€¯ á€˜á€¬á€€ á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€œá€²?
â”‚
â”œâ”€â”€ 1. Training Process Type (CPT, SFT, DPO, ORPO...)
â”œâ”€â”€ 2. Model Type (LLM, VLM, Speech...)
â””â”€â”€ 3. Axolotl Dataset Type Configuration
```

---

## Training Process Types á€”á€¾á€„á€·á€º Dataset Formats

### ğŸ“Š Training Process Type Summary

| Training Type | á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º | Dataset Format | Axolotl Support |
|---|---|---|---|
| **CPT** (Continued Pre-Training) | Domain knowledge á€‘á€Šá€·á€ºá€á€¼á€„á€ºá€¸ | Raw text / Completion | âœ… |
| **SFT** (Supervised Fine-Tuning) | Instruction following á€á€„á€ºá€á€¼á€„á€ºá€¸ | Instruction-Response pairs | âœ… |
| **DPO** (Direct Preference Optimization) | Human preference alignment | Chosen/Rejected pairs | âœ… |
| **ORPO** (Odds Ratio Preference Optimization) | SFT + Alignment á€á€…á€ºá€á€«á€á€Šá€ºá€¸ | Chosen/Rejected pairs | âœ… |
| **KTO** (Kahneman-Tversky Optimization) | Unpaired preference alignment | Completion + Label (true/false) | âœ… |
| **RLHF** (Reward Model Training) | Reward model train á€á€¼á€„á€ºá€¸ | Chosen/Rejected pairs | âš ï¸ Limited |
| **VLM SFT** | Vision-Language fine-tuning | Image + Conversation | âœ… Experimental |

---

## 1. Continued Pre-Training (CPT) - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

Model á€€á€­á€¯ **domain-specific knowledge** (á€¥á€•á€™á€¬: Medical, Legal, Finance, Myanmar Language) á€‘á€•á€ºá€‘á€Šá€·á€ºá€á€„á€ºá€•á€±á€¸á€–á€­á€¯á€· á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Instruction format á€™á€œá€­á€¯á€˜á€² **raw text** á€•á€² á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹

### Dataset Format

#### Format A: Plain Text (Completion)

```json
{"text": "á€¤á€á€Šá€ºá€™á€¾á€¬ á€•á€‘á€™ document á á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶ text á€–á€¼á€…á€ºá€•á€«á€á€Šá€ºá‹ Document á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ single text field á€™á€¾á€¬ á€‘á€Šá€·á€ºá€•á€«á‹"}
{"text": "á€’á€¯á€á€­á€š document á text á€–á€¼á€…á€ºá€•á€«á€á€Šá€ºá‹ á€’á€® format á€™á€¾á€¬ instruction/response á€á€½á€²á€…á€›á€¬ á€™á€œá€­á€¯á€•á€«á‹"}
{"text": "The quick brown fox jumps over the lazy dog. This is a sample document for continued pretraining."}
```

#### Format B: Pretraining Corpus (Large Text Blocks)

```json
{"text": "Chapter 1: Introduction to Machine Learning\n\nMachine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Unlike traditional programming where rules are explicitly coded, machine learning algorithms identify patterns in data and make decisions with minimal human intervention.\n\n## Types of Machine Learning\n\n### Supervised Learning\nSupervised learning involves training a model on labeled data..."}
```

### Axolotl Config (CPT)

```yaml
base_model: meta-llama/Llama-3.1-8B

# CPT specific settings
datasets:
  - path: ./data/pretrain_corpus.jsonl
    type: completion           # â† CPT á€¡á€á€½á€€á€º completion type á€á€¯á€¶á€¸
    field: text                # â† text field name

# CPT Training Settings
learning_rate: 2e-5            # SFT á€‘á€€á€º learning rate á€”á€­á€™á€·á€ºá€á€„á€·á€º
num_epochs: 1                  # CPT á€™á€¾á€¬ 1-2 epochs á€œá€±á€¬á€€á€ºá€•á€²
sequence_len: 4096
sample_packing: true           # Short texts á€á€½á€±á€€á€­á€¯ pack á€•á€¼á€®á€¸ efficiency á€á€­á€¯á€¸á€™á€¼á€„á€·á€º
```

### CPT Dataset Preparation Tips

```
âš ï¸ CPT á€¡á€á€½á€€á€º á€á€á€­á€‘á€¬á€¸á€›á€™á€šá€·á€º á€¡á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. Data Quality: Noisy/duplicate data á€–á€šá€ºá€›á€¾á€¬á€¸á€•á€«
2. Data Size: Model size á€›á€²á€· 1-10% tokens á€œá€±á€¬á€€á€º á€œá€­á€¯á€¡á€•á€º
   - 7B model â†’ 1B-10B tokens
   - 70B model â†’ 10B-100B tokens
3. Learning Rate: SFT (2e-4) á€‘á€€á€º á€”á€­á€™á€·á€ºá€•á€« (1e-5 ~ 5e-5)
4. Epochs: 1-2 epochs á€•á€² (overfit á€™á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º)
5. No special tokens: Chat template, instruction tags á€™á€œá€­á€¯
```

---

## 2. Supervised Fine-Tuning (SFT) - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

Model á€€á€­á€¯ **instruction following**, **chat**, **task-specific** abilities á€á€„á€ºá€•á€±á€¸á€–á€­á€¯á€· á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### Axolotl SFT Dataset Types

Axolotl á€™á€¾á€¬ SFT dataset format **á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸** support á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

---

### ğŸ“ Type 1: `alpaca` Format

**Single-turn instruction-response** format á€–á€¼á€…á€ºá€•á€¼á€®á€¸ á€¡á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€†á€¯á€¶á€¸ SFT format á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

#### Dataset Structure

```json
{
  "instruction": "Translate the following English text to Myanmar.",
  "input": "Hello, how are you?",
  "output": "á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«áŠ á€”á€±á€€á€±á€¬á€„á€ºá€¸á€œá€¬á€¸?"
}
```

```json
{
  "instruction": "Summarize the following article in 3 sentences.",
  "input": "Artificial intelligence (AI) has transformed numerous industries over the past decade. From healthcare to finance, AI systems are being deployed to automate tasks, analyze data, and make predictions. The technology continues to evolve rapidly, with new breakthroughs in natural language processing, computer vision, and robotics emerging regularly.",
  "output": "AI has significantly impacted multiple industries in recent years. It is being used for automation, data analysis, and predictions across sectors like healthcare and finance. The field continues to advance rapidly with breakthroughs in NLP, computer vision, and robotics."
}
```

#### Input Field á€™á€œá€­á€¯á€á€²á€· á€¥á€•á€™á€¬

```json
{
  "instruction": "What is the capital of Myanmar?",
  "input": "",
  "output": "Myanmar (Burma) á€›á€²á€· á€™á€¼á€­á€¯á€·á€á€±á€¬á€ºá€€ á€”á€±á€•á€¼á€Šá€ºá€á€±á€¬á€º (Naypyidaw) á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹"
}
```

#### Axolotl Config

```yaml
datasets:
  - path: ./data/my_alpaca_data.jsonl
    type: alpaca

# Custom field names á€á€¯á€¶á€¸á€á€»á€„á€ºá€›á€„á€º
datasets:
  - path: ./data/custom_data.jsonl
    type:
      system_prompt: "You are a helpful assistant."
      field_instruction: question       # instruction field name á€•á€¼á€±á€¬á€„á€ºá€¸
      field_input: context              # input field name á€•á€¼á€±á€¬á€„á€ºá€¸
      field_output: answer              # output field name á€•á€¼á€±á€¬á€„á€ºá€¸
      format: "[INST] {instruction} {input} [/INST]"
      no_input_format: "[INST] {instruction} [/INST]"
```

---

### ğŸ’¬ Type 2: `sharegpt` Format

**Multi-turn conversation** format á€–á€¼á€…á€ºá€•á€¼á€®á€¸ chat models train á€–á€­á€¯á€· á€¡á€á€¯á€¶á€¸á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ format á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

#### Dataset Structure

```json
{
  "conversations": [
    {"from": "system", "value": "You are a helpful Myanmar language assistant."},
    {"from": "human", "value": "á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«á‹ Python programming á€€á€­á€¯ á€˜á€šá€ºá€œá€­á€¯ á€…á€œá€±á€·á€œá€¬á€›á€™á€œá€²?"},
    {"from": "gpt", "value": "á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«! Python á€€á€­á€¯ á€…á€œá€±á€·á€œá€¬á€–á€­á€¯á€· á€¡á€†á€„á€·á€ºá€á€½á€±á€€á€­á€¯ á€•á€¼á€±á€¬á€•á€¼á€•á€«á€™á€šá€º:\n\n1. **Python Install á€œá€¯á€•á€ºá€•á€«** - python.org á€€á€”á€± download á€œá€¯á€•á€ºá€•á€«\n2. **Basic Syntax** - variables, data types, operators\n3. **Control Flow** - if/else, for/while loops\n4. **Functions** - def keyword á€”á€²á€· function á€›á€±á€¸á€•á€«\n5. **Practice** - LeetCode, HackerRank á€™á€¾á€¬ á€œá€±á€·á€€á€»á€„á€·á€ºá€•á€«"},
    {"from": "human", "value": "IDE á€˜á€¬á€á€¯á€¶á€¸á€›á€„á€º á€€á€±á€¬á€„á€ºá€¸á€™á€œá€²?"},
    {"from": "gpt", "value": "Python á€¡á€á€½á€€á€º IDE recommendation:\n\n1. **VS Code** - Lightweight, extensions á€™á€»á€¬á€¸, á€¡á€€á€¼á€¶á€•á€¼á€¯á€á€»á€„á€ºá€†á€¯á€¶á€¸\n2. **PyCharm** - Full-featured Python IDE\n3. **Jupyter Notebook** - Data Science/ML á€¡á€á€½á€€á€º\n\ná€¡á€…á€•á€­á€¯á€„á€ºá€¸ learner á€†á€­á€¯á€›á€„á€º **VS Code** á€€á€­á€¯ recommend á€•á€«á€á€šá€ºá‹"}
  ]
}
```

#### Multi-turn Conversation (á€›á€¾á€Šá€ºá€á€²á€· á€¥á€•á€™á€¬)

```json
{
  "conversations": [
    {"from": "system", "value": "You are an expert Python tutor."},
    {"from": "human", "value": "What is a list comprehension?"},
    {"from": "gpt", "value": "A list comprehension is a concise way to create lists in Python.\n\nBasic syntax: `[expression for item in iterable if condition]`\n\nExample:\n```python\nsquares = [x**2 for x in range(10)]\n# Result: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n```"},
    {"from": "human", "value": "Can you show a more complex example?"},
    {"from": "gpt", "value": "Sure! Here are more complex examples:\n\n```python\n# Nested list comprehension\nmatrix = [[1,2,3],[4,5,6],[7,8,9]]\nflat = [x for row in matrix for x in row]\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# With condition\neven_squares = [x**2 for x in range(20) if x % 2 == 0]\n# [0, 4, 16, 36, 64, 100, 144, 196, 256, 324]\n\n# Dictionary comprehension\nword_lengths = {word: len(word) for word in ['hello', 'world']}\n# {'hello': 5, 'world': 5}\n```"}
  ]
}
```

#### Axolotl Config

```yaml
datasets:
  - path: ./data/my_chat_data.jsonl
    type: sharegpt
    conversation: chatml       # â† Chat template format

# conversation options:
# chatml     â†’ <|im_start|>role\ncontent<|im_end|>
# llama3     â†’ Llama 3 format
# mistral    â†’ Mistral format
# gemma      â†’ Gemma format
# vicuna     â†’ Vicuna format
```

#### ShareGPT Field Name á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸

Default field names á€™á€Ÿá€¯á€á€ºá€›á€„á€º mapping á€œá€¯á€•á€ºá€•á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º:

```yaml
datasets:
  - path: ./data/custom_chat.jsonl
    type: sharegpt
    conversation: chatml
    field_messages: messages          # default: conversations
    message_field_role: role          # default: from
    message_field_content: content    # default: value
    roles:
      user:                           # â† "from" field values mapping
        - human
        - user
      assistant:
        - gpt
        - assistant
      system:
        - system
```

á€¥á€•á€™á€¬ - OpenAI format dataset á€€á€­á€¯ sharegpt type á€”á€²á€· á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸:

```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi! How can I help you today?"}
  ]
}
```

```yaml
datasets:
  - path: ./data/openai_format.jsonl
    type: sharegpt
    conversation: chatml
    field_messages: messages
    message_field_role: role
    message_field_content: content
```

---

### ğŸ·ï¸ Type 3: `chat_template` Format

Model á€›á€²á€· **native chat template** (tokenizer_config.json á€‘á€²á€€) á€€á€­á€¯ auto-detect á€œá€¯á€•á€ºá€•á€¼á€®á€¸ format á€á€»á€á€²á€· á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ **Axolotl á€™á€¾á€¬ recommended approach** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

#### Dataset Structure (OpenAI Messages Format)

```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is machine learning?"},
    {"role": "assistant", "content": "Machine learning is a branch of AI that enables systems to learn from data and improve without explicit programming."}
  ]
}
```

#### Axolotl Config

```yaml
chat_template: chatml              # or: llama3, mistral, gemma, tokenizer_default
datasets:
  - path: ./data/messages_data.jsonl
    type: chat_template
    field_messages: messages        # messages field name
    message_field_role: role
    message_field_content: content
    roles:
      user:
        - user
      assistant:
        - assistant
      system:
        - system
```

#### `chat_template` vs `sharegpt` á€˜á€šá€ºá€Ÿá€¬ á€á€¯á€¶á€¸á€á€„á€·á€ºá€œá€²?

| Feature | `sharegpt` | `chat_template` |
|---|---|---|
| Chat template | Manually specify (`conversation:`) | Auto from tokenizer |
| Flexibility | More manual control | More automatic |
| Model compatibility | Need correct conversation type | Auto-detect |
| Recommended for | Custom formats | Standard training |

---

### ğŸ“„ Type 4: `completion` Format

**Raw text completion** format - CPT (Continued Pre-Training) á€”á€²á€· raw text generation á€¡á€á€½á€€á€º:

```json
{"text": "Once upon a time, there was a small village nestled in the mountains of Myanmar. The villagers lived peacefully, growing rice in the terraced fields that cascaded down the hillsides."}
```

```yaml
datasets:
  - path: ./data/corpus.jsonl
    type: completion
    field: text
```

---

### ğŸ”§ Type 5: `input_output` Format

**Flexible instruction format** - input/output á€€á€­á€¯ explicitly á€á€½á€²á€•á€¼á€®á€¸ á€•á€±á€¸á€á€²á€· format:

```json
{
  "input": "Below is an instruction. Write a response.\n\n### Instruction:\nExplain quantum computing in simple terms.\n\n### Response:\n",
  "output": "Quantum computing uses quantum mechanics principles like superposition and entanglement to process information. Unlike classical computers that use bits (0 or 1), quantum computers use qubits that can be both 0 and 1 simultaneously, allowing them to solve certain problems much faster."
}
```

```yaml
datasets:
  - path: ./data/input_output.jsonl
    type: input_output
```

---

### ğŸ› ï¸ Type 6: Custom Prompt Template

á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º prompt template á€–á€”á€ºá€á€®á€¸á€œá€­á€¯á€·á€›á€•á€«á€á€šá€º:

```yaml
datasets:
  - path: ./data/custom.jsonl
    type:
      system_prompt: "You are an expert translator."
      field_instruction: source_text
      field_input: context
      field_output: translation
      format: "<|start_header_id|>user<|end_header_id|>\n\nTranslate this: {instruction}\nContext: {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
      no_input_format: "<|start_header_id|>user<|end_header_id|>\n\nTranslate this: {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
```

---

## 3. DPO (Direct Preference Optimization) - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

SFT á€•á€¼á€®á€¸á€á€¬á€¸ model á€€á€­á€¯ **human preference** á€”á€²á€· align á€–á€­á€¯á€· á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ "á€˜á€šá€º response á€€ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€œá€²" á€†á€­á€¯á€á€¬ á€á€„á€ºá€•á€±á€¸á€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### DPO Dataset Format

DPO á€¡á€á€½á€€á€º **chosen** (preferred) á€”á€¾á€„á€·á€º **rejected** (non-preferred) response pair á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€º:

#### Format A: ShareGPT Style DPO

```json
{
  "chosen": [
    {"from": "system", "value": "You are a helpful assistant."},
    {"from": "human", "value": "Explain gravity to a 5-year-old."},
    {"from": "gpt", "value": "Imagine the Earth is like a big magnet, but instead of pulling metal, it pulls everything! That's why when you throw a ball up, it comes back down. The Earth is giving it a big hug and pulling it back! ğŸŒ"}
  ],
  "rejected": [
    {"from": "system", "value": "You are a helpful assistant."},
    {"from": "human", "value": "Explain gravity to a 5-year-old."},
    {"from": "gpt", "value": "Gravity is a fundamental force described by Einstein's General Theory of Relativity, where mass curves spacetime, creating what we perceive as gravitational attraction proportional to the inverse square of distance."}
  ]
}
```

> **chosen** = á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€•á€¼á€®á€¸ 5 á€”á€¾á€…á€ºá€á€¬á€¸ á€”á€¬á€¸á€œá€Šá€ºá€”á€­á€¯á€„á€º â†’ âœ… á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸
> **rejected** = Technical á€˜á€¬á€á€¬á€…á€€á€¬á€¸áŠ á€€á€œá€±á€¸ á€”á€¬á€¸á€™á€œá€Šá€º â†’ âŒ á€™á€€á€±á€¬á€„á€ºá€¸

#### Format B: Instruction Style DPO

```json
{
  "prompt": "What is the best way to learn programming?",
  "chosen": "Start with a beginner-friendly language like Python. Follow structured courses, build small projects, practice daily, and join coding communities for support.",
  "rejected": "Just read documentation."
}
```

### Axolotl DPO Config

```yaml
# DPO training rl config
rl: dpo

chat_template: chatml
datasets:
  - path: ./data/dpo_data.jsonl
    type: chat_template.default
    field_messages: chosen
    message_field_role: from
    message_field_content: value
    roles:
      user:
        - human
      assistant:
        - gpt
      system:
        - system

# OR simpler sharegpt DPO format
datasets:
  - path: ./data/dpo_data.jsonl
    type: sharegpt.default
    split: train

# DPO specific parameters
rl: dpo
dpo_beta: 0.1                      # KL divergence penalty (0.1-0.5)
```

### DPO vs SFT Dataset Comparison

```
SFT Dataset:                         DPO Dataset:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ instruction: "..."  â”‚              â”‚ prompt: "..."                â”‚
â”‚ output: "good ans"  â”‚              â”‚ chosen: "good answer"    âœ…  â”‚
â”‚                     â”‚              â”‚ rejected: "bad answer"   âŒ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Single answer                        Answer pair (good vs bad)
```

---

## 4. ORPO (Odds Ratio Preference Optimization) - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

**SFT + DPO á€€á€­á€¯ single training step** á€™á€¾á€¬ á€á€…á€ºá€á€«á€á€Šá€ºá€¸ á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ SFT model á€á€®á€¸á€á€”á€·á€º train á€…á€›á€¬ á€™á€œá€­á€¯á€á€±á€¬á€·á€•á€«á‹

### ORPO Dataset Format

DPO á€”á€²á€· format **á€á€°á€á€°á€•á€²** á€–á€¼á€…á€ºá€•á€«á€á€šá€º (chosen/rejected pairs):

```json
{
  "chosen": [
    {"from": "system", "value": "You are a helpful coding assistant."},
    {"from": "human", "value": "Write a Python function to check if a number is prime."},
    {"from": "gpt", "value": "```python\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n```\nThis function checks divisibility up to âˆšn for efficiency."}
  ],
  "rejected": [
    {"from": "system", "value": "You are a helpful coding assistant."},
    {"from": "human", "value": "Write a Python function to check if a number is prime."},
    {"from": "gpt", "value": "```python\ndef is_prime(n):\n    for i in range(2, n):\n        if n % i == 0:\n            return False\n    return True\n```"}
  ]
}
```

### Axolotl ORPO Config

```yaml
rl: orpo

chat_template: chatml
datasets:
  - path: ./data/orpo_data.jsonl
    type: chat_template.default
    field_messages: chosen
    message_field_role: from
    message_field_content: value
    roles:
      user:
        - human
      assistant:
        - gpt
      system:
        - system

orpo_alpha: 0.1                     # ORPO loss weight
```

---

## 5. KTO (Kahneman-Tversky Optimization) - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

DPO á€™á€¾á€¬ chosen/rejected **pair** á€œá€­á€¯á€¡á€•á€ºá€•á€±á€™á€šá€·á€ºáŠ KTO á€™á€¾á€¬ **unpaired** data (individual completion + good/bad label) á€”á€²á€· alignment train á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

### KTO Dataset Format

```json
{
  "prompt": "What is the meaning of life?",
  "completion": "The meaning of life is a philosophical question that has been pondered throughout human history. Different perspectives include finding purpose through relationships, personal growth, contribution to society, or spiritual fulfillment.",
  "label": true
}
```

```json
{
  "prompt": "What is the meaning of life?",
  "completion": "42",
  "label": false
}
```

> `label: true` = á€€á€±á€¬á€„á€ºá€¸á€á€²á€· response âœ…
> `label: false` = á€™á€€á€±á€¬á€„á€ºá€¸á€á€²á€· response âŒ
> pair á€á€»á€­á€á€ºá€…á€›á€¬ **á€™á€œá€­á€¯á€•á€«**

### KTO vs DPO Dataset Comparison

```
DPO (Paired):                        KTO (Unpaired):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ prompt: "question"   â”‚              â”‚ prompt: "question"    â”‚
â”‚ chosen: "good" â”€â”€â”   â”‚              â”‚ completion: "answer"  â”‚
â”‚ rejected: "bad"â”€â”€â”˜   â”‚              â”‚ label: true / false   â”‚
â”‚    â†‘ Must be paired  â”‚              â”‚    â†‘ Independent      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Axolotl KTO Config

```yaml
rl: kto

datasets:
  - path: ./data/kto_data.jsonl
    type: ...  # standard format
    split: train

kto_desirable_weight: 1.0
kto_undesirable_weight: 1.0
```

---

## 6. VLM (Vision-Language Model) SFT - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

**Image + Text** understanding/generation train á€–á€­á€¯á€· á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### VLM Dataset Format

#### LLaVA Style Format

```json
{
  "id": "image_001",
  "image": "images/photo_001.jpg",
  "conversations": [
    {"from": "human", "value": "<image>\nWhat do you see in this image?"},
    {"from": "gpt", "value": "The image shows a beautiful sunset over the Irrawaddy River in Myanmar. The sky is painted in shades of orange and pink, with traditional boats silhouetted against the horizon."},
    {"from": "human", "value": "What time of day was this photo likely taken?"},
    {"from": "gpt", "value": "Based on the low angle of the sun and the warm colors, this photo was likely taken during golden hour, approximately 30-45 minutes before sunset."}
  ]
}
```

#### Multi-Image Format

```json
{
  "id": "multi_img_001",
  "images": ["images/before.jpg", "images/after.jpg"],
  "conversations": [
    {"from": "human", "value": "<image>\n<image>\nCompare these two images and describe the differences."},
    {"from": "gpt", "value": "The first image shows the building before renovation, while the second shows it after. Key differences include..."}
  ]
}
```

### Axolotl VLM Config

```yaml
base_model: llava-hf/llava-v1.6-mistral-7b-hf
model_type: LlavaForConditionalGeneration

adapter: lora
lora_r: 16
lora_alpha: 32

datasets:
  - path: ./data/vlm_data.jsonl
    type: llava

# Image processing settings
image_folder: ./data/images/
```

---

## 7. Function Calling / Tool Use - Dataset Format

### á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

Model á€€á€­á€¯ **function/tool calling** ability á€á€„á€ºá€•á€±á€¸á€–á€­á€¯á€· á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### Function Calling Dataset Format

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant with access to the following functions:\n\n{\"name\": \"get_weather\", \"description\": \"Get weather for a location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\"]}}"
    },
    {
      "role": "user",
      "content": "What's the weather in Yangon?"
    },
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [
        {
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"Yangon\", \"unit\": \"celsius\"}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "name": "get_weather",
      "content": "{\"temperature\": 32, \"condition\": \"Partly Cloudy\", \"humidity\": 78}"
    },
    {
      "role": "assistant",
      "content": "The weather in Yangon is currently 32Â°C and partly cloudy with 78% humidity."
    }
  ]
}
```

### Axolotl Config

```yaml
chat_template: chatml
datasets:
  - path: ./data/function_calling.jsonl
    type: chat_template
    field_messages: messages
    message_field_role: role
    message_field_content: content
    roles:
      user:
        - user
      assistant:
        - assistant
      system:
        - system
      tool:
        - tool
```

---

## Training Process Type á€¡á€œá€­á€¯á€€á€º Dataset Format á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯ Summary

### ğŸ”„ Same Data, Different Formats

á€á€°á€Šá€®á€á€²á€· task á€¡á€á€½á€€á€º training process type á€•á€¼á€±á€¬á€„á€ºá€¸á€›á€„á€º dataset format á€˜á€šá€ºá€œá€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸á€›á€œá€²:

#### á€¥á€•á€™á€¬ Task: "Myanmar á€˜á€¬á€á€¬ á€˜á€¬á€á€¬á€•á€¼á€”á€º"

**CPT (Domain Knowledge):**
```json
{"text": "á€™á€¼á€”á€ºá€™á€¬á€”á€­á€¯á€„á€ºá€„á€¶á€á€Šá€º á€¡á€›á€¾á€±á€·á€á€±á€¬á€„á€ºá€¡á€¬á€›á€¾á€á€½á€„á€º á€á€Šá€ºá€›á€¾á€­á€á€±á€¬ á€”á€­á€¯á€„á€ºá€„á€¶á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€á€Šá€ºá‹ á€™á€¼á€­á€¯á€·á€á€±á€¬á€ºá€™á€¾á€¬ á€”á€±á€•á€¼á€Šá€ºá€á€±á€¬á€ºá€–á€¼á€…á€ºá€•á€¼á€®á€¸..."}
```

**SFT (Instruction Following):**
```json
{
  "instruction": "Translate to Myanmar",
  "input": "The weather is nice today.",
  "output": "á€’á€®á€”á€±á€· á€›á€¬á€á€®á€¥á€á€¯ á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹"
}
```

**SFT Chat (Multi-turn):**
```json
{
  "conversations": [
    {"from": "human", "value": "Translate 'Good morning' to Myanmar"},
    {"from": "gpt", "value": "á€™á€„á€ºá€¹á€‚á€œá€¬ á€™á€”á€€á€ºá€á€„á€ºá€¸á€•á€« (Mingalar Manekhinbar)"},
    {"from": "human", "value": "How about 'Thank you'?"},
    {"from": "gpt", "value": "á€€á€»á€±á€¸á€‡á€°á€¸á€á€„á€ºá€•á€«á€á€šá€º (Kyay Zu Tin Par Tal)"}
  ]
}
```

**DPO (Preference Alignment):**
```json
{
  "prompt": "Translate 'I love Myanmar' to Burmese",
  "chosen": "á€€á€»á€½á€”á€ºá€á€±á€¬á€º á€™á€¼á€”á€ºá€™á€¬á€€á€­á€¯ á€á€»á€…á€ºá€•á€«á€á€šá€ºá‹ (Kyun Daw Myanmar Ko Chit Par Tal)",
  "rejected": "I love Myanmar = á€™á€¼á€”á€ºá€™á€¬ á€á€»á€…á€º"
}
```

**KTO (Unpaired Preference):**
```json
{"prompt": "Translate 'Hello' to Myanmar", "completion": "á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«", "label": true}
{"prompt": "Translate 'Hello' to Myanmar", "completion": "á€Ÿá€šá€ºá€œá€­á€¯", "label": false}
```

---

## Dataset Preparation Pipeline

### Step 1: Raw Data Collection

```
Data Sources:
â”œâ”€â”€ ğŸ“ Local files (CSV, JSON, TXT, PDF)
â”œâ”€â”€ ğŸ¤— Hugging Face Hub datasets
â”œâ”€â”€ ğŸŒ Web scraping / crawling
â”œâ”€â”€ ğŸ“Š API responses (OpenAI, Claude, etc.)
â””â”€â”€ ğŸ‘¥ Human annotation
```

### Step 2: Data Cleaning & Processing

```python
# data_preparation.py - Dataset preparation script example

import json

def prepare_alpaca_format(raw_data):
    """Raw data á€€á€­á€¯ Alpaca format á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    formatted = []
    for item in raw_data:
        formatted.append({
            "instruction": item["question"],
            "input": item.get("context", ""),
            "output": item["answer"]
        })
    return formatted

def prepare_sharegpt_format(raw_data):
    """Raw data á€€á€­á€¯ ShareGPT format á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    formatted = []
    for item in raw_data:
        conversations = []
        if "system" in item:
            conversations.append({"from": "system", "value": item["system"]})
        conversations.append({"from": "human", "value": item["question"]})
        conversations.append({"from": "gpt", "value": item["answer"]})
        formatted.append({"conversations": conversations})
    return formatted

def prepare_dpo_format(raw_data):
    """Raw data á€€á€­á€¯ DPO format á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    formatted = []
    for item in raw_data:
        formatted.append({
            "chosen": [
                {"from": "human", "value": item["question"]},
                {"from": "gpt", "value": item["good_answer"]}
            ],
            "rejected": [
                {"from": "human", "value": item["question"]},
                {"from": "gpt", "value": item["bad_answer"]}
            ]
        })
    return formatted

def save_jsonl(data, output_path):
    """JSONL format á€–á€¼á€„á€·á€º save á€á€¼á€„á€ºá€¸"""
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

# Usage
raw = json.load(open("raw_data.json"))
alpaca = prepare_alpaca_format(raw)
save_jsonl(alpaca, "train_alpaca.jsonl")
```

### Step 3: Data Validation

```python
def validate_alpaca(filepath):
    """Alpaca format dataset á€€á€­á€¯ validate á€á€¼á€„á€ºá€¸"""
    errors = []
    with open(filepath, 'r') as f:
        for i, line in enumerate(f):
            try:
                item = json.loads(line)
                if "instruction" not in item:
                    errors.append(f"Line {i+1}: Missing 'instruction'")
                if "output" not in item:
                    errors.append(f"Line {i+1}: Missing 'output'")
                if not item.get("output", "").strip():
                    errors.append(f"Line {i+1}: Empty 'output'")
            except json.JSONDecodeError:
                errors.append(f"Line {i+1}: Invalid JSON")
    return errors

def validate_sharegpt(filepath):
    """ShareGPT format dataset á€€á€­á€¯ validate á€á€¼á€„á€ºá€¸"""
    errors = []
    with open(filepath, 'r') as f:
        for i, line in enumerate(f):
            try:
                item = json.loads(line)
                convos = item.get("conversations", [])
                if not convos:
                    errors.append(f"Line {i+1}: Empty conversations")
                for j, msg in enumerate(convos):
                    if "from" not in msg or "value" not in msg:
                        errors.append(f"Line {i+1}, msg {j}: Missing from/value")
                    if msg.get("from") not in ["system", "human", "gpt"]:
                        errors.append(f"Line {i+1}, msg {j}: Invalid role '{msg.get('from')}'")
            except json.JSONDecodeError:
                errors.append(f"Line {i+1}: Invalid JSON")
    return errors

def validate_dpo(filepath):
    """DPO format dataset á€€á€­á€¯ validate á€á€¼á€„á€ºá€¸"""
    errors = []
    with open(filepath, 'r') as f:
        for i, line in enumerate(f):
            try:
                item = json.loads(line)
                if "chosen" not in item:
                    errors.append(f"Line {i+1}: Missing 'chosen'")
                if "rejected" not in item:
                    errors.append(f"Line {i+1}: Missing 'rejected'")
            except json.JSONDecodeError:
                errors.append(f"Line {i+1}: Invalid JSON")
    return errors
```

### Step 4: Data Quality Checks

```
âœ… Quality Checklist:
â”œâ”€â”€ â˜ Duplicate entries á€›á€¾á€­/á€™á€›á€¾á€­ á€…á€…á€ºá€†á€±á€¸
â”œâ”€â”€ â˜ Empty/null fields á€›á€¾á€­/á€™á€›á€¾á€­ á€…á€…á€ºá€†á€±á€¸
â”œâ”€â”€ â˜ Token length distribution á€€á€¼á€Šá€·á€º (too short/too long)
â”œâ”€â”€ â˜ Language consistency á€…á€…á€ºá€†á€±á€¸
â”œâ”€â”€ â˜ JSON format validity á€…á€…á€ºá€†á€±á€¸
â”œâ”€â”€ â˜ Special characters / encoding issues á€…á€…á€ºá€†á€±á€¸
â”œâ”€â”€ â˜ Train/eval split á€á€½á€²á€‘á€¬á€¸ (90/10 or 95/5)
â””â”€â”€ â˜ Sensitive/harmful content á€…á€…á€ºá€†á€±á€¸
```

---

## Axolotl Dataset Configuration - Advanced Features

### Multiple Datasets á€•á€±á€«á€„á€ºá€¸á€…á€Šá€ºá€¸á€á€¼á€„á€ºá€¸

Axolotl á€™á€¾á€¬ dataset **á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸á€€á€­á€¯** á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º á€•á€±á€«á€„á€ºá€¸á€á€¯á€¶á€¸á€œá€­á€¯á€· á€›á€•á€«á€á€šá€º:

```yaml
datasets:
  # Dataset 1: Alpaca format
  - path: ./data/general_instructions.jsonl
    type: alpaca
    split: train

  # Dataset 2: ShareGPT format (local file)
  - path: ./data/chat_conversations.jsonl
    type: sharegpt
    conversation: chatml

  # Dataset 3: Hugging Face Hub dataset
  - path: teknium/OpenHermes-2.5
    type: sharegpt
    split: train

  # Dataset 4: Completion (pretraining data)
  - path: ./data/domain_corpus.jsonl
    type: completion
    field: text

# Evaluation dataset
val_set_size: 0.05              # 5% for validation
```

### Dataset Sharding & Sampling

```yaml
datasets:
  - path: ./data/large_dataset.jsonl
    type: sharegpt
    shards: 10                    # Split into 10 shards for large datasets

  - path: ./data/small_high_quality.jsonl
    type: alpaca
    # Data á€€á€­á€¯ á€‘á€•á€ºá€á€«á€‘á€•á€ºá€á€« sample á€œá€¯á€•á€º
```

### Sample Packing

ì§§ì€ sequences á€á€½á€±á€€á€­á€¯ **pack** á€•á€¼á€®á€¸ GPU efficiency á€™á€¼á€„á€·á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸:

```yaml
sample_packing: true              # Short samples á€€á€­á€¯ pack
pad_to_sequence_len: true         # Pad to max sequence length
```

```
Without packing:                   With packing:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Sample1â”‚ PADDING  â”‚              â”‚ Sample1â”‚Sample2 â”‚Samp3 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sample2â”‚ PADDING  â”‚     â†’        â”‚Sample4 â”‚ Sample5â”‚Smp6  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
â”‚ Sample3â”‚ PADDING  â”‚              GPU utilization: ~95%
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
GPU utilization: ~40%
```

---

## File Formats Supported

### Axolotl Data File Types

| Format | Extension | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|---|
| **JSONL** | `.jsonl` | Line-delimited JSON (recommended) | `data.jsonl` |
| **JSON** | `.json` | JSON array | `data.json` |
| **Parquet** | `.parquet` | Columnar binary format | `data.parquet` |
| **CSV** | `.csv` | Comma-separated values | `data.csv` |
| **HuggingFace Dataset** | - | Hub dataset path | `org/dataset_name` |
| **Arrow** | `.arrow` | Apache Arrow format | `data.arrow` |

### JSONL vs JSON

```
JSONL (Recommended âœ…):              JSON:
{"instruction":"...", "output":".."}  [
{"instruction":"...", "output":".."}    {"instruction":"...", "output":".."},
{"instruction":"...", "output":".."}    {"instruction":"...", "output":".."},
                                        {"instruction":"...", "output":".."}
                                      ]
â†‘ Line by line, streamable            â†‘ Full file load needed
â†‘ Memory efficient                    â†‘ Memory heavy for large files
```

---

## Training Process Pipeline - Dataset Format Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Complete Training Pipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Stage 1: CPT (Continued Pre-Training)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Dataset: completion format (raw text)            â”‚                   â”‚
â”‚  â”‚ {"text": "domain knowledge corpus..."}           â”‚                   â”‚
â”‚  â”‚ Goal: Domain knowledge á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸                  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                     â†“                                                    â”‚
â”‚  Stage 2: SFT (Supervised Fine-Tuning)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Dataset: alpaca / sharegpt / chat_template       â”‚                   â”‚
â”‚  â”‚ {"instruction":"...", "output":"..."}             â”‚                   â”‚
â”‚  â”‚ Goal: Instruction following ability               â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                     â†“                                                    â”‚
â”‚  Stage 3: Preference Alignment (DPO/ORPO/KTO)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Dataset: chosen/rejected pairs (DPO/ORPO)        â”‚                   â”‚
â”‚  â”‚          completion + label (KTO)                 â”‚                   â”‚
â”‚  â”‚ Goal: Human preference alignment                  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                          â”‚
â”‚  ğŸ’¡ Stage 3 á€€á€­á€¯ DPO/ORPO/KTO á€‘á€²á€€ á€á€…á€ºá€á€¯á€•á€² á€›á€½á€±á€¸á€•á€«                        â”‚
â”‚  ğŸ’¡ ORPO á€†á€­á€¯á€›á€„á€º Stage 2+3 á€€á€­á€¯ combine á€œá€¯á€•á€ºá€•á€¼á€®á€¸                          â”‚
â”‚     SFT model á€á€®á€¸á€á€”á€·á€º train á€…á€›á€¬ á€™á€œá€­á€¯                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Reference: Training Type â†’ Dataset Format â†’ Axolotl Config

| Training Type | Dataset Format | Axolotl `type` | Axolotl `rl` | Key Fields |
|---|---|---|---|---|
| **CPT** | Raw text | `completion` | - | `text` |
| **SFT (single-turn)** | Instruction pairs | `alpaca` | - | `instruction`, `input`, `output` |
| **SFT (multi-turn)** | Conversations | `sharegpt` | - | `conversations[{from, value}]` |
| **SFT (auto template)** | Messages | `chat_template` | - | `messages[{role, content}]` |
| **SFT (flexible)** | Input/Output | `input_output` | - | `input`, `output` |
| **DPO** | Preference pairs | `sharegpt.default` / `chat_template.default` | `dpo` | `chosen[]`, `rejected[]` |
| **ORPO** | Preference pairs | `sharegpt.default` / `chat_template.default` | `orpo` | `chosen[]`, `rejected[]` |
| **KTO** | Labeled completions | custom | `kto` | `prompt`, `completion`, `label` |
| **VLM SFT** | Image + Conversation | `llava` | - | `image`, `conversations[]` |
| **Function Calling** | Tool use conversations | `chat_template` | - | `messages[]` with `tool_calls` |

---

## Common Mistakes & Troubleshooting

### âŒ Dataset Format Errors

| Error | á€–á€¼á€…á€ºá€á€á€ºá€á€²á€· á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€›á€„á€ºá€¸ | á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€Šá€ºá€¸ |
|---|---|---|
| `KeyError: 'instruction'` | Field name á€™á€¾á€¬á€¸ | `field_instruction` parameter á€…á€…á€ºá€•á€« |
| `KeyError: 'conversations'` | ShareGPT field name á€™á€¾á€¬á€¸ | `field_messages` parameter á€…á€…á€ºá€•á€« |
| `Invalid role` | Role name á€™á€¾á€¬á€¸ (á€¥á€•á€™á€¬ `user` vs `human`) | `roles` mapping á€…á€…á€ºá€•á€« |
| `Empty response` | Output/value field á€—á€œá€¬ | Data cleaning á€œá€¯á€•á€ºá€•á€« |
| `Token length exceeded` | Sequence á€›á€¾á€Šá€ºá€œá€½á€”á€ºá€¸ | `sequence_len` á€á€­á€¯á€¸á€•á€« á€á€­á€¯á€· data á€–á€¼á€á€ºá€•á€« |
| `JSON decode error` | JSONL format á€™á€¾á€¬á€¸ | JSON validity á€…á€…á€ºá€•á€« |
| `DPO missing chosen/rejected` | DPO field á€™á€›á€¾á€­ | chosen/rejected fields á€‘á€Šá€·á€ºá€•á€« |
| `Tokenizer chat template not found` | Chat template config á€™á€›á€¾á€­ | `chat_template` specify á€œá€¯á€•á€ºá€•á€« |

### âœ… Best Practices

```
Dataset Quality Tips:
â”œâ”€â”€ 1. Data Size: 1K-100K examples (SFT), 10K+ (DPO)
â”œâ”€â”€ 2. Quality > Quantity: á€€á€±á€¬á€„á€ºá€¸á€á€²á€· 1K á€Ÿá€¬ á€Šá€¶á€·á€á€²á€· 100K á€‘á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸
â”œâ”€â”€ 3. Diversity: Task/topic variety á€›á€¾á€­á€•á€«á€…á€±
â”œâ”€â”€ 4. Consistency: Format/style consistent á€–á€¼á€…á€ºá€•á€«á€…á€±
â”œâ”€â”€ 5. Deduplication: Duplicate data á€–á€šá€ºá€›á€¾á€¬á€¸á€•á€«
â”œâ”€â”€ 6. Validation split: 5-10% eval data á€á€½á€²á€‘á€¬á€¸á€•á€«
â”œâ”€â”€ 7. Token length: Model á€›á€²á€· max_seq_len á€‘á€€á€º á€™á€€á€»á€±á€¬á€ºá€•á€«á€…á€±
â””â”€â”€ 8. System prompt: Consistent system prompt á€á€¯á€¶á€¸á€•á€«
```
