# 02. Full Fine-Tuning (FFT) with Axolotl - RTX 3050 12GB

## RTX 3050 12GB VRAM á€”á€²á€· Full Fine-Tuning

### âš ï¸ VRAM Limitation á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸

Full Fine-Tuning (FFT) á€™á€¾á€¬ model parameter **á€¡á€¬á€¸á€œá€¯á€¶á€¸** á€€á€­á€¯ train á€œá€¯á€•á€ºá€›á€á€²á€·á€¡á€á€½á€€á€º VRAM usage á€€ LoRA/QLoRA á€‘á€€á€º **á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ á€•á€­á€¯á€€á€¯á€”á€º** á€•á€«á€á€šá€ºá‹

#### VRAM Usage Breakdown (Full Fine-Tuning)

```
Full Fine-Tuning VRAM = Model Weights + Gradients + Optimizer States + Activations

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Component         â”‚ Memory (per 1B params)     â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
fp16/bf16 Training:     â”‚ Model Weights     â”‚ ~2 GB   (2 bytes/param)   â”‚
                        â”‚ Gradients         â”‚ ~2 GB   (2 bytes/param)   â”‚
                        â”‚ Optimizer (AdamW) â”‚ ~4 GB   (8 bytes/param)   â”‚ â† 2 states
                        â”‚ Activations       â”‚ ~1-3 GB (varies)          â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ Total per 1B      â”‚ ~9-11 GB                  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### RTX 3050 12GB á€”á€²á€· Train á€”á€­á€¯á€„á€ºá€á€²á€· Model Size

| Model Size | FFT Memory (bf16) | RTX 3050 12GB | á€™á€¾á€á€ºá€á€»á€€á€º |
|---|---|---|---|
| **0.1B - 0.2B** | ~1.5 - 2.5 GB | âœ… á€¡á€†á€„á€ºá€•á€¼á€± | SmolLM2-135M |
| **0.5B** | ~5 - 6 GB | âœ… á€¡á€†á€„á€ºá€•á€¼á€± | Qwen2.5-0.5B |
| **1B - 1.1B** | ~9 - 11 GB | âš ï¸ Tight (gradient checkpointing á€œá€­á€¯) | TinyLlama-1.1B, Llama-3.2-1B |
| **1.5B** | ~14 - 16 GB | âŒ VRAM á€™á€œá€±á€¬á€€á€º | Qwen2.5-1.5B |
| **3B+** | ~27+ GB | âŒ á€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º | - |

> ğŸ’¡ **12GB VRAM** á€”á€²á€· full fine-tuning á€¡á€á€½á€€á€º **0.5B - 1B** model á€€á€­á€¯ recommend á€•á€«á€á€šá€ºá‹

---

## RTX 3050 GPU Specifications

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NVIDIA GeForce RTX 3050                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Architecture:    Ampere (SM 8.6)         â”‚
â”‚ VRAM:            12 GB GDDR6             â”‚
â”‚ CUDA Cores:      2560                    â”‚
â”‚ Memory Bus:      192-bit                 â”‚
â”‚ bf16 Support:    âœ… Yes                  â”‚
â”‚ Flash Attention: âœ… Yes (Ampere+)        â”‚
â”‚ Compute Cap:     8.6                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step-by-Step Guide

### Step 1: Docker Container á€–á€½á€„á€·á€ºá€á€¼á€„á€ºá€¸

01_settingup.md á€™á€¾á€¬ Docker + NVIDIA Container Toolkit install á€•á€¼á€®á€¸á€•á€¼á€®á€†á€­á€¯á€›á€„á€º:

```bash
# Axolotl container á€€á€­á€¯ run á€•á€«
# -v flag á€”á€²á€· local data folder á€€á€­á€¯ mount á€•á€« (dataset/config files á€¡á€á€½á€€á€º)
docker run --gpus '"all"' \
  --rm -it \
  --shm-size=4g \
  -v $(pwd)/workspace:/workspace/data \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8888:8888 \
  axolotlai/axolotl:main-latest
```

### Step 2: Container á€‘á€²á€™á€¾á€¬ GPU á€…á€…á€ºá€†á€±á€¸á€á€¼á€„á€ºá€¸

```bash
# GPU á€›á€¾á€­/á€™á€›á€¾á€­ á€…á€…á€ºá€†á€±á€¸
nvidia-smi

# Expected output:
# NVIDIA GeForce RTX 3050 | 12GB
```

```bash
# CUDA + PyTorch compatibility á€…á€…á€ºá€†á€±á€¸
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU: {torch.cuda.get_device_name(0)}')
print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
print(f'Compute Capability: {torch.cuda.get_device_capability()}')
print(f'bf16 support: {torch.cuda.is_bf16_supported()}')
"
```

### Step 3: Hugging Face Login

```bash
# HF Token á€”á€²á€· login (gated models á€šá€°á€–á€­á€¯á€·)
hf auth login --token $HF_TOKEN

# OR
# pip install huggingface_hub
# huggingface-cli login
```

---

## Full Fine-Tuning Test Runs

Config files á€”á€²á€· dataset files á€á€½á€±á€€á€­á€¯ model folder á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€™á€¾á€¬ á€á€½á€²á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ **á€¡á€…á€‰á€ºá€œá€­á€¯á€€á€º run á€•á€«:**

### Docker Container á€…á€á€„á€ºá€á€¼á€„á€ºá€¸ (Test Run á€¡á€¬á€¸á€œá€¯á€¶á€¸á€¡á€á€½á€€á€º)

```bash
docker run --gpus '"all"' --rm -it \
  --shm-size=4g \
  -v /home/mr_cobot/Desktop/dev_projects/ai/generative.ai/learning/fine_tuning:/workspace/data \
  -v /home/mr_cobot/.cache/huggingface:/root/.cache/huggingface \
  axolotlai/axolotl:main-latest
```

---

### Test Run á â€” SmolLM2-135M (Smoke Test)

> ğŸ§ª VRAM ~2GB | Pipeline á€…á€…á€ºá€–á€­á€¯á€· á€¡á€›á€„á€ºá€†á€¯á€¶á€¸ run á€•á€«

ğŸ“ **Folder:** [SmolLM2-135M/](SmolLM2-135M/) â€” Config, dataset, README á€•á€«á€•á€¼á€®á€¸á€á€¬á€¸

```bash
accelerate launch -m axolotl.cli.train /workspace/data/SmolLM2-135M/config.yml
```

---

### Test Run á‚ â€” Qwen2.5-0.5B (Recommended ğŸ¯)

> ğŸ¯ VRAM ~5-6GB | 12GB VRAM FFT sweet spot

ğŸ“ **Folder:** [Qwen2.5-0.5B/](Qwen2.5-0.5B/) â€” Config, dataset, README á€•á€«á€•á€¼á€®á€¸á€á€¬á€¸

```bash
accelerate launch -m axolotl.cli.train /workspace/data/Qwen2.5-0.5B/config.yml
```

---

### Test Run áƒ â€” Llama-3.2-1B (Maximum âš ï¸)

> âš ï¸ VRAM ~9-11GB | 12GB limit á€”á€¬á€¸á€€á€•á€ºáŠ OOM á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º
> ğŸ”‘ Gated model â€” HuggingFace login + access approval á€œá€­á€¯á€¡á€•á€º

ğŸ“ **Folder:** [Llama-3.2-1B/](Llama-3.2-1B/) â€” Config, dataset, README á€•á€«á€•á€¼á€®á€¸á€á€¬á€¸

```bash
# HuggingFace login (gated model á€¡á€á€½á€€á€º required)
huggingface-cli login

# Training
accelerate launch -m axolotl.cli.train /workspace/data/Llama-3.2-1B/config.yml
```

---

### Folder Structure

```
fine_tuning/
â”œâ”€â”€ SmolLM2-135M/              â† Test Run á
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.yml
â”‚   â””â”€â”€ train.jsonl
â”œâ”€â”€ Qwen2.5-0.5B/              â† Test Run á‚ (Recommended ğŸ¯)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.yml
â”‚   â””â”€â”€ train.jsonl
â”œâ”€â”€ Llama-3.2-1B/              â† Test Run áƒ (Maximum âš ï¸)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.yml
â”‚   â””â”€â”€ train.jsonl
â””â”€â”€ 02_fft.md                  â† á€’á€®á€–á€­á€¯á€„á€º (overview)
```

### VRAM Monitoring (Training run á€”á€±á€á€»á€­á€”á€ºá€™á€¾á€¬)

```bash
# á€”á€±á€¬á€€á€ºá€‘á€•á€º terminal á€€á€”á€± container á€‘á€²á€á€„á€ºá€•á€«
docker ps
docker exec -it <container_id> bash
watch -n 1 nvidia-smi
```

---

## Known Errors & Fixes

### âŒ Error: Tokenizer does not have a padding token

```
ValueError: Asking to pad but the tokenizer does not have a padding token.
Please select a token to use as `pad_token`
```

**á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€›á€„á€ºá€¸:** Model á€›á€²á€· tokenizer á€™á€¾á€¬ `pad_token` define á€™á€œá€¯á€•á€ºá€‘á€¬á€¸á€œá€­á€¯á€· evaluation step á€™á€¾á€¬ batch padding á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€« á€•á€»á€€á€ºá€•á€«á€á€šá€ºá‹ SmolLM2, GPT-2 á€…á€á€²á€· models á€á€½á€±á€™á€¾á€¬ á€’á€® error á€–á€¼á€…á€ºá€á€á€ºá€•á€«á€á€šá€ºá‹

**á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€Šá€ºá€¸:** Config YAML á€™á€¾á€¬ `special_tokens` section á€‘á€Šá€·á€ºá€•á€«:

```yaml
# SmolLM2 / GPT-2 style models:
special_tokens:
  pad_token: "<|endoftext|>"

# Llama 3.x models:
special_tokens:
  pad_token: "<|finetune_right_pad_id|>"

# General fallback (eos_token á€€á€­á€¯ pad_token á€¡á€–á€¼á€…á€ºá€á€¯á€¶á€¸):
special_tokens:
  pad_token: "</s>"
```

> ğŸ’¡ **Model á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€›á€²á€· special tokens á€€á€­á€¯ á€…á€…á€ºá€–á€­á€¯á€·:**
> ```python
> from transformers import AutoTokenizer
> tok = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-135M")
> print(f"eos: {tok.eos_token}, pad: {tok.pad_token}, bos: {tok.bos_token}")
> ```

---

## OOM (Out of Memory) á€–á€¼á€…á€ºá€›á€„á€º á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€Šá€ºá€¸

### OOM Prevention Checklist

```
VRAM á€™á€œá€±á€¬á€€á€ºá€›á€„á€º á€’á€® settings á€á€½á€±á€€á€­á€¯ á€¡á€…á€‰á€ºá€œá€­á€¯á€€á€º á€•á€¼á€„á€ºá€•á€«:

Step 1: gradient_checkpointing: true        â† ~40% VRAM save
Step 2: micro_batch_size: 1                 â† Batch size minimum
Step 3: sequence_len á€€á€­á€¯ á€œá€»á€¾á€±á€¬á€· (1024â†’512â†’256)
Step 4: flash_attention: true               â† Attention memory save
Step 5: optimizer: adafactor                 â† AdamW á€‘á€€á€º memory á€”á€Šá€ºá€¸
Step 6: sample_packing: false               â† Pack á€–á€¼á€¯á€á€ºá€€á€¼á€Šá€·á€º
Step 7: eval_batch_size: 1
Step 8: Model size á€€á€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸ (1B â†’ 0.5B â†’ 135M)
```

### Memory-Efficient Optimizer á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸

AdamW optimizer á€Ÿá€¬ parameter 1 á€á€¯á€¡á€á€½á€€á€º **8 bytes** (2 states Ã— 4 bytes) á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ Adafactor á€€ **á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸** á€•á€­á€¯á€á€€á€ºá€á€¬á€•á€«á€á€šá€º:

```yaml
# Option A: Standard AdamW (default)
optimizer: adamw_torch
# Memory: 8 bytes/param â†’ 1B model = ~8 GB for optimizer alone

# Option B: Fused AdamW (slightly better)
optimizer: adamw_torch_fused

# Option C: Adafactor (less memory, no momentum states)
optimizer: adafactor
# Memory: ~4 bytes/param â†’ 1B model = ~4 GB for optimizer

# Option D: 8-bit AdamW (significant savings)
optimizer: adamw_bnb_8bit
# Memory: ~4 bytes/param â†’ Uses BitsAndBytes 8-bit states
```

---

## Training á€•á€¼á€®á€¸á€›á€„á€º Inference Test

### 6.1: Trained Model Test

```bash
# Training á€•á€¼á€®á€¸á€›á€„á€º inference test
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = '/workspace/data/test_fft/output_qwen25_05b'  # output path

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map='auto'
)

# Test inference
prompt = '### Instruction:\nWhat is the capital of Myanmar?\n\n### Response:\n'
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
"
```

### 6.2: Axolotl Inference Command

```bash
# Axolotl á€›á€²á€· built-in inference
python -m axolotl.cli.inference /workspace/data/test_fft/config_qwen25_05b.yml \
  --lora_model_dir="/workspace/data/test_fft/output_qwen25_05b"
```

---

## VRAM Usage Summary â€” RTX 3050 12GB

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RTX 3050 12GB â€” Full Fine-Tuning VRAM Map          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  0 GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 12 GB    â”‚
â”‚        â”‚                                             â”‚          â”‚
â”‚        â”‚  SmolLM2-135M FFT                           â”‚          â”‚
â”‚        â”‚  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ ~2 GB    â”‚          â”‚
â”‚        â”‚                                             â”‚          â”‚
â”‚        â”‚  Qwen2.5-0.5B FFT                           â”‚          â”‚
â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ ~5-6 GB   â”‚ â† Sweet  â”‚
â”‚        â”‚                                             â”‚    Spot  â”‚
â”‚        â”‚  Llama-3.2-1B FFT                           â”‚          â”‚
â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ ~10-11 GB â”‚ â† Tight  â”‚
â”‚        â”‚                                             â”‚          â”‚
â”‚        â”‚  Qwen2.5-1.5B FFT                           â”‚          â”‚
â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~15 GBâ”‚ â† OOM âŒ â”‚
â”‚        â”‚                                             â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Full Fine-Tuning vs LoRA/QLoRA â€” 12GB GPU Comparison

| | Full Fine-Tuning | LoRA | QLoRA |
|---|---|---|---|
| **Max Model Size (12GB)** | ~1B | ~7B | ~7-8B |
| **Training Quality** | Best | Good | Good (slight loss) |
| **Trainable Params** | 100% | 0.1-10% | 0.1-10% |
| **Training Speed** | Slow | Fast | Medium |
| **VRAM Usage** | High | Medium | Low |
| **Use Case** | Small model mastery | Large model adaptation | Large model, low VRAM |

> ğŸ’¡ **12GB VRAM Recommendation:**
> - **á€…á€™á€ºá€¸á€á€•á€ºá€–á€­á€¯á€· / learning:** Full Fine-Tuning + small model (á€’á€® guide)
> - **Production quality:** QLoRA + 7B-8B model (PEFT_Types.md á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«)

---

## Complete Workflow Summary

```bash
# ============================================================
# RTX 3050 12GB â€” Full Fine-Tuning Quick Start
# ============================================================

# 1. Container start
docker run --gpus '"all"' --rm -it \
  --shm-size=4g \
  -v $(pwd)/workspace:/workspace/data \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  axolotlai/axolotl:main-latest

# 2. GPU check
nvidia-smi

# 3. HuggingFace login (gated models á€¡á€á€½á€€á€º)
huggingface-cli login

# 4. Dataset + Config á€•á€¼á€„á€ºá€†á€„á€º (Step 3.1, 3.2 á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«)
mkdir -p /workspace/data/test_fft
# ... create train.jsonl and config yml ...

# 5. Data preprocess (optional, validates config)
python -m axolotl.cli.preprocess /workspace/data/test_fft/config_qwen25_05b.yml

# 6. Train!
accelerate launch -m axolotl.cli.train /workspace/data/test_fft/config_qwen25_05b.yml

# 7. Monitor VRAM (another terminal)
watch -n 1 nvidia-smi

# 8. Inference test
python -m axolotl.cli.inference /workspace/data/test_fft/config_qwen25_05b.yml

# 9. Output á€‘á€¯á€á€ºá€šá€° (container á€•á€¼á€„á€ºá€•)
# Trained model: /workspace/data/test_fft/output_qwen25_05b/
```

---

## Next Steps

- **LoRA/QLoRA** á€”á€²á€· 7B-8B model train á€á€»á€„á€ºá€›á€„á€º â†’ [PEFT_Types.md](PEFT_Types.md) á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«
- **Dataset format** á€¡á€á€±á€¸á€…á€­á€á€º â†’ [Datasets.md](Datasets.md) á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«
- **Model á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸** â†’ [Models.md](Models.md) á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«
