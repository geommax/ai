# Qwen2.5-0.5B â€” Full Fine-Tuning (Recommended)

## Overview

| Item | Detail |
|---|---|
| **Model** | [Qwen/Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) |
| **Parameters** | 0.5B (494M) |
| **Training Type** | Full Fine-Tuning (FFT) |
| **Expected VRAM** | ~5-6 GB |
| **GPU** | RTX 3050 12GB |
| **á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º** | ğŸ¯ 12GB VRAM FFT Sweet Spot |

## Folder Structure

```
Qwen2.5-0.5B/
â”œâ”€â”€ README.md          â† á€’á€®á€–á€­á€¯á€„á€º
â”œâ”€â”€ config.yml         â† Axolotl training config
â”œâ”€â”€ train.jsonl        â† Test dataset (10 examples)
â”œâ”€â”€ prepared/          â† (auto) Preprocessed data cache
â””â”€â”€ output/            â† (auto) Trained model output
```

## Quick Start

### 1. Docker Container á€…á€á€„á€ºá€á€¼á€„á€ºá€¸

```bash
docker run --gpus '"all"' --rm -it \
  --shm-size=4g \
  -v /home/mr_cobot/Desktop/dev_projects/ai/generative.ai/learning/fine_tuning:/workspace/data \
  -v /home/mr_cobot/.cache/huggingface:/root/.cache/huggingface \
  axolotlai/axolotl:main-latest
```

### 2. Preprocess (Optional)

```bash
accelerate launch -m axolotl.cli.preprocess /workspace/data/Qwen2.5-0.5B/config.yml
```

### 3. Training Run

```bash
accelerate launch -m axolotl.cli.train /workspace/data/Qwen2.5-0.5B/config.yml
```

### 4. VRAM Monitoring

```bash
# á€”á€±á€¬á€€á€ºá€‘á€•á€º terminal á€™á€¾á€¬
docker ps
docker exec -it <container_id> bash
watch -n 1 nvidia-smi
```

### 5. Inference Test

```bash
# Interactive inference
accelerate launch -m axolotl.cli.inference /workspace/data/Qwen2.5-0.5B/config.yml

# OR Python script á€”á€²á€·
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = '/workspace/data/Qwen2.5-0.5B/output'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path, torch_dtype=torch.bfloat16, device_map='auto'
)

prompt = '### Instruction:\nWhat is the capital of Myanmar?\n\n### Response:\n'
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
"
```

## Config Key Settings

```yaml
base_model: Qwen/Qwen2.5-0.5B
special_tokens:
  pad_token: "<|endoftext|>"
sequence_len: 1024
micro_batch_size: 1
gradient_accumulation_steps: 8       # effective batch = 8
gradient_checkpointing: true
flash_attention: true
sample_packing: true                 # GPU efficiency á€á€­á€¯á€¸á€™á€¼á€„á€·á€º
bf16: auto
```

## VRAM Estimate

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen2.5-0.5B FFT â€” VRAM Breakdown              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model Weights (bf16):       ~1.0 GB             â”‚
â”‚ Gradients:                  ~1.0 GB             â”‚
â”‚ Optimizer States (AdamW):   ~2.0 GB             â”‚
â”‚ Activations + Overhead:     ~1.0-2.0 GB         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total:                      ~5-6 GB / 12 GB     â”‚
â”‚ Headroom:                   ~6-7 GB âœ…          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Recommend á€œá€¯á€•á€ºá€á€¬á€œá€²?

- **VRAM 50% á€•á€² á€á€¯á€¶á€¸** â†’ OOM risk á€™á€›á€¾á€­
- **Qwen2.5 architecture** â†’ Multilingual á€€á€±á€¬á€„á€ºá€¸áŠ performance á€€á€±á€¬á€„á€ºá€¸
- **sequence_len: 1024** á€‘á€­ á€á€¯á€¶á€¸á€”á€­á€¯á€„á€º â†’ longer context training
- **sample_packing** á€–á€½á€„á€·á€ºá€œá€­á€¯á€· á€› â†’ training speed up

## OOM á€–á€¼á€…á€ºá€›á€„á€º (á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€¼á€± á€”á€Šá€ºá€¸á€•á€«á€á€šá€º)

```yaml
# sequence_len á€œá€»á€¾á€±á€¬á€·
sequence_len: 512

# sample_packing á€•á€­á€á€º
sample_packing: false
```

## Notes

- âœ… 12GB GPU á€¡á€á€½á€€á€º FFT **sweet spot** á€–á€¼á€…á€ºá€•á€«á€á€šá€º
- Production dataset (1K-10K examples) á€”á€²á€· train á€›á€„á€º meaningful results á€›á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º
- SmolLM2-135M smoke test á€•á€¼á€®á€¸á€™á€¾ á€’á€® model á€€á€­á€¯ run á€•á€«
