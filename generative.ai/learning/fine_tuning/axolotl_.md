# Axolotl Framework â€” á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€•á€¯á€¶ á€”á€¾á€„á€·á€º Essential Commands

## Axolotl á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²?

Axolotl á€Ÿá€¬ **LLM fine-tuning** á€¡á€á€½á€€á€º á€›á€±á€¸á€‘á€¬á€¸á€á€²á€· open-source framework á€–á€¼á€…á€ºá€•á€¼á€®á€¸áŠ YAML config file á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€”á€²á€· training pipeline á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€»á€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```
GitHub: https://github.com/axolotl-ai-cloud/axolotl
```

---

## Axolotl Framework Architecture â€” á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€•á€¯á€¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AXOLOTL FRAMEWORK OVERVIEW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                           â”‚
â”‚   â”‚  YAML Config â”‚  â† á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€”á€²á€· á€¡á€¬á€¸á€œá€¯á€¶á€¸ á€‘á€­á€”á€ºá€¸á€á€»á€¯á€•á€º                        â”‚
â”‚   â”‚  (.yml file) â”‚                                                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚
â”‚          â”‚                                                                   â”‚
â”‚          â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚                   CONFIG PARSER                          â”‚              â”‚
â”‚   â”‚  Model config + Dataset config + Training config parse   â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚               â”‚                  â”‚                                â”‚
â”‚          â–¼               â–¼                  â–¼                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚   â”‚   Model    â”‚  â”‚  Dataset   â”‚    â”‚   Training   â”‚                        â”‚
â”‚   â”‚  Loading   â”‚  â”‚  Loading   â”‚    â”‚   Config     â”‚                        â”‚
â”‚   â”‚            â”‚  â”‚            â”‚    â”‚              â”‚                        â”‚
â”‚   â”‚ â€¢ HF Hub   â”‚  â”‚ â€¢ Local    â”‚    â”‚ â€¢ Optimizer  â”‚                        â”‚
â”‚   â”‚ â€¢ Local    â”‚  â”‚ â€¢ HF Hub   â”‚    â”‚ â€¢ Scheduler  â”‚                        â”‚
â”‚   â”‚ â€¢ 4/8-bit  â”‚  â”‚ â€¢ Multiple â”‚    â”‚ â€¢ Precision  â”‚                        â”‚
â”‚   â”‚ â€¢ Adapter  â”‚  â”‚   datasets â”‚    â”‚ â€¢ Batch size â”‚                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚               â”‚                  â”‚                                â”‚
â”‚         â–¼               â–¼                  â”‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                                â”‚
â”‚   â”‚ Tokenizer  â”‚  â”‚  Preprocess  â”‚         â”‚                                â”‚
â”‚   â”‚  Loading   â”‚  â”‚  & Format    â”‚         â”‚                                â”‚
â”‚   â”‚            â”‚  â”‚              â”‚         â”‚                                â”‚
â”‚   â”‚ â€¢ Chat     â”‚  â”‚ â€¢ Tokenize   â”‚         â”‚                                â”‚
â”‚   â”‚   template â”‚  â”‚ â€¢ Pack/Pad   â”‚         â”‚                                â”‚
â”‚   â”‚ â€¢ Special  â”‚  â”‚ â€¢ Train/Val  â”‚         â”‚                                â”‚
â”‚   â”‚   tokens   â”‚  â”‚   split      â”‚         â”‚                                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                                â”‚
â”‚         â”‚               â”‚                  â”‚                                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                                â”‚
â”‚                 â”‚                          â”‚                                â”‚
â”‚                 â–¼                          â–¼                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚              ğŸ”¥ TRAINING LOOP (HF Trainer)           â”‚                  â”‚
â”‚   â”‚                                                      â”‚                  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚   â”‚   â”‚  for each epoch:                             â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚    for each batch:                           â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      1. Forward Pass  â†’ Loss è¨ˆç®—            â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      2. Backward Pass â†’ Gradients è¨ˆç®—       â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      3. Optimizer Step â†’ Weights Update      â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      4. Logging (loss, lr, VRAM)             â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      5. Eval (if eval_steps reached)         â”‚  â”‚                  â”‚
â”‚   â”‚   â”‚      6. Save checkpoint (if save_steps)      â”‚  â”‚                  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚   â”‚                                                      â”‚                  â”‚
â”‚   â”‚   Powered by: ğŸ¤— Transformers Trainer               â”‚                  â”‚
â”‚   â”‚              + ğŸš€ Accelerate (multi-GPU/DeepSpeed)  â”‚                  â”‚
â”‚   â”‚              + âš¡ FlashAttention                     â”‚                  â”‚
â”‚   â”‚              + ğŸ”§ PEFT (LoRA/QLoRA)                 â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚                    OUTPUT                            â”‚                  â”‚
â”‚   â”‚                                                      â”‚                  â”‚
â”‚   â”‚   ğŸ“ output_dir/                                    â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ checkpoint-100/  (intermediate saves)         â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ checkpoint-200/                               â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors (if LoRA)           â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ model.safetensors (if FFT)                    â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ tokenizer.json                                â”‚                  â”‚
â”‚   â”‚   â”œâ”€â”€ config.json                                   â”‚                  â”‚
â”‚   â”‚   â””â”€â”€ training_args.bin                             â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Axolotl CLI Command Pipeline â€” á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€•á€¯á€¶ Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AXOLOTL COMMAND PIPELINE                        â”‚
â”‚                                                                    â”‚
â”‚    config.yml â”€â”€ preprocess â”€â”€â†’ train â”€â”€â†’ inference / merge        â”‚
â”‚                                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚ 1.Config â”‚â”€â”€â”€â†’â”‚2.Preproc â”‚â”€â”€â”€â†’â”‚ 3.Train  â”‚â”€â”€â”€â†’â”‚4.Infer/  â”‚   â”‚
â”‚    â”‚  á€›á€±á€¸á€á€¬á€¸   â”‚    â”‚  Data    â”‚    â”‚  Model   â”‚    â”‚  Merge   â”‚   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚               â”‚               â”‚               â”‚         â”‚
â”‚    YAML file       Tokenize &      Training loop    Test or       â”‚
â”‚    á€•á€¼á€„á€ºá€†á€„á€º           validate        run            deploy       â”‚
â”‚                                                                    â”‚
â”‚    Optional:                                                       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚    â”‚ 5.Eval   â”‚  â† Benchmark evaluation                          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Essential Commands â€” á€¡á€™á€¼á€²á€á€¯á€¶á€¸á€›á€™á€šá€·á€º Commands

### ğŸ”§ 1. Preprocess (Data á€•á€¼á€„á€ºá€†á€„á€ºá€á€¼á€„á€ºá€¸)

Training á€™á€…á€á€„á€º dataset á€€á€­á€¯ tokenize + validate + cache á€œá€¯á€•á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹

```bash
# Basic preprocess
python -m axolotl.cli.preprocess config.yml

# Accelerate á€”á€²á€· (recommended)
accelerate launch -m axolotl.cli.preprocess config.yml
```

**á€˜á€¬á€œá€¯á€•á€ºá€•á€±á€¸á€œá€²?**
```
preprocess command:
â”œâ”€â”€ âœ… Config YAML á€€á€­á€¯ validate
â”œâ”€â”€ âœ… Dataset load + format check
â”œâ”€â”€ âœ… Tokenization (text â†’ token IDs)
â”œâ”€â”€ âœ… Sample packing (if enabled)
â”œâ”€â”€ âœ… Train/Val split
â”œâ”€â”€ âœ… Preprocessed data á€€á€­á€¯ disk á€™á€¾á€¬ cache
â””â”€â”€ âœ… Token count / sequence length statistics á€•á€¼
```

**á€˜á€šá€ºá€¡á€á€« á€á€¯á€¶á€¸á€á€„á€·á€ºá€œá€²?**
- Config á€›á€±á€¸á€•á€¼á€®á€¸á€á€­á€¯á€„á€ºá€¸ (validate á€–á€­á€¯á€·)
- Dataset á€€á€¼á€®á€¸á€›á€„á€º (preprocess á€á€…á€ºá€á€« run á€•á€¼á€®á€¸ cache á€‘á€¬á€¸á€›á€„á€º training á€•á€­á€¯á€™á€¼á€”á€º)
- Dataset format error á€›á€¾á€­/á€™á€›á€¾á€­ á€…á€…á€ºá€–á€­á€¯á€·

---

### ğŸš€ 2. Train (Training Run)

Model training á€€á€­á€¯ á€…á€á€„á€ºá€•á€«á€á€šá€ºá‹ Axolotl á€›á€²á€· **á€¡á€“á€­á€€ command** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

```bash
# Single GPU training
accelerate launch -m axolotl.cli.train config.yml

# OR (accelerate á€™á€á€¯á€¶á€¸á€˜á€²)
python -m axolotl.cli.train config.yml

# Multi-GPU training (2 GPUs)
accelerate launch --num_processes 2 -m axolotl.cli.train config.yml

# DeepSpeed á€”á€²á€· training
accelerate launch --config_file deepspeed_config.yaml -m axolotl.cli.train config.yml

# Resume from checkpoint
accelerate launch -m axolotl.cli.train config.yml --resume_from_checkpoint /path/to/checkpoint
```

**á€˜á€¬á€œá€¯á€•á€ºá€•á€±á€¸á€œá€²?**
```
train command:
â”œâ”€â”€ 1. Config parse
â”œâ”€â”€ 2. Model load (HF Hub / local)
â”œâ”€â”€ 3. Tokenizer load
â”œâ”€â”€ 4. Dataset load (preprocess if not cached)
â”œâ”€â”€ 5. Adapter setup (if LoRA/QLoRA)
â”œâ”€â”€ 6. Training loop start
â”‚   â”œâ”€â”€ Forward pass
â”‚   â”œâ”€â”€ Loss calculation
â”‚   â”œâ”€â”€ Backward pass
â”‚   â”œâ”€â”€ Optimizer step
â”‚   â”œâ”€â”€ Logging (WandB / TensorBoard / console)
â”‚   â”œâ”€â”€ Evaluation (periodic)
â”‚   â””â”€â”€ Checkpoint save (periodic)
â”œâ”€â”€ 7. Final model save
â””â”€â”€ 8. Training complete! ğŸ‰
```

---

### ğŸ’¬ 3. Inference (á€…á€™á€ºá€¸á€á€•á€ºá€á€¼á€„á€ºá€¸)

Train á€•á€¼á€®á€¸á€á€¬á€¸ model á€€á€­á€¯ interactive chat mode á€™á€¾á€¬ á€…á€™á€ºá€¸á€€á€¼á€Šá€·á€ºá€•á€«á€á€šá€ºá‹

```bash
# Basic inference (interactive prompt)
accelerate launch -m axolotl.cli.inference config.yml

# LoRA model inference (adapter path specify)
accelerate launch -m axolotl.cli.inference config.yml \
  --lora_model_dir="./output/checkpoint-final"

# Gradio UI á€”á€²á€· inference
accelerate launch -m axolotl.cli.inference config.yml --gradio

# Specific prompt á€”á€²á€· inference
accelerate launch -m axolotl.cli.inference config.yml \
  --prompter_type="alpaca" \
  --instruction="What is the capital of Myanmar?"
```

**á€˜á€¬á€œá€¯á€•á€ºá€•á€±á€¸á€œá€²?**
```
inference command:
â”œâ”€â”€ Model + Tokenizer load
â”œâ”€â”€ Adapter merge (if LoRA)
â”œâ”€â”€ Interactive mode start
â”‚   â”œâ”€â”€ User prompt input â†â”€â”€â”
â”‚   â”œâ”€â”€ Tokenize              â”‚
â”‚   â”œâ”€â”€ Generate              â”‚
â”‚   â”œâ”€â”€ Decode + Display â”€â”€â”€â”€â”€â”˜
â”‚   â””â”€â”€ Loop until quit
â””â”€â”€ OR Gradio web UI launch
```

---

### ğŸ”— 4. Merge LoRA (Adapter á€•á€±á€«á€„á€ºá€¸á€á€¼á€„á€ºá€¸)

LoRA/QLoRA adapter weights á€€á€­á€¯ base model á€‘á€² **merge** á€œá€¯á€•á€ºá€•á€¼á€®á€¸ standalone model á€–á€”á€ºá€á€®á€¸á€•á€«á€á€šá€ºá‹

```bash
# LoRA adapter á€€á€­á€¯ base model á€‘á€² merge
python -m axolotl.cli.merge_lora config.yml \
  --lora_model_dir="./output"

# Output directory specify
python -m axolotl.cli.merge_lora config.yml \
  --lora_model_dir="./output" \
  --output_dir="./merged_model"
```

**á€˜á€¬á€œá€¯á€•á€ºá€•á€±á€¸á€œá€²?**
```
merge_lora command:
â”œâ”€â”€ Base model load
â”œâ”€â”€ LoRA adapter load
â”œâ”€â”€ Weights merge (W' = W + AÃ—B)
â”œâ”€â”€ Merged model save (safetensors)
â””â”€â”€ Tokenizer + Config copy

Merged model á€€á€­á€¯ á€’á€®á€”á€±á€›á€¬á€á€½á€±á€™á€¾á€¬ á€á€¯á€¶á€¸á€”á€­á€¯á€„á€º:
â”œâ”€â”€ ğŸ”„ GGUF convert â†’ Ollama/llama.cpp
â”œâ”€â”€ ğŸ“¦ HF Hub upload
â”œâ”€â”€ ğŸš€ vLLM / TGI serving
â””â”€â”€ ğŸ”§ á€‘á€•á€º fine-tune
```

**á€˜á€šá€ºá€¡á€á€« merge á€œá€¯á€•á€ºá€á€„á€·á€ºá€œá€²?**
- LoRA/QLoRA training á€•á€¼á€®á€¸á€á€­á€¯á€„á€ºá€¸ (deployment á€¡á€á€½á€€á€º)
- GGUF/AWQ/GPTQ convert á€™á€œá€¯á€•á€ºá€á€„á€º
- HF Hub á€€á€­á€¯ upload á€™á€œá€¯á€•á€ºá€á€„á€º

> âš ï¸ Full Fine-Tuning (FFT) á€™á€¾á€¬ merge command **á€™á€œá€­á€¯á€•á€«** â€” model weights á€€á€­á€¯ directly save á€‘á€¬á€¸á€•á€¼á€®á€¸á€á€¬á€¸á€•á€«á‹

---

### ğŸ“Š 5. Evaluate (á€¡á€€á€²á€–á€¼á€á€ºá€á€¼á€„á€ºá€¸)

Model á€›á€²á€· performance á€€á€­á€¯ benchmark datasets á€•á€±á€«á€ºá€™á€¾á€¬ evaluate á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

```bash
# Evaluation run
accelerate launch -m axolotl.cli.evaluate config.yml

# Specific eval dataset á€”á€²á€·
accelerate launch -m axolotl.cli.evaluate config.yml \
  --lora_model_dir="./output"
```

---

## Command Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AXOLOTL COMMAND CHEAT SHEET                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  ğŸ“‹ PREPROCESS (validate + tokenize + cache)                          â”‚
â”‚  accelerate launch -m axolotl.cli.preprocess config.yml               â”‚
â”‚                                                                        â”‚
â”‚  ğŸš€ TRAIN (model training)                                            â”‚
â”‚  accelerate launch -m axolotl.cli.train config.yml                    â”‚
â”‚                                                                        â”‚
â”‚  ğŸ”„ RESUME TRAINING (checkpoint á€€á€”á€± á€†á€€á€º)                              â”‚
â”‚  accelerate launch -m axolotl.cli.train config.yml \                  â”‚
â”‚    --resume_from_checkpoint output/checkpoint-500                     â”‚
â”‚                                                                        â”‚
â”‚  ğŸ’¬ INFERENCE (interactive test)                                       â”‚
â”‚  accelerate launch -m axolotl.cli.inference config.yml                â”‚
â”‚                                                                        â”‚
â”‚  ğŸŒ INFERENCE + GRADIO UI                                             â”‚
â”‚  accelerate launch -m axolotl.cli.inference config.yml --gradio       â”‚
â”‚                                                                        â”‚
â”‚  ğŸ”— MERGE LORA (adapter â†’ full model)                                 â”‚
â”‚  python -m axolotl.cli.merge_lora config.yml \                        â”‚
â”‚    --lora_model_dir="./output"                                        â”‚
â”‚                                                                        â”‚
â”‚  ğŸ“Š EVALUATE (benchmark test)                                         â”‚
â”‚  accelerate launch -m axolotl.cli.evaluate config.yml                 â”‚
â”‚                                                                        â”‚
â”‚  ğŸ› DEBUG (1 step train for testing)                                  â”‚
â”‚  accelerate launch -m axolotl.cli.train config.yml --debug            â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Training Workflow Diagram â€” Config á€€á€”á€± Deployment á€¡á€‘á€­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE AXOLOTL WORKFLOW                                â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                                          â”‚
â”‚  â•‘  1. PREPARE  â•‘                                                          â”‚
â”‚  â•šâ•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ ğŸ“ config.yml á€›á€±á€¸á€á€¬á€¸                                            â”‚
â”‚         â”‚     â”œâ”€â”€ base_model: meta-llama/Llama-3.2-1B                      â”‚
â”‚         â”‚     â”œâ”€â”€ adapter: lora / qlora / (none=FFT)                       â”‚
â”‚         â”‚     â”œâ”€â”€ datasets: [{path, type}]                                 â”‚
â”‚         â”‚     â””â”€â”€ training params (lr, epochs, batch...)                   â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ ğŸ“ Dataset á€•á€¼á€„á€ºá€†á€„á€º (JSONL/JSON/Parquet)                         â”‚
â”‚         â”‚     â”œâ”€â”€ alpaca format: {instruction, input, output}              â”‚
â”‚         â”‚     â”œâ”€â”€ sharegpt format: {conversations: [...]}                  â”‚
â”‚         â”‚     â””â”€â”€ completion format: {text: "..."}                         â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â””â”€â”€ ğŸ”‘ HF Login: huggingface-cli login                            â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                                          â”‚
â”‚  â•‘ 2. VALIDATE  â•‘                                                          â”‚
â”‚  â•šâ•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â””â”€â”€ âš™ï¸ accelerate launch -m axolotl.cli.preprocess config.yml     â”‚
â”‚              â”œâ”€â”€ Config validation âœ“                                       â”‚
â”‚              â”œâ”€â”€ Dataset format check âœ“                                    â”‚
â”‚              â”œâ”€â”€ Tokenization âœ“                                            â”‚
â”‚              â””â”€â”€ Cache to disk âœ“                                           â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                                          â”‚
â”‚  â•‘  3. TRAIN    â•‘                                                          â”‚
â”‚  â•šâ•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â””â”€â”€ ğŸš€ accelerate launch -m axolotl.cli.train config.yml          â”‚
â”‚              â”‚                                                              â”‚
â”‚              â”‚  Training Loop:                                              â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚              â”‚  â”‚ Epoch 1/3 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 80%   â”‚                â”‚
â”‚              â”‚  â”‚ Loss: 2.34 â†’ 1.12 â†’ 0.67 â†’ 0.45       â”‚                â”‚
â”‚              â”‚  â”‚ LR:   2e-5 â†’ 1.5e-5 â†’ ... â†’ 0         â”‚                â”‚
â”‚              â”‚  â”‚ VRAM: 8.5 GB / 12 GB                    â”‚                â”‚
â”‚              â”‚  â”‚ Speed: 2.3 samples/sec                  â”‚                â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚              â”‚                                                              â”‚
â”‚              â”œâ”€â”€ ğŸ’¾ Checkpoints saved: output/checkpoint-{N}/              â”‚
â”‚              â””â”€â”€ âœ… Final model saved: output/                             â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                                          â”‚
â”‚  â•‘  4. TEST     â•‘                                                          â”‚
â”‚  â•šâ•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ ğŸ’¬ accelerate launch -m axolotl.cli.inference config.yml      â”‚
â”‚         â”‚      > Prompt: What is AI?                                       â”‚
â”‚         â”‚      > Response: AI is a branch of computer science...           â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â””â”€â”€ ğŸ“Š accelerate launch -m axolotl.cli.evaluate config.yml       â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                                          â”‚
â”‚  â•‘  5. DEPLOY   â•‘                                                          â”‚
â”‚  â•šâ•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•                                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ [If LoRA] ğŸ”— python -m axolotl.cli.merge_lora config.yml      â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ ğŸ“¤ Upload to HF Hub                                            â”‚
â”‚         â”‚     huggingface-cli upload ./output org/model-name               â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”œâ”€â”€ ğŸ”„ Convert to GGUF (for Ollama/llama.cpp)                     â”‚
â”‚         â”‚     python convert_hf_to_gguf.py ./merged --outtype q4_k_m      â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â””â”€â”€ ğŸŒ Serve with vLLM / TGI / Ollama                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Docker Container á€‘á€²á€™á€¾á€¬ á€á€¯á€¶á€¸á€›á€™á€šá€·á€º Commands (Step-by-Step)

### Container Start + Setup

```bash
# 1. Container á€–á€½á€„á€·á€º
docker run --gpus '"all"' --rm -it \
  --shm-size=4g \
  -v $(pwd)/workspace:/workspace/data \
  -p 7860:7860 \
  axolotlai/axolotl:main-latest

# 2. GPU check
nvidia-smi

# 3. HF Login
huggingface-cli login --token $HF_TOKEN

# 4. Working directory
cd /workspace
```

### Training Session

```bash
# 5. Config + Data á€•á€¼á€„á€ºá€†á€„á€º (á€€á€­á€¯á€šá€·á€º config.yml á€€á€­á€¯ /workspace/data/ á€™á€¾á€¬ á€‘á€¬á€¸á€•á€«)
ls /workspace/data/

# 6. Preprocess â€” Data validate + cache
accelerate launch -m axolotl.cli.preprocess /workspace/data/config.yml

# 7. Train â€” Training start
accelerate launch -m axolotl.cli.train /workspace/data/config.yml

# 8. VRAM Monitor (another terminal)
watch -n 1 nvidia-smi
```

### Post-Training

```bash
# 9. Inference â€” Model á€…á€™á€ºá€¸á€á€•á€º
accelerate launch -m axolotl.cli.inference /workspace/data/config.yml

# 10. Gradio UI á€”á€²á€· á€…á€™á€ºá€¸á€á€•á€º
accelerate launch -m axolotl.cli.inference /workspace/data/config.yml --gradio

# 11. LoRA Merge (LoRA/QLoRA training á€•á€¼á€®á€¸á€™á€¾)
python -m axolotl.cli.merge_lora /workspace/data/config.yml \
  --lora_model_dir="/workspace/data/output" \
  --output_dir="/workspace/data/merged_model"

# 12. Merged model á€€á€­á€¯ host machine á€‘á€² á€€á€°á€¸á€šá€°
# (container á€•á€¼á€„á€ºá€•á€™á€¾á€¬)
# docker cp <container_id>:/workspace/data/merged_model ./merged_model
```

---

## Config YAML â€” Essential Fields Guide

Config YAML á€Ÿá€¬ Axolotl á€›á€²á€· **á€¡á€á€€á€º** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ á€’á€® fields á€á€½á€±á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€º:

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ MODEL SECTION â€” á€˜á€šá€º model á€€á€­á€¯ train á€™á€œá€²
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
base_model: meta-llama/Llama-3.2-1B      # HF model name or local path
model_type: LlamaForCausalLM              # Model architecture class
tokenizer_type: AutoTokenizer
trust_remote_code: true                    # Custom model code á€á€½á€„á€·á€ºá€•á€¼á€¯

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ADAPTER SECTION â€” Training method
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# adapter:                                # â† Comment out = Full Fine-Tuning
adapter: lora                             # lora / qlora
lora_r: 32                                # LoRA rank
lora_alpha: 16                            # Scaling factor
lora_dropout: 0.05
lora_target_linear: true                  # All linear layers á€€á€­á€¯ target
# load_in_4bit: true                      # QLoRA á€¡á€á€½á€€á€º uncomment

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ DATASET SECTION â€” Training data
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
datasets:
  - path: ./data/train.jsonl              # Local file path
    type: alpaca                           # Format type
    # type: sharegpt                       # Chat format
    # conversation: chatml                 # Chat template

  # Multiple datasets á€•á€±á€«á€„á€ºá€¸á€œá€­á€¯á€· á€›á€á€šá€º
  # - path: org/dataset_name              # HF Hub dataset
  #   type: sharegpt
  #   split: train

val_set_size: 0.05                        # 5% for validation
dataset_prepared_path: ./prepared          # Cache directory

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ TRAINING SECTION â€” Training hyperparameters
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
output_dir: ./output                       # Output directory

sequence_len: 1024                         # Max token length
num_epochs: 3                              # Training epochs
micro_batch_size: 1                        # Batch per GPU (VRAM dependent)
gradient_accumulation_steps: 8             # Effective batch = micro Ã— accum
eval_batch_size: 1

learning_rate: 2e-4                        # Learning rate (SFT default)
optimizer: adamw_torch                     # Optimizer
lr_scheduler: cosine                       # LR schedule
weight_decay: 0.01
warmup_ratio: 0.1                          # Warmup portion

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ PRECISION & MEMORY â€” GPU optimization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
bf16: auto                                 # bfloat16 (Ampere+ GPUs)
tf32: true                                 # TF32 (faster matrix math)
gradient_checkpointing: true               # Trade compute for memory
flash_attention: true                      # FlashAttention2
sample_packing: true                       # Pack short sequences
pad_to_sequence_len: true

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ LOGGING & SAVING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logging_steps: 1                           # Log every N steps
eval_steps: 20                             # Evaluate every N steps
save_steps: 100                            # Save checkpoint every N steps
save_total_limit: 3                        # Keep N latest checkpoints

# Weights & Biases logging (optional)
# wandb_project: my-project
# wandb_run_id: run-001

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ SPECIAL FEATURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# chat_template: chatml                    # Chat template
# neftune_noise_alpha: 5                   # NEFTune noise
# rl: dpo                                  # DPO training
# dpo_beta: 0.1                            # DPO beta

seed: 42
strict: false
```

---

## Common Command Patterns

### Pattern 1: Quick Test (1-step debug)

Config á€™á€¾á€”á€º/á€™á€™á€¾á€”á€º **á€™á€¼á€”á€ºá€™á€¼á€”á€º á€…á€…á€ºá€–á€­á€¯á€·**:

```bash
# Debug mode â€” 1 training step á€•á€² run
accelerate launch -m axolotl.cli.train config.yml --debug

# á€’á€«á€€ VRAM á€˜á€šá€ºá€œá€±á€¬á€€á€ºá€€á€¯á€”á€ºá€œá€²áŠ config error á€›á€¾á€­/á€™á€›á€¾á€­ á€á€»á€€á€ºá€á€»á€„á€ºá€¸ á€á€­á€”á€­á€¯á€„á€º
```

### Pattern 2: Preprocess â†’ Train â†’ Inference (Full Pipeline)

```bash
# Step 1: Validate & cache data
accelerate launch -m axolotl.cli.preprocess config.yml

# Step 2: Train
accelerate launch -m axolotl.cli.train config.yml

# Step 3: Test
accelerate launch -m axolotl.cli.inference config.yml
```

### Pattern 3: LoRA Training â†’ Merge â†’ Deploy

```bash
# Train with LoRA
accelerate launch -m axolotl.cli.train lora_config.yml

# Merge adapter into base model
python -m axolotl.cli.merge_lora lora_config.yml \
  --lora_model_dir="./output"

# Upload merged model
huggingface-cli upload ./merged_model your-username/model-name
```

### Pattern 4: Resume Interrupted Training

```bash
# Training á€€á€»á€­á€¯á€¸á€á€½á€¬á€¸á€›á€„á€º checkpoint á€€á€”á€± á€†á€€á€º
accelerate launch -m axolotl.cli.train config.yml \
  --resume_from_checkpoint ./output/checkpoint-500
```

### Pattern 5: Multi-GPU Training

```bash
# 2 GPUs
accelerate launch --num_processes 2 -m axolotl.cli.train config.yml

# 4 GPUs
accelerate launch --num_processes 4 -m axolotl.cli.train config.yml

# DeepSpeed ZeRO-2 (multi-GPU memory optimization)
accelerate launch --use_deepspeed \
  --deepspeed_config_file ds_config.json \
  -m axolotl.cli.train config.yml
```

---

## Axolotl Internal Architecture â€” Technical Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AXOLOTL INTERNAL COMPONENTS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  config.yml                                                          â”‚
â”‚      â”‚                                                               â”‚
â”‚      â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ axolotl.utils.   â”‚                                               â”‚
â”‚  â”‚ config.normalize â”‚  â† Config parsing + validation                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚           â”‚                                                          â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚           â–¼                      â–¼                    â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Model Layer   â”‚    â”‚  Dataset Layer  â”‚   â”‚ Trainer Layerâ”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚                â”‚    â”‚                 â”‚   â”‚              â”‚      â”‚
â”‚  â”‚ transformers   â”‚    â”‚ datasets (HF)   â”‚   â”‚ HF Trainer   â”‚      â”‚
â”‚  â”‚ AutoModel      â”‚    â”‚ load_dataset()  â”‚   â”‚ + Accelerate â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚
â”‚  â”‚ â”‚BitsAndBytâ”‚   â”‚    â”‚ â”‚Prompters: â”‚   â”‚   â”‚ â”‚Callbacks:â”‚â”‚      â”‚
â”‚  â”‚ â”‚es (4/8bitâ”‚   â”‚    â”‚ â”‚ alpaca    â”‚   â”‚   â”‚ â”‚ logging  â”‚â”‚      â”‚
â”‚  â”‚ â”‚ quant)   â”‚   â”‚    â”‚ â”‚ sharegpt  â”‚   â”‚   â”‚ â”‚ saving   â”‚â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚ â”‚ chat_tmpl â”‚   â”‚   â”‚ â”‚ eval     â”‚â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚ â”‚ completn  â”‚   â”‚   â”‚ â”‚ early    â”‚â”‚      â”‚
â”‚  â”‚ â”‚PEFT      â”‚   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚ stopping â”‚â”‚      â”‚
â”‚  â”‚ â”‚ LoRA     â”‚   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚
â”‚  â”‚ â”‚ QLoRA    â”‚   â”‚    â”‚ â”‚Tokenizer  â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚
â”‚  â”‚ â”‚ DoRA     â”‚   â”‚    â”‚ â”‚ + Chat    â”‚   â”‚   â”‚ â”‚Optimizer:â”‚â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚ â”‚ template  â”‚   â”‚   â”‚ â”‚ AdamW    â”‚â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚ Adafactorâ”‚â”‚      â”‚
â”‚  â”‚ â”‚Flash     â”‚   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚ 8bit     â”‚â”‚      â”‚
â”‚  â”‚ â”‚Attention â”‚   â”‚    â”‚ â”‚Sample     â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚
â”‚  â”‚ â”‚ 2        â”‚   â”‚    â”‚ â”‚ Packing   â”‚   â”‚   â”‚              â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                      â”‚
â”‚  Underlying Libraries:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ ğŸ¤— Transformers â”‚ ğŸ¤— PEFT â”‚ ğŸ¤— Accelerate â”‚ ğŸ¤— Datasets â”‚     â”‚
â”‚  â”‚ PyTorch â”‚ FlashAttention2 â”‚ BitsAndBytes â”‚ DeepSpeed     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Troubleshooting Commands

```bash
# â”€â”€ GPU/CUDA Issues â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
nvidia-smi                                    # GPU status
python -c "import torch; print(torch.cuda.is_available())"  # CUDA check

# â”€â”€ VRAM Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
watch -n 1 nvidia-smi                         # Real-time VRAM
python -c "
import torch
print(f'{torch.cuda.memory_allocated()/1024**3:.1f}GB allocated')
print(f'{torch.cuda.memory_reserved()/1024**3:.1f}GB reserved')
"

# â”€â”€ Config Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
accelerate launch -m axolotl.cli.preprocess config.yml   # Validates config

# â”€â”€ Dataset Debug â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -c "
from datasets import load_dataset
ds = load_dataset('json', data_files='train.jsonl')
print(ds)
print(ds['train'][0])
"

# â”€â”€ Disk Space â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df -h                                         # Disk usage
du -sh ./output/*                             # Output size

# â”€â”€ Kill Zombie GPU Processes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
nvidia-smi --query-compute-apps=pid --format=csv,noheader | xargs -I{} kill -9 {}

# â”€â”€ Clear GPU Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -c "
import torch
torch.cuda.empty_cache()
print('GPU cache cleared')
"
```
