# ═══════════════════════════════════════════════════════════════════════
#  LLM Server.AI — Docker Compose
# ═══════════════════════════════════════════════════════════════════════
#
#  Usage:
#    docker compose run --rm llm-server        # TUI (interactive)
#    docker compose up -d llm-server-daemon    # daemon only (headless)
#
#  Host paths (change to your preference):
#    MODEL_DIR   — where HF model cache lives on the host
#    CONFIG_DIR  — where server.db + config live on the host
# ═══════════════════════════════════════════════════════════════════════

services:
  # ── Interactive TUI mode ─────────────────────────────────────────────
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-server-ai:latest
    container_name: llm-server-ai

    # TUI needs an interactive terminal
    stdin_open: true
    tty: true

    # ── GPU access ─────────────────────────────────────────────────────
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # ── Network ────────────────────────────────────────────────────────
    # host network so API server (0.0.0.0:8000) is directly accessible
    network_mode: host

    # ── Volumes ────────────────────────────────────────────────────────
    volumes:
      - ${MODEL_DIR:-./data/models}:/root/.cache/huggingface/hub
      - ${CONFIG_DIR:-./data/config}:/root/.config/llm_server_ai

    # ── Environment ────────────────────────────────────────────────────
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TERM=xterm-256color
      - HF_TOKEN=${HF_TOKEN:-}

    # ── Entrypoint: TUI ───────────────────────────────────────────────
    command: ["python", "run.py"]

  # ── Headless daemon mode (no TUI) ───────────────────────────────────
  llm-server-daemon:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-server-ai:latest
    container_name: llm-server-ai-daemon

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    network_mode: host

    volumes:
      - ${MODEL_DIR:-./data/models}:/root/.cache/huggingface/hub
      - ${CONFIG_DIR:-./data/config}:/root/.config/llm_server_ai

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN:-}

    # Run daemon in foreground (no TUI)
    command: ["python", "run.py", "--daemon"]
    restart: unless-stopped
